<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kubernetes Cluster API Provider AWS</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        <link
  href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Lato:ital,wght@0,400;0,700;1,400;1,700&display=swap"
  rel="stylesheet">
<link rel="stylesheet" href="https://cdn.datatables.net/1.10.24/css/jquery.dataTables.min.css">
<script src="https://code.jquery.com/jquery-3.5.1.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/jquery.dataTables.min.js"></script>
<meta name="google-site-verification" content="FGwibyytASqneHUQBmwzN0eOO3-Nd_coIx2YlbhzzSM" />


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="quick-start.html">Quick Start</a></li><li class="chapter-item expanded "><a href="quick-start-operator.html">Quick Start Operator</a></li><li class="chapter-item expanded "><a href="topics/images/amis.html">AMIs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/images/built-amis.html">Published AMIs</a></li><li class="chapter-item expanded "><a href="topics/images/custom-amis.html">Custom AMIs</a></li></ol></li><li class="chapter-item expanded "><a href="topics/index.html">Topics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/using-clusterawsadm-to-fulfill-prerequisites.html">Using clusterawsadm to fulfill prerequisites</a></li><li class="chapter-item expanded "><a href="topics/accessing-ec2-instances.html">Accessing EC2 instances</a></li><li class="chapter-item expanded "><a href="topics/spot-instances.html">Spot instances</a></li><li class="chapter-item expanded "><a href="topics/machinepools.html">Machine Pools</a></li><li class="chapter-item expanded "><a href="topics/multitenancy.html">Multi-tenancy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/full-multitenancy-implementation.html">Multi-tenancy in EKS-managed clusters</a></li></ol></li><li class="chapter-item expanded "><a href="topics/eks/index.html">EKS Support</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/eks/prerequisites.html">Prerequisites</a></li><li class="chapter-item expanded "><a href="topics/eks/enabling.html">Enabling EKS Support</a></li><li class="chapter-item expanded "><a href="topics/eks/pod-networking.html">Pod Networking</a></li><li class="chapter-item expanded "><a href="topics/eks/creating-a-cluster.html">Creating a cluster</a></li><li class="chapter-item expanded "><a href="topics/eks/eks-console.html">Using EKS Console</a></li><li class="chapter-item expanded "><a href="topics/eks/addons.html">Using EKS Addons</a></li><li class="chapter-item expanded "><a href="topics/eks/encryption.html">Enabling Encryption</a></li><li class="chapter-item expanded "><a href="topics/eks/cluster-upgrades.html">Cluster Upgrades</a></li></ol></li><li class="chapter-item expanded "><a href="topics/rosa/index.html">ROSA Support</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/rosa/enabling.html">Enabling ROSA Support</a></li><li class="chapter-item expanded "><a href="topics/rosa/creating-a-cluster.html">Creating a cluster</a></li><li class="chapter-item expanded "><a href="topics/rosa/creating-rosa-machinepools.html">Creating MachinePools</a></li><li class="chapter-item expanded "><a href="topics/rosa/upgrades.html">Upgrades</a></li><li class="chapter-item expanded "><a href="topics/rosa/external-auth.html">External Auth Providers</a></li><li class="chapter-item expanded "><a href="topics/rosa/support.html">Support</a></li></ol></li><li class="chapter-item expanded "><a href="topics/bring-your-own-aws-infrastructure.html">Bring Your Own AWS Infrastructure</a></li><li class="chapter-item expanded "><a href="topics/specify-management-iam-role.html">Specifying the IAM Role to use for Management Components</a></li><li class="chapter-item expanded "><a href="topics/external-cloud-provider-with-ebs-csi-driver.html">Using external cloud provider with EBS CSI driver</a></li><li class="chapter-item expanded "><a href="topics/restricting-cluster-api-to-certain-namespaces.html">Restricting Cluster API to certain namespaces</a></li><li class="chapter-item expanded "><a href="topics/using-iam-roles-in-mgmt-cluster.html">Using IAM roles in management cluster instead of credentials</a></li><li class="chapter-item expanded "><a href="topics/failure-domains/index.html">Failure domains</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/failure-domains/control-planes.html">Control planes</a></li><li class="chapter-item expanded "><a href="topics/failure-domains/worker-nodes.html">Worker nodes</a></li></ol></li><li class="chapter-item expanded "><a href="topics/userdata-privacy.html">Userdata Privacy</a></li><li class="chapter-item expanded "><a href="topics/troubleshooting.html">Troubleshooting</a></li><li class="chapter-item expanded "><a href="topics/iam-permissions.html">IAM Permissions Used</a></li><li class="chapter-item expanded "><a href="topics/ignition-support.html">Ignition support</a></li><li class="chapter-item expanded "><a href="topics/external-resource-gc.html">External Resource Garbage Collection</a></li><li class="chapter-item expanded "><a href="topics/instance-metadata.html">Instance Metadata</a></li><li class="chapter-item expanded "><a href="topics/network-load-balancer-with-awscluster.html">Network Load Balancers</a></li><li class="chapter-item expanded "><a href="topics/secondary-load-balancer.html">Secondary Control Plane Load Balancer</a></li><li class="chapter-item expanded "><a href="topics/provision-edge-zones.html">Provision AWS Local Zone subnets</a></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm.html">clusterawsadm command reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap.html">bootstrap</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_credentials.html">credentials</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_credentials_encode-as-profile.html">encode-as-profile</a></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">iam</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-config.html">print-config</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-policy.html">print-policy</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam_create-cloudformation-stack.html">create-cloudformation-stack</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam_delete-cloudformation-stack.html">delete-cloudformation-stack</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-cloudformation-template.html">print-cloudformation-template</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_controller.html">controller</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_controller_print-credentials.html">print-credentials</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_controller_rollout-controller.html">rollout-controller</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_controller_update-credentials.html">update-credentials</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_controller_zero-credentials.html">zero-credentials</a></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_eks.html">eks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_eks_addons.html">addons</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_eks_addons_list-available.html">list-available</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_eks_addons_list-installed.html">list-installed</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_gc.html">gc</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_gc_configure.html">configure</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_gc_disable.html">disable</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_gc_enable.html">enable</a></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_resource.html">resource</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_resource_list.html">list</a></li></ol></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_version.html">version</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_ami.html">ami</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_ami_copy.html">copy</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_ami_encrypted-copy.html">encrypted-copy</a></li><li class="chapter-item expanded "><a href="clusterawsadm/clusterawsadm_ami_list.html">list</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="development/development.html">Developer Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="development/tilt-setup.html">Development with Tilt</a></li><li class="chapter-item expanded "><a href="development/e2e.html">Developing E2E tests</a></li><li class="chapter-item expanded "><a href="development/conventions.html">Coding Conventions</a></li><li class="chapter-item expanded "><a href="development/nightlies.html">Try unreleased changes with Nightly Builds</a></li><li class="chapter-item expanded "><a href="development/amis.html">Publishing AMIs</a></li></ol></li><li class="chapter-item expanded "><a href="crd/index.html">CRD Reference</a></li><li class="chapter-item expanded "><a href="topics/reference/reference.html">Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="topics/reference/glossary.html">Glossary</a></li><li class="chapter-item expanded "><a href="topics/reference/ports.html">Ports</a></li><li class="chapter-item expanded "><a href="topics/reference/jobs.html">Jobs</a></li><li class="chapter-item expanded "><a href="topics/reference/versions.html">Version Support</a></li><li class="chapter-item expanded "><a href="topics/reference/contributing.html">Contributing</a></li></ol></li><li class="chapter-item expanded "><a href="roadmap.html">Roadmap</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Kubernetes Cluster API Provider AWS</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://sigs.k8s.io/cluster-api-provider-aws" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p align="center">
<img src="https://github.com/kubernetes/kubernetes/raw/master/logo/logo.png"  width="100x"><a href="https://aws.amazon.com/opensource/"><img width="192x" src="https://d0.awsstatic.com/logos/powered-by-aws.png" alt="Powered by AWS Cloud Computing"></a>
</p>
<p align="center">
<!-- go doc / reference card -->
<a href="https://godoc.org/sigs.k8s.io/cluster-api-provider-aws">
<img src="https://godoc.org/sigs.k8s.io/cluster-api-provider-aws?status.svg"></a>
<!-- goreportcard badge -->
<a href="https://goreportcard.com/report/sigs.k8s.io/cluster-api-provider-aws">
<img src="https://goreportcard.com/badge/sigs.k8s.io/cluster-api-provider-aws"></a>
<!-- join kubernetes slack channel for cluster-api-aws-provider -->
<a href="http://slack.k8s.io/">
<img src="https://img.shields.io/badge/join%20slack-%23cluster--api--aws-brightgreen"></a>
<!-- openssf badge -->
<a href="https://bestpractices.coreinfrastructure.org/projects/5688">
<img src="https://bestpractices.coreinfrastructure.org/projects/5688/badge"></a>
</p>
<hr />
<p>Kubernetes-native declarative infrastructure for AWS.</p>
<h2><a class="header" href="#what-is-the-cluster-api-provider-aws" id="what-is-the-cluster-api-provider-aws">What is the Cluster API Provider AWS</a></h2>
<p>The <a href="https://github.com/kubernetes-sigs/cluster-api">Cluster API</a> brings
declarative, Kubernetes-style APIs to cluster creation, configuration and
management.</p>
<p>The API itself is shared across multiple cloud providers allowing for true AWS
hybrid deployments of Kubernetes. It is built atop the lessons learned from
previous cluster managers such as <a href="https://github.com/kubernetes/kops">kops</a> and
<a href="http://kubicorn.io/">kubicorn</a>.</p>
<h2><a class="header" href="#documentation" id="documentation">Documentation</a></h2>
<p>Please see our <a href="https://cluster-api-aws.sigs.k8s.io">book</a> for in-depth documentation.</p>
<h2><a class="header" href="#launching-a-kubernetes-cluster-on-aws" id="launching-a-kubernetes-cluster-on-aws">Launching a Kubernetes cluster on AWS</a></h2>
<p>Check out the <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">Cluster API Quick Start</a> for launching a
cluster on AWS.</p>
<h2><a class="header" href="#features" id="features">Features</a></h2>
<ul>
<li>Native Kubernetes manifests and API</li>
<li>Manages the bootstrapping of VPCs, gateways, security groups and instances.</li>
<li>Choice of Linux distribution using <a href="https://cluster-api-aws.sigs.k8s.io/topics/images/built-amis.html">pre-baked AMIs</a>.</li>
<li>Deploys Kubernetes control planes into private subnets with a separate
bastion server.</li>
<li>Doesn’t use SSH for bootstrapping nodes.</li>
<li>Installs only the minimal components to bootstrap a control plane and workers.</li>
<li>Supports control planes on EC2 instances.</li>
<li><a href="https://cluster-api-aws.sigs.k8s.io/topics/eks/index.html">EKS support</a></li>
</ul>
<hr />
<h2><a class="header" href="#compatibility-with-cluster-api-and-kubernetes-versions" id="compatibility-with-cluster-api-and-kubernetes-versions">Compatibility with Cluster API and Kubernetes Versions</a></h2>
<p>This provider’s versions are compatible with the following versions of Cluster API
and support all Kubernetes versions that is supported by its compatible Cluster API version:</p>
<table><thead><tr><th></th><th align="center">Cluster API v1alpha4 (v0.4)</th><th align="center">Cluster API v1beta1 (v1.x)</th></tr></thead><tbody>
<tr><td>CAPA v1alpha4 <code>(v0.7)</code></td><td align="center">✓</td><td align="center">☓</td></tr>
<tr><td>CAPA v1beta1  <code>(v1.x)</code></td><td align="center">☓</td><td align="center">✓</td></tr>
<tr><td>CAPA v1beta2  <code>(v2.x, main)</code></td><td align="center">☓</td><td align="center">✓</td></tr>
</tbody></table>
<p>(See <a href="https://cluster-api.sigs.k8s.io/reference/versions.html">Kubernetes support matrix</a> of Cluster API versions).</p>
<hr />
<h2><a class="header" href="#kubernetes-versions-with-published-amis" id="kubernetes-versions-with-published-amis">Kubernetes versions with published AMIs</a></h2>
<p>See <a href="https://cluster-api-aws.sigs.k8s.io/topics/images/amis.html">amis</a> for the list of most recently published AMIs.</p>
<hr />
<h2><a class="header" href="#clusterawsadm" id="clusterawsadm">clusterawsadm</a></h2>
<p><code>clusterawsadm</code> CLI tool provides bootstrapping, AMI, EKS, and controller related helpers.</p>
<p><code>clusterawsadm</code> binaries are released with each release, can be found under <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/latest">assets</a> section.</p>
<p><code>clusterawsadm</code> could also be installed via Homebrew on macOS and linux OS.
Install the latest release using homebrew:</p>
<pre><code class="language-shell">brew install clusterawsadm
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-shell">clusterawsadm version
</code></pre>
<hr />
<h2><a class="header" href="#getting-involved-and-contributing" id="getting-involved-and-contributing">Getting involved and contributing</a></h2>
<p>Are you interested in contributing to cluster-api-provider-aws? We, the
maintainers and community, would love your suggestions, contributions, and help!
Also, the maintainers can be contacted at any time to learn more about how to get
involved.</p>
<p>In the interest of getting more new people involved we tag issues with
<a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A%22good+first+issue%22"><code>good first issue</code></a>.
These are typically issues that have smaller scope but are good ways to start
to get acquainted with the codebase.</p>
<p>We also encourage ALL active community participants to act as if they are
maintainers, even if you don’t have “official” write permissions. This is a
community effort, we are here to serve the Kubernetes community. If you have an
active interest and you want to get involved, you have real power! Don’t assume
that the only people who can get things done around here are the “maintainers”.</p>
<p>We also would love to add more “official” maintainers, so show us what you can
do!</p>
<p>This repository uses the Kubernetes bots.  See a full list of the commands <a href="https://go.k8s.io/bot-commands">here</a>.</p>
<h3><a class="header" href="#build-the-images-locally" id="build-the-images-locally">Build the images locally</a></h3>
<p>If you want to just build the CAPA containers locally, run</p>
<pre><code class="language-shell">  REGISTRY=docker.io/my-reg make docker-build
</code></pre>
<h3><a class="header" href="#tilt-based-development-environment" id="tilt-based-development-environment">Tilt-based development environment</a></h3>
<p>See <a href="https://cluster-api-aws.sigs.k8s.io/development/development.html">development</a> section for details.</p>
<h3><a class="header" href="#implementer-office-hours" id="implementer-office-hours">Implementer office hours</a></h3>
<p>Maintainers hold office hours every two weeks, with sessions open to all
developers working on this project.</p>
<p>Office hours are hosted on a zoom video chat every other Monday
at 09:00 (Pacific) / 12:00 (Eastern) / 17:00 (Europe/London),
and are published on the <a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com">Kubernetes community meetings calendar</a>.</p>
<h3><a class="header" href="#other-ways-to-communicate-with-the-contributors" id="other-ways-to-communicate-with-the-contributors">Other ways to communicate with the contributors</a></h3>
<p>Please check in with us in the <a href="https://kubernetes.slack.com/messages/CD6U2V71N">#cluster-api-aws</a> channel on Slack.</p>
<h2><a class="header" href="#github-issues" id="github-issues">Github issues</a></h2>
<h3><a class="header" href="#bugs" id="bugs">Bugs</a></h3>
<p>If you think you have found a bug please follow the instructions below.</p>
<ul>
<li>Please spend a small amount of time giving due diligence to the issue tracker. Your issue might be a duplicate.</li>
<li>Get the logs from the cluster controllers. Please paste this into your issue.</li>
<li>Open a <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/new">new issue</a>.</li>
<li>Remember that users might be searching for your issue in the future, so please give it a meaningful title to help others.</li>
<li>Feel free to reach out to the cluster-api community on the <a href="https://kubernetes.slack.com/messages/CD6U2V71N">kubernetes slack</a>.</li>
</ul>
<h3><a class="header" href="#tracking-new-features" id="tracking-new-features">Tracking new features</a></h3>
<p>We also use the issue tracker to track features. If you have an idea for a feature, or think you can help kops become even more awesome follow the steps below.</p>
<ul>
<li>Open a <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/new">new issue</a>.</li>
<li>Remember that users might be searching for your issue in the future, so please
give it a meaningful title to help others.</li>
<li>Clearly define the use case, using concrete examples. EG: I type <code>this</code> and
cluster-api-provider-aws does <code>that</code>.</li>
<li>Some of our larger features will require some design. If you would like to
include a technical design for your feature please include it in the issue.</li>
<li>After the new feature is well understood, and the design agreed upon, we can
start coding the feature. We would love for you to code it. So please open
up a <strong>WIP</strong> <em>(work in progress)</em> pull request, and happy coding.</li>
</ul>
<blockquote>
<p>“Amazon Web Services, AWS, and the “Powered by AWS” logo materials are
trademarks of Amazon.com, Inc. or its affiliates in the United States
and/or other countries.”</p>
</blockquote>
<h2><a class="header" href="#our-contributors" id="our-contributors">Our Contributors</a></h2>
<p>Thank you to all contributors and a special thanks to our current maintainers &amp; reviewers:</p>
<table><thead><tr><th>Maintainers</th><th>Reviewers</th></tr></thead><tbody>
<tr><td><a href="https://github.com/richardcase">@richardcase</a> (from 2020-12-04)</td><td><a href="https://github.com/cnmcavoy">@cnmcavoy</a> (from 2023-10-16)</td></tr>
<tr><td><a href="https://github.com/Ankitasw">@Ankitasw</a> (from 2022-10-19)</td><td><a href="https://github.com/AverageMarcus">@AverageMarcus</a> (from 2022-10-19)</td></tr>
<tr><td><a href="https://github.com/dlipovetsky">@dlipovetsky</a> (from 2021-10-31)</td><td><a href="https://github.com/luthermonson">@luthermonson</a> (from 2023-03-08)</td></tr>
<tr><td><a href="https://github.com/nrb">@nrb</a> (from 2024-05-24)</td><td><a href="https://github.com/faiq">@faiq</a> (from 2023-10-16)</td></tr>
<tr><td><a href="https://github.com/AndiDog">@AndiDog</a> (from 2023-12-13)</td><td><a href="https://github.com/fiunchinho">@fiunchinho</a> (from 2023-11-6)</td></tr>
<tr><td></td><td><a href="https://github.com/damdo">@damdo</a> (from 2023-03-01)</td></tr>
</tbody></table>
<p>and the previous/emeritus maintainers &amp; reviewers:</p>
<table><thead><tr><th>Emeritus Maintainers</th><th>Emeritus Reviewers</th></tr></thead><tbody>
<tr><td><a href="https://github.com/chuckha">@chuckha</a></td><td><a href="https://github.com/ashish-amarnath">@ashish-amarnath</a></td></tr>
<tr><td><a href="https://github.com/detiber">@detiber</a></td><td><a href="https://github.com/davidewatson">@davidewatson</a></td></tr>
<tr><td><a href="https://github.com/ncdc">@ncdc</a></td><td><a href="https://github.com/enxebre">@enxebre</a></td></tr>
<tr><td><a href="https://github.com/randomvariable">@randomvariable</a></td><td><a href="https://github.com/ingvagabund">@ingvagabund</a></td></tr>
<tr><td><a href="https://github.com/rudoi">@rudoi</a></td><td><a href="https://github.com/michaelbeaumont">@michaelbeaumont</a></td></tr>
<tr><td><a href="https://github.com/sedefsavas">@sedefsavas</a></td><td><a href="https://github.com/sethp-nr">@sethp-nr</a></td></tr>
<tr><td><a href="https://github.com/Skarlso">@Skarlso</a></td><td><a href="https://github.com/shivi28">@shivi28</a></td></tr>
<tr><td><a href="https://github.com/vincepri">@vincepri</a></td><td><a href="https://github.com/dthorsen">@dthorsen</a></td></tr>
<tr><td></td><td><a href="https://github.com/pydctw">@pydctw</a></td></tr>
</tbody></table>
<p>All the CAPA contributors:</p>
<p>
<a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=kubernetes-sigs/cluster-api-provider-aws" />
</a>
</p>
<!-- References -->
<h1><a class="header" href="#quick-start" id="quick-start">Quick Start</a></h1>
<p>In this tutorial we’ll cover the basics of how to use Cluster API to create one or more Kubernetes clusters.</p>
<h2><a class="header" href="#installation" id="installation">Installation</a></h2>
<p>There are two major quickstart paths:  Using clusterctl or the Cluster API Operator.</p>
<p>This article describes a path that uses the <code>clusterctl</code> CLI tool to handle the lifecycle of a Cluster API <a href="https://cluster-api.sigs.k8s.io/reference/glossary#management-cluster">management cluster</a>.</p>
<p>The clusterctl command line interface is specifically designed for providing a simple “day 1 experience” and a quick start with Cluster API. It automates fetching the YAML files defining <a href="https://cluster-api.sigs.k8s.io/reference/glossary#provider-components">provider components</a> and installing them.</p>
<p>Additionally it encodes a set of best practices in managing providers, that helps the user in avoiding mis-configurations or in managing day 2 operations such as upgrades.</p>
<p>The Cluster API Operator is a Kubernetes Operator built on top of clusterctl and designed to empower cluster administrators to handle the lifecycle of Cluster API providers within a management cluster using a declarative approach. It aims to improve user experience in deploying and managing Cluster API, making it easier to handle day-to-day tasks and automate workflows with GitOps. Visit the <a href="./quick-start-operator.html">CAPI Operator quickstart</a> if you want to experiment with this tool.</p>
<h3><a class="header" href="#common-prerequisites" id="common-prerequisites">Common Prerequisites</a></h3>
<ul>
<li>Install and setup <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> in your local environment</li>
<li>Install <a href="https://kind.sigs.k8s.io/">kind</a> and <a href="https://www.docker.com/">Docker</a></li>
<li>Install <a href="https://helm.sh/docs/intro/install/">Helm</a></li>
</ul>
<h3><a class="header" href="#install-andor-configure-a-kubernetes-cluster" id="install-andor-configure-a-kubernetes-cluster">Install and/or configure a Kubernetes cluster</a></h3>
<p>Cluster API requires an existing Kubernetes cluster accessible via kubectl. During the installation process the
Kubernetes cluster will be transformed into a <a href="../reference/glossary.html#management-cluster">management cluster</a> by installing the Cluster API <a href="../reference/glossary.html#provider-components">provider components</a>, so it
is recommended to keep it separated from any application workload.</p>
<p>It is a common practice to create a temporary, local bootstrap cluster which is then used to provision
a target <a href="../reference/glossary.html#management-cluster">management cluster</a> on the selected <a href="../reference/glossary.html#infrastructure-provider">infrastructure provider</a>.</p>
<p><strong>Choose one of the options below:</strong></p>
<ol>
<li>
<p><strong>Existing Management Cluster</strong></p>
<p>For production use-cases a “real” Kubernetes cluster should be used with appropriate backup and disaster recovery policies and procedures in place. The Kubernetes cluster must be at least v1.20.0.</p>
<pre><code class="language-bash">export KUBECONFIG=&lt;...&gt;
</code></pre>
</li>
</ol>
<p><strong>OR</strong></p>
<ol start="2">
<li>
<p><strong>Kind</strong></p>
<aside class="note warning">
<h1><a class="header" href="#warning" id="warning">Warning</a></h1>
<p><a href="https://kind.sigs.k8s.io/">kind</a> is not designed for production use.</p>
<p><strong>Minimum <a href="https://kind.sigs.k8s.io/">kind</a> supported version</strong>: v0.31.0</p>
<p><strong>Help with common issues can be found in the <a href="./troubleshooting.html">Troubleshooting Guide</a>.</strong></p>
<p>Note for macOS users: you may need to <a href="https://docs.docker.com/docker-for-mac/#resources">increase the memory available</a> for containers (recommend 6 GB for CAPD).</p>
<p>Note for Linux users: you may need to <a href="./troubleshooting.html#cluster-api-with-docker----too-many-open-files">increase <code>ulimit</code> and <code>inotify</code> when using Docker (CAPD)</a>.</p>
</aside>
<p><a href="https://kind.sigs.k8s.io/">kind</a> can be used for creating a local Kubernetes cluster for development environments or for
the creation of a temporary <a href="../reference/glossary.html#bootstrap-cluster">bootstrap cluster</a> used to provision a target <a href="../reference/glossary.html#management-cluster">management cluster</a> on the selected infrastructure provider.</p>
<p>The installation procedure depends on the version of kind; if you are planning to use the Docker infrastructure provider,
please follow the additional instructions in the dedicated tab:</p>
<div id="install-kind" class="tabset"><input type="radio" name="install-kind" id="install-kind-Default" aria-controls="install-kind-Default" checked><label for="install-kind-Default">Default</label><input type="radio" name="install-kind" id="install-kind-Docker" aria-controls="install-kind-Docker" ><label for="install-kind-Docker">Docker</label><input type="radio" name="install-kind" id="install-kind-KubeVirt" aria-controls="install-kind-KubeVirt" ><label for="install-kind-KubeVirt">KubeVirt</label><div class="tab-panels">
<section id="tab-Default" class="tab-panel">
<p>Create the kind cluster:</p>
<pre><code class="language-bash">kind create cluster
</code></pre>
<p>Test to ensure the local kind cluster is ready:</p>
<pre><code class="language-bash">kubectl cluster-info
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<p>Run the following command to create a kind config file for allowing the Docker provider to access Docker on the host:</p>
<pre><code class="language-bash">cat &gt; kind-cluster-with-extramounts.yaml &lt;&lt;EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  extraMounts:
    - hostPath: /var/run/docker.sock
      containerPath: /var/run/docker.sock
EOF
</code></pre>
<p>Then follow the instruction for your kind version using  <code>kind create cluster --config kind-cluster-with-extramounts.yaml</code>
to create the management cluster using the above file.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<h4><a class="header" href="#create-the-kind-cluster" id="create-the-kind-cluster">Create the Kind Cluster</a></h4>
<p><a href="https://kubevirt.io/">KubeVirt</a> is a cloud native virtualization solution. The virtual machines we’re going to create and use for
the workload cluster’s nodes, are actually running within pods in the management cluster. In order to communicate with
the workload cluster’s API server, we’ll need to expose it. We are using Kind which is a limited environment. The
easiest way to expose the workload cluster’s API server (a pod within a node running in a VM that is itself running
within a pod in the management cluster, that is running inside a Docker container), is to use a LoadBalancer service.</p>
<p>To allow using a LoadBalancer service, we can’t use the kind’s default CNI (kindnet), but we’ll need to install
another CNI, like Calico. In order to do that, we’ll need first to initiate the kind cluster with two modifications:</p>
<ol>
<li>Disable the default CNI</li>
<li>Add the Docker credentials to the cluster, to avoid the Docker Hub pull rate limit of the calico images; read more
about it in the <a href="https://docs.docker.com/docker-hub/download-rate-limit/">docker documentation</a>, and in the
<a href="https://kind.sigs.k8s.io/docs/user/private-registries/#mount-a-config-file-to-each-node">kind documentation</a>.</li>
</ol>
<p>Create a configuration file for kind. Please notice the Docker config file path, and adjust it to your local setting:</p>
<pre><code class="language-bash">cat &lt;&lt;EOF &gt; kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
# the default CNI will not be installed
  disableDefaultCNI: true
nodes:
- role: control-plane
  extraMounts:
   - containerPath: /var/lib/kubelet/config.json
     hostPath: &lt;YOUR DOCKER CONFIG FILE PATH&gt;
EOF
</code></pre>
<p>Now, create the kind cluster with the configuration file:</p>
<pre><code class="language-bash">kind create cluster --config=kind-config.yaml
</code></pre>
<p>Test to ensure the local kind cluster is ready:</p>
<pre><code class="language-bash">kubectl cluster-info
</code></pre>
<h4><a class="header" href="#install-the-calico-cni" id="install-the-calico-cni">Install the Calico CNI</a></h4>
<p>Now we’ll need to install a CNI. In this example, we’re using calico, but other CNIs should work as well. Please see
<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico">calico installation guide</a>
for more details (use the “Manifest” tab). Below is an example of how to install calico version v3.29.1.</p>
<p>Use the Calico manifest to create the required resources; e.g.:</p>
<pre><code class="language-bash">kubectl create -f  https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/calico.yaml
</code></pre>
</section>
</div></div>
</li>
</ol>
<h3><a class="header" href="#install-clusterctl" id="install-clusterctl">Install clusterctl</a></h3>
<p>The clusterctl CLI tool handles the lifecycle of a Cluster API management cluster.</p>
<div id="install-clusterctl" class="tabset"><input type="radio" name="install-clusterctl" id="install-clusterctl-Linux" aria-controls="install-clusterctl-Linux" checked><label for="install-clusterctl-Linux">Linux</label><input type="radio" name="install-clusterctl" id="install-clusterctl-macOS" aria-controls="install-clusterctl-macOS" ><label for="install-clusterctl-macOS">macOS</label><input type="radio" name="install-clusterctl" id="install-clusterctl-homebrew" aria-controls="install-clusterctl-homebrew" ><label for="install-clusterctl-homebrew">homebrew</label><input type="radio" name="install-clusterctl" id="install-clusterctl-Windows" aria-controls="install-clusterctl-Windows" ><label for="install-clusterctl-Windows">Windows</label><div class="tab-panels">
<section id="tab-Linux" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-linux" id="install-clusterctl-binary-with-curl-on-linux">Install clusterctl binary with curl on Linux</a></h4>
<p>If you are unsure you can determine your computers architecture by running <code>uname -a</code></p>
<p>Download for AMD64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-amd64 -o clusterctl
</code></pre>
<p>Download for ARM64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-arm64 -o clusterctl
</code></pre>
<p>Download for PPC64LE:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-ppc64le -o clusterctl
</code></pre>
<p>Install clusterctl:</p>
<pre><code class="language-bash">sudo install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-macOS" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-macos" id="install-clusterctl-binary-with-curl-on-macos">Install clusterctl binary with curl on macOS</a></h4>
<p>If you are unsure you can determine your computers architecture by running <code>uname -a</code></p>
<p>Download for AMD64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-darwin-amd64 -o clusterctl
</code></pre>
<p>Download for M1 CPU (”Apple Silicon”) / ARM64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-darwin-arm64 -o clusterctl
</code></pre>
<p>Make the clusterctl binary executable.</p>
<pre><code class="language-bash">chmod +x ./clusterctl
</code></pre>
<p>Move the binary in to your PATH.</p>
<pre><code class="language-bash">sudo mv ./clusterctl /usr/local/bin/clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-homebrew" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-with-homebrew-on-macos-and-linux" id="install-clusterctl-with-homebrew-on-macos-and-linux">Install clusterctl with homebrew on macOS and Linux</a></h4>
<p>Install the latest release using homebrew:</p>
<pre><code class="language-bash">brew install clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-windows" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-windows-using-powershell" id="install-clusterctl-binary-with-curl-on-windows-using-powershell">Install clusterctl binary with curl on Windows using PowerShell</a></h4>
<p>Go to the working directory where you want clusterctl downloaded.</p>
<p>Download the latest release; on Windows, type:</p>
<pre><code class="language-powershell">curl.exe -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-windows-amd64.exe -o clusterctl.exe
</code></pre>
<p>Append or prepend the path of that directory to the <code>PATH</code> environment variable.</p>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-powershell">clusterctl.exe version
</code></pre>
</section>
</div></div>
<h3><a class="header" href="#initialize-the-management-cluster" id="initialize-the-management-cluster">Initialize the management cluster</a></h3>
<p>Now that we’ve got clusterctl installed and all the prerequisites in place, let’s transform the Kubernetes cluster
into a management cluster by using <code>clusterctl init</code>.</p>
<p>The command accepts as input a list of providers to install; when executed for the first time, <code>clusterctl init</code>
automatically adds to the list the <code>cluster-api</code> core provider, and if unspecified, it also adds the <code>kubeadm</code> bootstrap
and <code>kubeadm</code> control-plane providers.</p>
<h4><a class="header" href="#enabling-feature-gates" id="enabling-feature-gates">Enabling Feature Gates</a></h4>
<p>Feature gates can be enabled by exporting environment variables before executing <code>clusterctl init</code>.
For example, the <code>ClusterTopology</code> feature, which is required to enable support for managed topologies and ClusterClass,
can be enabled via:</p>
<pre><code class="language-bash">export CLUSTER_TOPOLOGY=true
</code></pre>
<p>Additional documentation about experimental features can be found in <a href="../tasks/experimental-features/experimental-features.html">Experimental Features</a>.</p>
<h4><a class="header" href="#initialization-for-common-providers" id="initialization-for-common-providers">Initialization for common providers</a></h4>
<p>Depending on the infrastructure provider you are planning to use, some additional prerequisites should be satisfied
before getting started with Cluster API. See below for the expected settings for common providers.</p>
<div id="tab-installation-infrastructure" class="tabset"><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Akamai (Linode)" aria-controls="tab-installation-infrastructure-Akamai (Linode)" checked><label for="tab-installation-infrastructure-Akamai (Linode)">Akamai (Linode)</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-AWS" aria-controls="tab-installation-infrastructure-AWS" ><label for="tab-installation-infrastructure-AWS">AWS</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Azure" aria-controls="tab-installation-infrastructure-Azure" ><label for="tab-installation-infrastructure-Azure">Azure</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-CloudStack" aria-controls="tab-installation-infrastructure-CloudStack" ><label for="tab-installation-infrastructure-CloudStack">CloudStack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-DigitalOcean" aria-controls="tab-installation-infrastructure-DigitalOcean" ><label for="tab-installation-infrastructure-DigitalOcean">DigitalOcean</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Docker" aria-controls="tab-installation-infrastructure-Docker" ><label for="tab-installation-infrastructure-Docker">Docker</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-GCP" aria-controls="tab-installation-infrastructure-GCP" ><label for="tab-installation-infrastructure-GCP">GCP</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Harvester" aria-controls="tab-installation-infrastructure-Harvester" ><label for="tab-installation-infrastructure-Harvester">Harvester</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Hetzner" aria-controls="tab-installation-infrastructure-Hetzner" ><label for="tab-installation-infrastructure-Hetzner">Hetzner</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Hivelocity" aria-controls="tab-installation-infrastructure-Hivelocity" ><label for="tab-installation-infrastructure-Hivelocity">Hivelocity</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Huawei" aria-controls="tab-installation-infrastructure-Huawei" ><label for="tab-installation-infrastructure-Huawei">Huawei</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-IBM Cloud" aria-controls="tab-installation-infrastructure-IBM Cloud" ><label for="tab-installation-infrastructure-IBM Cloud">IBM Cloud</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-IONOS Cloud" aria-controls="tab-installation-infrastructure-IONOS Cloud" ><label for="tab-installation-infrastructure-IONOS Cloud">IONOS Cloud</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-K0smotron" aria-controls="tab-installation-infrastructure-K0smotron" ><label for="tab-installation-infrastructure-K0smotron">K0smotron</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-KubeKey" aria-controls="tab-installation-infrastructure-KubeKey" ><label for="tab-installation-infrastructure-KubeKey">KubeKey</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-KubeVirt" aria-controls="tab-installation-infrastructure-KubeVirt" ><label for="tab-installation-infrastructure-KubeVirt">KubeVirt</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Metal3" aria-controls="tab-installation-infrastructure-Metal3" ><label for="tab-installation-infrastructure-Metal3">Metal3</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-metal-stack" aria-controls="tab-installation-infrastructure-metal-stack" ><label for="tab-installation-infrastructure-metal-stack">metal-stack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Nutanix" aria-controls="tab-installation-infrastructure-Nutanix" ><label for="tab-installation-infrastructure-Nutanix">Nutanix</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OCI" aria-controls="tab-installation-infrastructure-OCI" ><label for="tab-installation-infrastructure-OCI">OCI</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OpenNebula" aria-controls="tab-installation-infrastructure-OpenNebula" ><label for="tab-installation-infrastructure-OpenNebula">OpenNebula</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OpenStack" aria-controls="tab-installation-infrastructure-OpenStack" ><label for="tab-installation-infrastructure-OpenStack">OpenStack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Outscale" aria-controls="tab-installation-infrastructure-Outscale" ><label for="tab-installation-infrastructure-Outscale">Outscale</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Proxmox" aria-controls="tab-installation-infrastructure-Proxmox" ><label for="tab-installation-infrastructure-Proxmox">Proxmox</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Scaleway" aria-controls="tab-installation-infrastructure-Scaleway" ><label for="tab-installation-infrastructure-Scaleway">Scaleway</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-VCD" aria-controls="tab-installation-infrastructure-VCD" ><label for="tab-installation-infrastructure-VCD">VCD</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-vcluster" aria-controls="tab-installation-infrastructure-vcluster" ><label for="tab-installation-infrastructure-vcluster">vcluster</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Virtink" aria-controls="tab-installation-infrastructure-Virtink" ><label for="tab-installation-infrastructure-Virtink">Virtink</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-vSphere" aria-controls="tab-installation-infrastructure-vSphere" ><label for="tab-installation-infrastructure-vSphere">vSphere</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Vultr" aria-controls="tab-installation-infrastructure-Vultr" ><label for="tab-installation-infrastructure-Vultr">Vultr</label><div class="tab-panels">
<section id="tab-Akamai (Linode)" class="tab-panel">
<pre><code class="language-bash">export LINODE_TOKEN=&lt;your-access-token&gt;

# Initialize the management cluster
clusterctl init --infrastructure linode-linode
</code></pre>
</section>
<section id="tab-AWS" class="tab-panel">
<p>Download the latest binary of <code>clusterawsadm</code> from the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases">AWS provider releases</a>. The <a href="https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm.html">clusterawsadm</a> command line utility assists with identity and access management (IAM) for <a href="https://cluster-api-aws.sigs.k8s.io">Cluster API Provider AWS</a>.</p>
<div id="install-clusterawsadm" class="tabset"><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-Linux" aria-controls="install-clusterawsadm-Linux" checked><label for="install-clusterawsadm-Linux">Linux</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-macOS" aria-controls="install-clusterawsadm-macOS" ><label for="install-clusterawsadm-macOS">macOS</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-homebrew" aria-controls="install-clusterawsadm-homebrew" ><label for="install-clusterawsadm-homebrew">homebrew</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-Windows" aria-controls="install-clusterawsadm-Windows" ><label for="install-clusterawsadm-Windows">Windows</label><div class="tab-panels">
<section id="tab-Linux" class="tab-panel">
<p>Download the latest release; on Linux, type:</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-linux-amd64 -o clusterawsadm
</code></pre>
<p>Make it executable</p>
<pre><code>chmod +x clusterawsadm
</code></pre>
<p>Move the binary to a directory present in your PATH</p>
<pre><code>sudo mv clusterawsadm /usr/local/bin
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-macOS" class="tab-panel">
<p>Download the latest release; on macOs, type:</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-darwin-amd64 -o clusterawsadm
</code></pre>
<p>Or if your Mac has an M1 CPU (”Apple Silicon”):</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-darwin-arm64 -o clusterawsadm
</code></pre>
<p>Make it executable</p>
<pre><code>chmod +x clusterawsadm
</code></pre>
<p>Move the binary to a directory present in your PATH</p>
<pre><code>sudo mv clusterawsadm /usr/local/bin
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-homebrew" class="tab-panel">
<p>Install the latest release using homebrew:</p>
<pre><code>brew install clusterawsadm
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-Windows" class="tab-panel">
<p>Download the latest release; on Windows, type:</p>
<pre><code>curl.exe -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-windows-amd64.exe -o clusterawsadm.exe
</code></pre>
<p>Append or prepend the path of that directory to the <code>PATH</code> environment variable.
Check version to confirm installation</p>
<pre><code>clusterawsadm.exe version
</code></pre>
<p><strong>Example Usage in Powershell</strong></p>
<pre><code class="language-bash">$Env:AWS_REGION=&quot;us-east-1&quot; # This is used to help encode your environment variables
$Env:AWS_ACCESS_KEY_ID=&quot;&lt;your-access-key&gt;&quot;
$Env:AWS_SECRET_ACCESS_KEY=&quot;&lt;your-secret-access-key&gt;&quot;
$Env:AWS_SESSION_TOKEN=&quot;&lt;session-token&gt;&quot; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
$Env:AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
</div></div>
<p>See the <a href="https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites.html">AWS provider prerequisites</a> document for more details.</p>
</section>
<section id="tab-Azure" class="tab-panel">
<p>For more information about authorization, AAD, or requirements for Azure, visit the <a href="https://capz.sigs.k8s.io/getting-started-with-aks.html#prerequisites">Azure provider prerequisites</a> document.</p>
<pre><code class="language-bash">export AZURE_SUBSCRIPTION_ID=&quot;&lt;SubscriptionId&gt;&quot;

# Create an Azure Service Principal and paste the output here
export AZURE_TENANT_ID=&quot;&lt;Tenant&gt;&quot;
export AZURE_CLIENT_ID=&quot;&lt;AppId&gt;&quot;
export AZURE_CLIENT_ID_USER_ASSIGNED_IDENTITY=$AZURE_CLIENT_ID # for compatibility with CAPZ v1.16 templates
export AZURE_CLIENT_SECRET=&quot;&lt;Password&gt;&quot;

# Settings needed for AzureClusterIdentity used by the AzureCluster
export AZURE_CLUSTER_IDENTITY_SECRET_NAME=&quot;cluster-identity-secret&quot;
export CLUSTER_IDENTITY_NAME=&quot;cluster-identity&quot;
export AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE=&quot;default&quot;

# Create a secret to include the password of the Service Principal identity created in Azure
# This secret will be referenced by the AzureClusterIdentity used by the AzureCluster
kubectl create secret generic &quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAME}&quot; --from-literal=clientSecret=&quot;${AZURE_CLIENT_SECRET}&quot; --namespace &quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}&quot;

# Finally, initialize the management cluster
clusterctl init --infrastructure azure
</code></pre>
</section>
<section id="tab-CloudStack" class="tab-panel">
<p>Create a file named cloud-config in the repo’s root directory, substituting in your own environment’s values</p>
<pre><code class="language-bash">[Global]
api-url = &lt;cloudstackApiUrl&gt;
api-key = &lt;cloudstackApiKey&gt;
secret-key = &lt;cloudstackSecretKey&gt;
</code></pre>
<p>Create the base64 encoded credentials by catting your credentials file.
This command uses your environment variables and encodes
them in a value to be stored in a Kubernetes Secret.</p>
<pre><code class="language-bash">export CLOUDSTACK_B64ENCODED_SECRET=`cat cloud-config | base64 | tr -d '\n'`
</code></pre>
<p>Finally, initialize the management cluster</p>
<pre><code class="language-bash">clusterctl init --infrastructure cloudstack
</code></pre>
</section>
<section id="tab-DigitalOcean" class="tab-panel">
<pre><code class="language-bash">export DIGITALOCEAN_ACCESS_TOKEN=&lt;your-access-token&gt;
export DO_B64ENCODED_CREDENTIALS=&quot;$(echo -n &quot;${DIGITALOCEAN_ACCESS_TOKEN}&quot; | base64 | tr -d '\n')&quot;

# Initialize the management cluster
clusterctl init --infrastructure digitalocean
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-1" id="warning-1">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<p>The Docker provider requires the <code>ClusterTopology</code> and <code>MachinePool</code> features to deploy ClusterClass-based clusters.
We are only supporting ClusterClass-based cluster-templates in this quickstart as ClusterClass makes it possible to
adapt configuration based on Kubernetes version. This is required to install Kubernetes clusters &lt; v1.24 and
for the upgrade from v1.23 to v1.24 as we have to use different cgroupDrivers depending on Kubernetes version.</p>
<pre><code class="language-bash"># Enable the experimental Cluster topology feature.
export CLUSTER_TOPOLOGY=true

# Initialize the management cluster
clusterctl init --infrastructure docker
</code></pre>
</section>
<section id="tab-GCP" class="tab-panel">
<pre><code class="language-bash"># Create the base64 encoded credentials by catting your credentials json.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export GCP_B64ENCODED_CREDENTIALS=$( cat /path/to/gcp-credentials.json | base64 | tr -d '\n' )

# Finally, initialize the management cluster
clusterctl init --infrastructure gcp
</code></pre>
</section>
<section id="tab-Harvester" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure harvester-harvester
</code></pre>
<p>For more information, please visit the <a href="https://github.com/rancher-sandbox/cluster-api-provider-harvester">Harvester project</a>.</p>
</section>
<section id="tab-Hetzner" class="tab-panel">
<p>Please visit the <a href="https://github.com/syself/cluster-api-provider-hetzner">Hetzner project</a>.</p>
</section>
<section id="tab-Hivelocity" class="tab-panel">
<p>Please visit the <a href="https://github.com/hivelocity/cluster-api-provider-hivelocity">Hivelocity project</a>.</p>
</section>
<section id="tab-Huawei" class="tab-panel">
<pre><code class="language-bash"># Please ensure that the values for `CLOUD_SDK_AK` and `CLOUD_SDK_SK` are base64 encoded.
export CLOUD_SDK_AK=$( echo $AccessKey | base64 | tr -d '\n' )
export CLOUD_SDK_SK=$( echo $SecretKey | base64 | tr -d '\n' )

# Finally, initialize the management cluster
clusterctl init --infrastructure huawei
</code></pre>
</section>
<section id="tab-IBM Cloud" class="tab-panel">
<p>In order to initialize the IBM Cloud Provider you have to expose the environment
variable <code>IBMCLOUD_API_KEY</code>. This variable is used to authorize the infrastructure
provider manager against the IBM Cloud API. To create one from the UI, refer <a href="https://cloud.ibm.com/docs/account?topic=account-userapikey&amp;interface=ui#create_user_key">here</a>.</p>
<pre><code class="language-bash">export IBMCLOUD_API_KEY=&lt;you_api_key&gt;

# Finally, initialize the management cluster
clusterctl init --infrastructure ibmcloud
</code></pre>
</section>
<section id="tab-IONOS Cloud" class="tab-panel">
<p>The IONOS Cloud credentials are configured in the <code>IONOSCloudCluster</code>.
Therefore, there is no need to specify them during the provider initialization.</p>
<pre><code class="language-bash">clusterctl init --infrastructure ionoscloud-ionoscloud
</code></pre>
<p>For more information, please visit the <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud">IONOS Cloud project</a>.</p>
</section>
<section id="tab-K0smotron" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure k0sproject-k0smotron
</code></pre>
</section>
<section id="tab-KubeKey" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure kubekey
</code></pre>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt/">KubeVirt project</a> for more information.</p>
<p>As described above, we want to use a LoadBalancer service in order to expose the workload cluster’s API server. In the
example below, we will use <a href="https://metallb.universe.tf/">MetalLB</a> solution to implement load balancing to our kind
cluster. Other solution should work as well.</p>
<h4><a class="header" href="#install-metallb-for-load-balancing" id="install-metallb-for-load-balancing">Install MetalLB for load balancing</a></h4>
<p>Install MetalLB, as described <a href="https://metallb.universe.tf/installation/#installation-by-manifest">here</a>; for example:</p>
<pre><code class="language-bash">METALLB_VER=$(curl &quot;https://api.github.com/repos/metallb/metallb/releases/latest&quot; | jq -r &quot;.tag_name&quot;)
kubectl apply -f &quot;https://raw.githubusercontent.com/metallb/metallb/${METALLB_VER}/config/manifests/metallb-native.yaml&quot;
kubectl wait pods -n metallb-system -l app=metallb,component=controller --for=condition=Ready --timeout=10m
kubectl wait pods -n metallb-system -l app=metallb,component=speaker --for=condition=Ready --timeout=2m
</code></pre>
<p>Now, we’ll create the <code>IPAddressPool</code> and the <code>L2Advertisement</code> custom resources. For that, we’ll need to set the IP
range. First, we’ll read the <code>kind</code> network in order to find its subnet:</p>
<pre><code class="language-bash">SUBNET=$(docker network inspect -f '{{range .IPAM.Config}}{{if .Gateway}}{{.Subnet}}{{end}}{{end}}' kind)
PREFIX=$(echo $SUBNET | sed -E 's|^([0-9]+\.[0-9]+)\..*$|\1|g')

cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: capi-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${PREFIX}.255.200-${PREFIX}.255.250
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: empty
  namespace: metallb-system
EOF
</code></pre>
<aside class="note warning">
<h1><a class="header" href="#notice" id="notice">Notice</a></h1>
<p>The example above is based on the Docker container runtime. The output of <code>docker network inspect</code> may be different when
using another runtime. In such a case, the IPAddressPool’s <code>spec.addresses</code> field should be populated manually,
according to the specific network.</p>
</aside>
<h4><a class="header" href="#install-kubevirt-on-the-kind-cluster" id="install-kubevirt-on-the-kind-cluster">Install KubeVirt on the kind cluster</a></h4>
<pre><code class="language-bash"># get KubeVirt version
KV_VER=$(curl &quot;https://api.github.com/repos/kubevirt/kubevirt/releases/latest&quot; | jq -r &quot;.tag_name&quot;)
# deploy required CRDs
kubectl apply -f &quot;https://github.com/kubevirt/kubevirt/releases/download/${KV_VER}/kubevirt-operator.yaml&quot;
# deploy the KubeVirt custom resource
kubectl apply -f &quot;https://github.com/kubevirt/kubevirt/releases/download/${KV_VER}/kubevirt-cr.yaml&quot;
kubectl wait -n kubevirt kv kubevirt --for=condition=Available --timeout=10m
</code></pre>
<h4><a class="header" href="#initialize-the-management-cluster-with-the-kubevirt-provider" id="initialize-the-management-cluster-with-the-kubevirt-provider">Initialize the management cluster with the KubeVirt Provider</a></h4>
<pre><code class="language-bash">clusterctl init --infrastructure kubevirt
</code></pre>
</section>
<section id="tab-Metal3" class="tab-panel">
<p>Please visit the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/">Metal3 project</a>.</p>
</section>
<section id="tab-metal-stack" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure metal-stack
</code></pre>
<p>Please follow the Cluster API Provider for <a href="https://metal-stack.io/docs/references/cluster-api-provider-metal-stack#getting-started">metal-stack Getting Started Guide</a></p>
</section>
<section id="tab-Nutanix" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://opendocs.nutanix.com/capx/latest/getting_started/">Nutanix Getting Started Guide</a></p>
</section>
<section id="tab-OCI" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://oracle.github.io/cluster-api-provider-oci/#getting-started">Oracle Cloud Infrastructure (OCI) Getting Started Guide</a></p>
</section>
<section id="tab-OpenNebula" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure opennebula
</code></pre>
<p>Please visit <a href="https://github.com/OpenNebula/cluster-api-provider-opennebula/wiki">OpenNebula Cluster API Provider Wiki</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>Cluster API Provider OpenStack depends on <a href="https://k-orc.cloud/">openstack-resource-controller</a> since v0.12.</p>
<pre><code class="language-bash"># Install ORC (needed for CAPO &gt;=v0.12)
kubectl apply -f https://github.com/k-orc/openstack-resource-controller/releases/latest/download/install.yaml
# Initialize the management cluster
clusterctl init --infrastructure openstack
</code></pre>
</section>
<section id="tab-Outscale" class="tab-panel">
<pre><code class="language-bash">export OSC_SECRET_KEY=&lt;your-secret-key&gt;
export OSC_ACCESS_KEY=&lt;your-access-key&gt;
export OSC_REGION=&lt;you-region&gt;
# Create namespace
kubectl create namespace cluster-api-provider-outscale-system
# Create secret
kubectl create secret generic cluster-api-provider-outscale --from-literal=access_key=${OSC_ACCESS_KEY} --from-literal=secret_key=${OSC_SECRET_KEY} --from-literal=region=${OSC_REGION}  -n cluster-api-provider-outscale-system
# Initialize the management cluster
clusterctl init --infrastructure outscale
</code></pre>
</section>
<section id="tab-Proxmox" class="tab-panel">
<p>The Proxmox credentials are optional, when creating a cluster they can be set in the <code>ProxmoxCluster</code> resource,
if you do not set them here.</p>
<pre><code class="language-bash"># The host for the Proxmox cluster
export PROXMOX_URL=&quot;https://pve.example:8006&quot;
# The Proxmox token ID to access the remote Proxmox endpoint
export PROXMOX_TOKEN='root@pam!capi'
# The secret associated with the token ID
# You may want to set this in `$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml` so your password is not in
# bash history
export PROXMOX_SECRET=&quot;1234-1234-1234-1234&quot;


# Finally, initialize the management cluster
clusterctl init --infrastructure proxmox --ipam in-cluster
</code></pre>
<p>For more information about the CAPI provider for Proxmox, see the <a href="https://github.com/ionos-cloud/cluster-api-provider-proxmox/blob/main/docs/Usage.md">Proxmox
project</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure scaleway
</code></pre>
</section>
<section id="tab-VCD" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://github.com/vmware/cluster-api-provider-cloud-director/blob/main/README.md">Cloud Director Getting Started Guide</a></p>
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure vcd
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure vcluster
</code></pre>
<p>Please follow the Cluster API Provider for <a href="https://github.com/loft-sh/cluster-api-provider-vcluster/blob/main/docs/quick-start.md">vcluster Quick Start Guide</a></p>
</section>
<section id="tab-Virtink" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure virtink
</code></pre>
</section>
<section id="tab-vSphere" class="tab-panel">
<pre><code class="language-bash"># The username used to access the remote vSphere endpoint
export VSPHERE_USERNAME=&quot;vi-admin@vsphere.local&quot;
# The password used to access the remote vSphere endpoint
# You may want to set this in `$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml` so your password is not in
# bash history
export VSPHERE_PASSWORD=&quot;admin!23&quot;

# Finally, initialize the management cluster
clusterctl init --infrastructure vsphere
</code></pre>
<p>For more information about prerequisites, credentials management, or permissions for vSphere, see the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md">vSphere
project</a>.</p>
</section>
<section id="tab-Vultr" class="tab-panel">
<pre><code class="language-bash">export VULTR_API_KEY=&quot;$(echo -n &quot;${VULTR_API_KEY}&quot; | base64 | tr -d '\n')&quot;

# initialize the management cluster
clusterctl init --infrastructure vultr-vultr
</code></pre>
</section>
</div></div>
<p>The output of <code>clusterctl init</code> is similar to this:</p>
<pre><code class="language-bash">Fetching providers
Installing cert-manager Version=&quot;v1.11.0&quot;
Waiting for cert-manager to be available...
Installing Provider=&quot;cluster-api&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-system&quot;
Installing Provider=&quot;bootstrap-kubeadm&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-kubeadm-bootstrap-system&quot;
Installing Provider=&quot;control-plane-kubeadm&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-kubeadm-control-plane-system&quot;
Installing Provider=&quot;infrastructure-docker&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capd-system&quot;

Your management cluster has been initialized successfully!

You can now create your first workload cluster by running the following:

  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -
</code></pre>
<aside class="note">
<h1><a class="header" href="#alternatives-to-environment-variables" id="alternatives-to-environment-variables">Alternatives to environment variables</a></h1>
<p>Throughout this quickstart guide we’ve given instructions on setting parameters using environment variables. For most
environment variables in the rest of the guide, you can also set them in <code>$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml</code></p>
<p>See <a href="../clusterctl/commands/init.html"><code>clusterctl init</code></a> for more details.</p>
</aside>
<h3><a class="header" href="#create-your-first-workload-cluster" id="create-your-first-workload-cluster">Create your first workload cluster</a></h3>
<p>Once the management cluster is ready, you can create your first workload cluster.</p>
<h4><a class="header" href="#preparing-the-workload-cluster-configuration" id="preparing-the-workload-cluster-configuration">Preparing the workload cluster configuration</a></h4>
<p>The <code>clusterctl generate cluster</code> command returns a YAML template for creating a <a href="../reference/glossary.html#workload-cluster">workload cluster</a>.</p>
<aside class="note">
<h1><a class="header" href="#which-provider-will-be-used-for-my-cluster" id="which-provider-will-be-used-for-my-cluster"> Which provider will be used for my cluster? </a></h1>
<p>The <code>clusterctl generate cluster</code> command uses smart defaults in order to simplify the user experience; for example,
if only the <code>aws</code> infrastructure provider is deployed, it detects and uses that when creating the cluster.</p>
</aside>
<aside class="note">
<h1><a class="header" href="#what-topology-will-be-used-for-my-cluster" id="what-topology-will-be-used-for-my-cluster"> What topology will be used for my cluster? </a></h1>
<p>The <code>clusterctl generate cluster</code> command by default uses cluster templates which are provided by the infrastructure
providers. See the provider’s documentation for more information.</p>
<p>See the <code>clusterctl generate cluster</code> <a href="../clusterctl/commands/generate-cluster.html">command</a> documentation for
details about how to use alternative sources. for cluster templates.</p>
</aside>
<h4><a class="header" href="#required-configuration-for-common-providers" id="required-configuration-for-common-providers">Required configuration for common providers</a></h4>
<p>Depending on the infrastructure provider you are planning to use, some additional prerequisites should be satisfied
before configuring a cluster with Cluster API. Instructions are provided for common providers below.</p>
<p>Otherwise, you can look at the <code>clusterctl generate cluster</code> <a href="../clusterctl/commands/generate-cluster.html">command</a> documentation for details about how to
discover the list of variables required by a cluster templates.</p>
<div id="tab-configuration-infrastructure" class="tabset"><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Akamai (Linode)" aria-controls="tab-configuration-infrastructure-Akamai (Linode)" checked><label for="tab-configuration-infrastructure-Akamai (Linode)">Akamai (Linode)</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-AWS" aria-controls="tab-configuration-infrastructure-AWS" ><label for="tab-configuration-infrastructure-AWS">AWS</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Azure" aria-controls="tab-configuration-infrastructure-Azure" ><label for="tab-configuration-infrastructure-Azure">Azure</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-CloudStack" aria-controls="tab-configuration-infrastructure-CloudStack" ><label for="tab-configuration-infrastructure-CloudStack">CloudStack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-DigitalOcean" aria-controls="tab-configuration-infrastructure-DigitalOcean" ><label for="tab-configuration-infrastructure-DigitalOcean">DigitalOcean</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Docker" aria-controls="tab-configuration-infrastructure-Docker" ><label for="tab-configuration-infrastructure-Docker">Docker</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-GCP" aria-controls="tab-configuration-infrastructure-GCP" ><label for="tab-configuration-infrastructure-GCP">GCP</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Harvester" aria-controls="tab-configuration-infrastructure-Harvester" ><label for="tab-configuration-infrastructure-Harvester">Harvester</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Huawei" aria-controls="tab-configuration-infrastructure-Huawei" ><label for="tab-configuration-infrastructure-Huawei">Huawei</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-IBM Cloud" aria-controls="tab-configuration-infrastructure-IBM Cloud" ><label for="tab-configuration-infrastructure-IBM Cloud">IBM Cloud</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-IONOS Cloud" aria-controls="tab-configuration-infrastructure-IONOS Cloud" ><label for="tab-configuration-infrastructure-IONOS Cloud">IONOS Cloud</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-K0smotron" aria-controls="tab-configuration-infrastructure-K0smotron" ><label for="tab-configuration-infrastructure-K0smotron">K0smotron</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-KubeKey" aria-controls="tab-configuration-infrastructure-KubeKey" ><label for="tab-configuration-infrastructure-KubeKey">KubeKey</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-KubeVirt" aria-controls="tab-configuration-infrastructure-KubeVirt" ><label for="tab-configuration-infrastructure-KubeVirt">KubeVirt</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Metal3" aria-controls="tab-configuration-infrastructure-Metal3" ><label for="tab-configuration-infrastructure-Metal3">Metal3</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-metal-stack" aria-controls="tab-configuration-infrastructure-metal-stack" ><label for="tab-configuration-infrastructure-metal-stack">metal-stack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Nutanix" aria-controls="tab-configuration-infrastructure-Nutanix" ><label for="tab-configuration-infrastructure-Nutanix">Nutanix</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-OpenNebula" aria-controls="tab-configuration-infrastructure-OpenNebula" ><label for="tab-configuration-infrastructure-OpenNebula">OpenNebula</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-OpenStack" aria-controls="tab-configuration-infrastructure-OpenStack" ><label for="tab-configuration-infrastructure-OpenStack">OpenStack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Outscale" aria-controls="tab-configuration-infrastructure-Outscale" ><label for="tab-configuration-infrastructure-Outscale">Outscale</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Proxmox" aria-controls="tab-configuration-infrastructure-Proxmox" ><label for="tab-configuration-infrastructure-Proxmox">Proxmox</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Scaleway" aria-controls="tab-configuration-infrastructure-Scaleway" ><label for="tab-configuration-infrastructure-Scaleway">Scaleway</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Tinkerbell" aria-controls="tab-configuration-infrastructure-Tinkerbell" ><label for="tab-configuration-infrastructure-Tinkerbell">Tinkerbell</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-VCD" aria-controls="tab-configuration-infrastructure-VCD" ><label for="tab-configuration-infrastructure-VCD">VCD</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-vcluster" aria-controls="tab-configuration-infrastructure-vcluster" ><label for="tab-configuration-infrastructure-vcluster">vcluster</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Virtink" aria-controls="tab-configuration-infrastructure-Virtink" ><label for="tab-configuration-infrastructure-Virtink">Virtink</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-vSphere" aria-controls="tab-configuration-infrastructure-vSphere" ><label for="tab-configuration-infrastructure-vSphere">vSphere</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Vultr" aria-controls="tab-configuration-infrastructure-Vultr" ><label for="tab-configuration-infrastructure-Vultr">Vultr</label><div class="tab-panels">
<section id="tab-Akamai (Linode)" class="tab-panel">
<pre><code class="language-bash">export LINODE_REGION=us-ord
export LINODE_TOKEN=&lt;your linode PAT&gt;
export LINODE_CONTROL_PLANE_MACHINE_TYPE=g6-standard-2
export LINODE_MACHINE_TYPE=g6-standard-2
</code></pre>
<p>See the <a href="https://linode.github.io/cluster-api-provider-linode/introduction.html">Akamai (Linode) provider</a> for more information.</p>
</section>
<section id="tab-AWS" class="tab-panel">
<pre><code class="language-bash">export AWS_REGION=us-east-1
export AWS_SSH_KEY_NAME=default
# Select instance types
export AWS_CONTROL_PLANE_MACHINE_TYPE=t3.large
export AWS_NODE_MACHINE_TYPE=t3.large
</code></pre>
<p>See the <a href="https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites.html">AWS provider prerequisites</a> document for more details.</p>
</section>
<section id="tab-Azure" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-2" id="warning-2">Warning</a></h1>
<p>Make sure you choose a VM size which is available in the desired location for your subscription. To see available SKUs, use <code>az vm list-skus -l &lt;your_location&gt; -r virtualMachines -o table</code></p>
</aside>
<pre><code class="language-bash"># Name of the Azure datacenter location. Change this value to your desired location.
export AZURE_LOCATION=&quot;centralus&quot;

# Select VM types.
export AZURE_CONTROL_PLANE_MACHINE_TYPE=&quot;Standard_D2s_v3&quot;
export AZURE_NODE_MACHINE_TYPE=&quot;Standard_D2s_v3&quot;

# [Optional] Select resource group. The default value is ${CLUSTER_NAME}.
export AZURE_RESOURCE_GROUP=&quot;&lt;ResourceGroupName&gt;&quot;
</code></pre>
</section>
<section id="tab-CloudStack" class="tab-panel">
<p>A Cluster API compatible image must be available in your CloudStack installation. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/providers/cloudstack.html">image-builder (CloudStack)</a></p>
<p>Prebuilt images can be found <a href="http://packages.shapeblue.com/cluster-api-provider-cloudstack/images/">here</a></p>
<p>To see all required CloudStack environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure cloudstack --list-variables capi-quickstart
</code></pre>
<p>Apart from the script, the following CloudStack environment variables are required.</p>
<pre><code class="language-bash"># Set this to the name of the zone in which to deploy the cluster
export CLOUDSTACK_ZONE_NAME=&lt;zone name&gt;
# The name of the network on which the VMs will reside
export CLOUDSTACK_NETWORK_NAME=&lt;network name&gt;
# The endpoint of the workload cluster
export CLUSTER_ENDPOINT_IP=&lt;cluster endpoint address&gt;
export CLUSTER_ENDPOINT_PORT=&lt;cluster endpoint port&gt;
# The service offering of the control plane nodes
export CLOUDSTACK_CONTROL_PLANE_MACHINE_OFFERING=&lt;control plane service offering name&gt;
# The service offering of the worker nodes
export CLOUDSTACK_WORKER_MACHINE_OFFERING=&lt;worker node service offering name&gt;
# The capi compatible template to use
export CLOUDSTACK_TEMPLATE_NAME=&lt;template name&gt;
# The ssh key to use to log into the nodes
export CLOUDSTACK_SSH_KEY_NAME=&lt;ssh key name&gt;

</code></pre>
<p>A full configuration reference can be found in <a href="https://github.com/kubernetes-sigs/cluster-api-provider-cloudstack/blob/master/docs/book/src/clustercloudstack/configuration.md">configuration.md</a>.</p>
</section>
<section id="tab-DigitalOcean" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your DigitalOcean account. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash">export DO_REGION=nyc1
export DO_SSH_KEY_FINGERPRINT=&lt;your-ssh-key-fingerprint&gt;
export DO_CONTROL_PLANE_MACHINE_TYPE=s-2vcpu-2gb
export DO_CONTROL_PLANE_MACHINE_IMAGE=&lt;your-capi-image-id&gt;
export DO_NODE_MACHINE_TYPE=s-2vcpu-2gb
export DO_NODE_MACHINE_IMAGE==&lt;your-capi-image-id&gt;
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-3" id="warning-3">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<p>The Docker provider does not require additional configurations for cluster templates.</p>
<p>However, if you require special network settings you can set the following environment variables:</p>
<pre><code class="language-bash"># The list of service CIDR, default [&quot;10.128.0.0/12&quot;]
export SERVICE_CIDR=[&quot;10.96.0.0/12&quot;]

# The list of pod CIDR, default [&quot;192.168.0.0/16&quot;]
export POD_CIDR=[&quot;192.168.0.0/16&quot;]

# The service domain, default &quot;cluster.local&quot;
export SERVICE_DOMAIN=&quot;k8s.test&quot;
</code></pre>
<p>It is also possible but <strong>not recommended</strong> to disable the per-default enabled <a href="../security/pod-security-standards.html">Pod Security Standard</a>:</p>
<pre><code class="language-bash">export POD_SECURITY_STANDARD_ENABLED=&quot;false&quot;
</code></pre>
</section>
<section id="tab-GCP" class="tab-panel">
<pre><code class="language-bash"># Name of the GCP datacenter location. Change this value to your desired location
export GCP_REGION=&quot;&lt;GCP_REGION&gt;&quot;
export GCP_PROJECT=&quot;&lt;GCP_PROJECT&gt;&quot;
# Make sure to use same Kubernetes version here as building the GCE image
export KUBERNETES_VERSION=1.23.3
# This is the image you built. See https://github.com/kubernetes-sigs/image-builder
export IMAGE_ID=projects/$GCP_PROJECT/global/images/&lt;built image&gt;
export GCP_CONTROL_PLANE_MACHINE_TYPE=n1-standard-2
export GCP_NODE_MACHINE_TYPE=n1-standard-2
export GCP_NETWORK_NAME=&lt;GCP_NETWORK_NAME or default&gt;
export CLUSTER_NAME=&quot;&lt;CLUSTER_NAME&gt;&quot;
</code></pre>
<p>See the <a href="https://cluster-api-gcp.sigs.k8s.io/">GCP provider</a> for more information.</p>
</section>
<section id="tab-Harvester" class="tab-panel">
<pre><code class="language-bash"># Cloud Provider credentials, which are a Kubeconfig generated using this process: https://docs.harvesterhci.io/v1.3/rancher/cloud-provider/#deploying-to-the-rke2-custom-cluster-experimental
# Since v0.1.5, this can be left &quot;&quot;, because the controller can update it automatically
export CLOUD_CONFIG_KUBECONFIG_B64=&quot;&quot;
# Name of the CAPI Cluster
export CLUSTER_NAME=&quot;&lt;CLUSTER_NAME&gt;&quot;
# Number of Control Plane machines
export CONTROL_PLANE_MACHINE_COUNT=3
# URL to access the Harvester Cluster, this will be overridden by the controller
export HARVESTER_ENDPOINT=&quot;&quot;
# Base64-Encoded Kubeconfig to access Harvester, which can be downloaded from Harvester's UI or from a Harvester Manager Node.
export HARVESTER_KUBECONFIG_B64=&quot;&lt;HARVESTER_KUBECONFIG_ENCODED_IN_BASE64&gt;&quot;
# Namespace for all resources in the Management Cluster
export NAMESPACE=&quot;test&quot;
# Pod CIDR for the Workload Cluster, it should have the format: 192.168.0.0/16
export POD_CIDR=&quot;10.42.0.0/16&quot;
# Service CIDR for the Workload Cluster, it should have the format : 192.168.0.0/16 and be different from POD_CIDR
export SERVICE_CIDR=&quot;10.43.0.0/16&quot;
# Reference to SSH Keypair in Harvester. It should follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export SSH_KEYPAIR=&quot;default/ssk-key-pair&quot;
# Namespace in Harvester where the VMs will be created.
export TARGET_HARVESTER_NAMESPACE=&quot;default&quot;
# Disk Size to be used by the VMs
export VM_DISK_SIZE=&quot;50Gi&quot;
# Reference to OS Image in Harvester which will be used for creating VMs, It must follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export VM_IMAGE_NAME=&quot;default/jammy-server&quot;
# Reference to VM Network in Harvester. It must follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export VM_NETWORK=&quot;default/untagged&quot;
# Linux Username for the VMs
export VM_SSH_USER=&quot;ubuntu&quot;
# Number of Worker nodes in the target Workload cluster
export WORKER_MACHINE_COUNT=2
</code></pre>
<p>See the <a href="https://github.com/rancher-sandbox/cluster-api-provider-harvester">Harvester provider</a> for more information.</p>
</section>
<section id="tab-Huawei" class="tab-panel">
<pre><code class="language-bash"># huawei cloud region
export HC_REGION=&quot;cn-east-1&quot;
# ECS SSH key name
export HC_SSH_KEY_NAME=&quot;default&quot;
# kubernetes version
export KUBERNETES_VERSION=&quot;1.32.0&quot;
# number of control plane machines
export CONTROL_PLANE_MACHINE_COUNT=&quot;1&quot;
# number of worker machines
export WORKER_MACHINE_COUNT=&quot;1&quot;
# control plane machine type
export HC_CONTROL_PLANE_MACHINE_TYPE=&quot;x1e.2u.4g&quot;
# worker node machine type
export HC_NODE_MACHINE_TYPE=&quot;x1e.2u.4g&quot;
# ECS image ID
export ECS_IMAGE_ID=&quot;218ca5t7-bxf3-5dg0-852p-y703c9fe1a52&quot;
</code></pre>
<p>See the <a href="https://github.com/HuaweiCloudDeveloper/cluster-api-provider-huawei">Huawei Cloud provider</a> for more information.</p>
</section>
<section id="tab-IBM Cloud" class="tab-panel">
<pre><code class="language-bash"># Required environment variables for VPC
# VPC region
export IBMVPC_REGION=us-south
# VPC zone within the region
export IBMVPC_ZONE=us-south-1
# ID of the resource group in which the VPC will be created
export IBMVPC_RESOURCEGROUP=&lt;your-resource-group-id&gt;
# Name of the VPC
export IBMVPC_NAME=ibm-vpc-0
export IBMVPC_IMAGE_ID=&lt;you-image-id&gt;
# Profile for the virtual server instances
export IBMVPC_PROFILE=bx2-4x16
export IBMVPC_SSHKEY_ID=&lt;your-sshkey-id&gt;

# Required environment variables for PowerVS
export IBMPOWERVS_SSHKEY_NAME=&lt;your-ssh-key&gt;
# Internal and external IP of the network
export IBMPOWERVS_VIP=&lt;internal-ip&gt;
export IBMPOWERVS_VIP_EXTERNAL=&lt;external-ip&gt;
export IBMPOWERVS_VIP_CIDR=29
export IBMPOWERVS_IMAGE_NAME=&lt;your-capi-image-name&gt;
# ID of the PowerVS service instance
export IBMPOWERVS_SERVICE_INSTANCE_ID=&lt;service-instance-id&gt;
export IBMPOWERVS_NETWORK_NAME=&lt;your-capi-network-name&gt;
</code></pre>
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-ibmcloud">IBM Cloud provider</a> for more information.</p>
</section>
<section id="tab-IONOS Cloud" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your IONOS Cloud contract.
For instructions on how to build a compatible Image, see <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud/blob/main/docs/custom-image.md">our docs</a>.</p>
<pre><code class="language-bash"># The token which is used to authenticate against the IONOS Cloud API
export IONOS_TOKEN=&lt;your-token&gt;
# The datacenter ID where the cluster will be deployed
export IONOSCLOUD_DATACENTER_ID=&quot;&lt;your-datacenter-id&gt;&quot;
# The IP of the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=10.10.10.4
# The location of the data center where the cluster will be deployed
export CONTROL_PLANE_ENDPOINT_LOCATION=de/txl
# The image ID of the custom image that will be used for the VMs
export IONOSCLOUD_MACHINE_IMAGE_ID=&quot;&lt;your-image-id&gt;&quot;
# The SSH key that will be used to access the VMs
export IONOSCLOUD_MACHINE_SSH_KEYS=&quot;&lt;your-ssh-key&gt;&quot;
</code></pre>
<p>For more configuration options check our list of <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud/blob/main/docs/quickstart.md#environment-variables">available variables</a></p>
</section>
<section id="tab-K0smotron" class="tab-panel">
<p>Please visit the <a href="https://github.com/k0sproject/k0smotron">K0smotron provider</a> for more information.</p>
</section>
<section id="tab-KubeKey" class="tab-panel">
<pre><code class="language-bash"># Required environment variables
# The KKZONE is used to specify where to download the binaries. (e.g. &quot;&quot;, &quot;cn&quot;)
export KKZONE=&quot;&quot;
# The ssh name of the all instance Linux user. (e.g. root, ubuntu)
export USER_NAME=&lt;your-linux-user&gt;
# The ssh password of the all instance Linux user.
export PASSWORD=&lt;your-linux-user-password&gt;
# The ssh IP address of the all instance. (e.g. &quot;[{address: 192.168.100.3}, {address: 192.168.100.4}]&quot;)
export INSTANCES=&lt;your-linux-ip-address&gt;
# The cluster control plane VIP. (e.g. &quot;192.168.100.100&quot;)
export CONTROL_PLANE_ENDPOINT_IP=&lt;your-control-plane-virtual-ip&gt;
</code></pre>
<p>Please visit the <a href="https://github.com/kubesphere/kubekey">KubeKey provider</a> for more information.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>In this example, we’ll use the image for Kubernetes v1.32.1:</p>
<pre><code class="language-bash">export NODE_VM_IMAGE_TEMPLATE=&quot;quay.io/capk/ubuntu-2404-container-disk:v1.32.1&quot;
export CAPK_GUEST_K8S_VERSION=&quot;${NODE_VM_IMAGE_TEMPLATE/*:/}&quot;
export CRI_PATH=&quot;unix:///var/run/containerd/containerd.sock&quot;
</code></pre>
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt/">KubeVirt project</a> for more information.</p>
<aside class="note">
<h1><a class="header" href="#note" id="note">Note</a></h1>
<p>Find additional images under <a href="https://quay.io/capk/ubuntu-2404-container-disk">quay.io/capk/ubuntu-2404-container-disk</a>,
<a href="https://quay.io/capk/ubuntu-2204-container-disk">quay.io/capk/ubuntu-2204-container-disk</a>,
or <a href="https://quay.io/capk/ubuntu-2004-container-disk">quay.io/capk/ubuntu-2004-container-disk</a>.</p>
<p>Alternatively, create your own image; see <a href="https://github.com/kubernetes-sigs/image-builder">here</a>.</p>
</aside>
</section>
<section id="tab-Metal3" class="tab-panel">
<p><strong>Note</strong>: If you are running CAPM3 release prior to v0.5.0, make sure to export the following
environment variables. However, you don’t need them to be exported if you use
CAPM3 release v0.5.0 or higher.</p>
<pre><code class="language-bash"># The URL of the kernel to deploy.
export DEPLOY_KERNEL_URL=&quot;http://172.22.0.1:6180/images/ironic-python-agent.kernel&quot;
# The URL of the ramdisk to deploy.
export DEPLOY_RAMDISK_URL=&quot;http://172.22.0.1:6180/images/ironic-python-agent.initramfs&quot;
# The URL of the Ironic endpoint.
export IRONIC_URL=&quot;http://172.22.0.1:6385/v1/&quot;
# The URL of the Ironic inspector endpoint.
export IRONIC_INSPECTOR_URL=&quot;http://172.22.0.1:5050/v1/&quot;
# Do not use a dedicated CA certificate for Ironic API. Any value provided in this variable disables additional CA certificate validation.
# To provide a CA certificate, leave this variable unset. If unset, then IRONIC_CA_CERT_B64 must be set.
export IRONIC_NO_CA_CERT=true
# Disables basic authentication for Ironic API. Any value provided in this variable disables authentication.
# To enable authentication, leave this variable unset. If unset, then IRONIC_USERNAME and IRONIC_PASSWORD must be set.
export IRONIC_NO_BASIC_AUTH=true
# Disables basic authentication for Ironic inspector API. Any value provided in this variable disables authentication.
# To enable authentication, leave this variable unset. If unset, then IRONIC_INSPECTOR_USERNAME and IRONIC_INSPECTOR_PASSWORD must be set.
export IRONIC_INSPECTOR_NO_BASIC_AUTH=true
</code></pre>
<p>Please visit the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/docs/getting-started.md">Metal3 getting started guide</a> for more details.</p>
</section>
<section id="tab-metal-stack" class="tab-panel">
<pre><code class="language-bash">export METAL_PARTITION=&lt;metal-stack-partition&gt;
export METAL_PROJECT_ID=&lt;metal-stack-project-id&gt;
export CONTROL_PLANE_IP=&lt;metal-stack-control-plane-ip&gt;

export FIREWALL_MACHINE_IMAGE=&lt;firewall-os-image&gt;
export FIREWALL_MACHINE_SIZE=&lt;firewall-size&gt;

export CONTROL_PLANE_MACHINE_IMAGE=&lt;machine-os-image&gt;
export CONTROL_PLANE_MACHINE_SIZE=&lt;machine-size&gt;
export WORKER_MACHINE_IMAGE=&lt;machine-os-image&gt;
export WORKER_MACHINE_SIZE=&lt;machine-size&gt;
</code></pre>
<p>Please visit the <a href="https://metal-stack.io/docs/references/cluster-api-provider-metal-stack#getting-started">metal-stack getting started guide</a> for more details.</p>
</section>
<section id="tab-Nutanix" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Nutanix image library. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<p>To see all required Nutanix environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure nutanix --list-variables capi-quickstart
</code></pre>
</section>
<section id="tab-OpenNebula" class="tab-panel">
<pre><code class="language-bash"># OpenNebula API endpoint and credentials
export ONE_XMLRPC='http://10.2.11.40:2633/RPC2'
export ONE_AUTH='oneadmin:opennebula'

# VM and VR templates to construct workload clusters from
export MACHINE_TEMPLATE_NAME='capone131'
export ROUTER_TEMPLATE_NAME='capone131-vr'

# VNs to deploy workload clusters into
export PUBLIC_NETWORK_NAME='service'
export PRIVATE_NETWORK_NAME='private'

# Name of the new workload cluster
export CLUSTER_NAME='one'

# Cloud-Provider image to deploy inside the new workload cluster
export CCM_IMG='ghcr.io/opennebula/cloud-provider-opennebula:latest'

# Initial size of the new workload cluster
export CONTROL_PLANE_MACHINE_COUNT='1'
export WORKER_MACHINE_COUNT='1'
</code></pre>
<p>Please visit <a href="https://github.com/OpenNebula/cluster-api-provider-opennebula/wiki">OpenNebula Cluster API Provider Wiki</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your OpenStack. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.
Depending on your OpenStack and underlying hypervisor the following options might be of interest:</p>
<ul>
<li><a href="https://image-builder.sigs.k8s.io/capi/providers/openstack.html">image-builder (OpenStack)</a></li>
<li><a href="https://image-builder.sigs.k8s.io/capi/providers/vsphere.html">image-builder (vSphere)</a></li>
</ul>
<p>To see all required OpenStack environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure openstack --list-variables capi-quickstart
</code></pre>
<p>The following script can be used to export some of them:</p>
<pre><code class="language-bash">wget https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc -O /tmp/env.rc
source /tmp/env.rc &lt;path/to/clouds.yaml&gt; &lt;cloud&gt;
</code></pre>
<p>Apart from the script, the following OpenStack environment variables are required.</p>
<pre><code class="language-bash"># The list of nameservers for OpenStack Subnet being created.
# Set this value when you need create a new network/subnet while the access through DNS is required.
export OPENSTACK_DNS_NAMESERVERS=&lt;dns nameserver&gt;
# FailureDomain is the failure domain the machine will be created in.
export OPENSTACK_FAILURE_DOMAIN=&lt;availability zone name&gt;
# The flavor reference for the flavor for your server instance.
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=&lt;flavor&gt;
# The flavor reference for the flavor for your server instance.
export OPENSTACK_NODE_MACHINE_FLAVOR=&lt;flavor&gt;
# The name of the image to use for your server instance. If the RootVolume is specified, this will be ignored and use rootVolume directly.
export OPENSTACK_IMAGE_NAME=&lt;image name&gt;
# The SSH key pair name
export OPENSTACK_SSH_KEY_NAME=&lt;ssh key pair name&gt;
# The external network
export OPENSTACK_EXTERNAL_NETWORK_ID=&lt;external network ID&gt;
</code></pre>
<p>A full configuration reference can be found in <a href="https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/master/docs/book/src/clusteropenstack/configuration.md">configuration.md</a>.</p>
</section>
<section id="tab-Outscale" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Outscale account. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash"># The outscale root disk iops
export OSC_IOPS=&quot;&lt;IOPS&gt;&quot;
# The outscale root disk size
export OSC_VOLUME_SIZE=&quot;&lt;VOLUME_SIZE&gt;&quot;
# The outscale root disk volumeType
export OSC_VOLUME_TYPE=&quot;&lt;VOLUME_TYPE&gt;&quot;
# The outscale key pair
export OSC_KEYPAIR_NAME=&quot;&lt;KEYPAIR_NAME&gt;&quot;
# The outscale subregion name
export OSC_SUBREGION_NAME=&quot;&lt;SUBREGION_NAME&gt;&quot;
# The outscale vm type
export OSC_VM_TYPE=&quot;&lt;VM_TYPE&gt;&quot;
# The outscale image name
export OSC_IMAGE_NAME=&quot;&lt;IMAGE_NAME&gt;&quot;
</code></pre>
</section>
<section id="tab-Proxmox" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Proxmox cluster. For instructions on how to build a compatible VM template
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash"># The node that hosts the VM template to be used to provision VMs
export PROXMOX_SOURCENODE=&quot;pve&quot;
# The template VM ID used for cloning VMs
export TEMPLATE_VMID=100
# The ssh authorized keys used to ssh to the machines.
export VM_SSH_KEYS=&quot;ssh-ed25519 ..., ssh-ed25519 ...&quot;
# The IP address used for the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=10.10.10.4
# The IP ranges for Cluster nodes
export NODE_IP_RANGES=&quot;[10.10.10.5-10.10.10.50, 10.10.10.55-10.10.10.70]&quot;
# The gateway for the machines network-config.
export GATEWAY=&quot;10.10.10.1&quot;
# Subnet Mask in CIDR notation for your node IP ranges
export IP_PREFIX=24
# The Proxmox network device for VMs
export BRIDGE=&quot;vmbr1&quot;
# The dns nameservers for the machines network-config.
export DNS_SERVERS=&quot;[8.8.8.8,8.8.4.4]&quot;
# The Proxmox nodes used for VM deployments
export ALLOWED_NODES=&quot;[pve1,pve2,pve3]&quot;
</code></pre>
<p>For more information about prerequisites and advanced setups for Proxmox, see the <a href="https://github.com/ionos-cloud/cluster-api-provider-proxmox/blob/main/docs/Usage.md">Proxmox getting started guide</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<pre><code class="language-bash"># Scaleway credentials, project ID and region.
export SCW_ACCESS_KEY=&quot;&lt;ACCESS_KEY&gt;&quot;
export SCW_SECRET_KEY=&quot;&lt;SECRET_KEY&gt;&quot;
export SCW_PROJECT_ID=&quot;&lt;PROJECT_ID&gt;&quot;
export SCW_REGION=&quot;fr-par&quot;

# Scaleway Instance image names that will be used to provision servers.
export CONTROL_PLANE_MACHINE_IMAGE=&quot;&lt;IMAGE_NAME&gt;&quot;
export WORKER_MACHINE_IMAGE=&quot;&lt;IMAGE_NAME&gt;&quot;
</code></pre>
<p>For more information about prerequisites and advanced setups for CAPS, see the <a href="https://github.com/scaleway/cluster-api-provider-scaleway/blob/main/docs/getting-started.md">CAPS getting started guide</a>.</p>
</section>
<section id="tab-Tinkerbell" class="tab-panel">
<pre><code class="language-bash">export TINKERBELL_IP=&lt;hegel ip&gt;
</code></pre>
<p>For more information please visit <a href="https://github.com/tinkerbell/cluster-api-provider-tinkerbell/blob/main/docs/QUICK-START.md">Tinkerbell getting started guide</a>.</p>
</section>
<section id="tab-VCD" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your VCD catalog. For instructions on how to build and upload a compatible image
see <a href="https://github.com/vmware/cluster-api-provider-cloud-director">CAPVCD</a></p>
<p>To see all required VCD environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure vcd --list-variables capi-quickstart
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">export CLUSTER_NAME=kind
export CLUSTER_NAMESPACE=vcluster
export KUBERNETES_VERSION=1.23.4
export HELM_VALUES=&quot;service:\n  type: NodePort&quot;
</code></pre>
<p>Please see the <a href="https://github.com/loft-sh/cluster-api-provider-vcluster#installation-instructions">vcluster installation instructions</a> for more details.</p>
</section>
<section id="tab-Virtink" class="tab-panel">
<p>To see all required Virtink environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure virtink --list-variables capi-quickstart
</code></pre>
<p>See the <a href="https://github.com/smartxworks/cluster-api-provider-virtink">Virtink provider</a> document for more details.</p>
</section>
<section id="tab-vSphere" class="tab-panel">
<p>It is required to use an official CAPV machine images for your vSphere VM templates. See <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md#uploading-the-machine-images">uploading CAPV machine images</a> for instructions on how to do this.</p>
<pre><code class="language-bash"># The vCenter server IP or FQDN
export VSPHERE_SERVER=&quot;10.0.0.1&quot;
# The vSphere datacenter to deploy the management cluster on
export VSPHERE_DATACENTER=&quot;SDDC-Datacenter&quot;
# The vSphere datastore to deploy the management cluster on
export VSPHERE_DATASTORE=&quot;vsanDatastore&quot;
# The VM network to deploy the management cluster on
export VSPHERE_NETWORK=&quot;VM Network&quot;
# The vSphere resource pool for your VMs
export VSPHERE_RESOURCE_POOL=&quot;*/Resources&quot;
# The VM folder for your VMs. Set to &quot;&quot; to use the root vSphere folder
export VSPHERE_FOLDER=&quot;vm&quot;
# The VM template to use for your VMs
export VSPHERE_TEMPLATE=&quot;ubuntu-1804-kube-v1.17.3&quot;
# The public ssh authorized key on all machines
export VSPHERE_SSH_AUTHORIZED_KEY=&quot;ssh-rsa AAAAB3N...&quot;
# The certificate thumbprint for the vCenter server
export VSPHERE_TLS_THUMBPRINT=&quot;97:48:03:8D:78:A9...&quot;
# The storage policy to be used (optional). Set to &quot;&quot; if not required
export VSPHERE_STORAGE_POLICY=&quot;policy-one&quot;
# The IP address used for the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=&quot;1.2.3.4&quot;
</code></pre>
<p>For more information about prerequisites, credentials management, or permissions for vSphere, see the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md">vSphere getting started guide</a>.</p>
</section>
<section id="tab-Vultr" class="tab-panel">
<p>A Cluster API compatible image must be available in your Vultr account. For instructions on how to build a compatible image see image-builder for <a href="https://github.com/vultr/cluster-api-provider-vultr/blob/main/docs/getting-started.md">Vultr</a></p>
<pre><code class="language-bash">export CLUSTER_NAME=&lt;clustername&gt;
export KUBERNETES_VERSION=v1.28.9
export CONTROL_PLANE_MACHINE_COUNT=1
export CONTROL_PLANE_PLANID=&lt;plan_id&gt;
export WORKER_MACHINE_COUNT=1
export WORKER_PLANID=&lt;plan_id&gt;
export MACHINE_IMAGE=&lt;snapshot_id&gt;
export REGION=&lt;region&gt;
export PLANID=&lt;plan_id&gt;
export VPCID=&lt;vpc_id&gt;
export SSHKEY_ID=&lt;sshKey_id&gt;
</code></pre>
</section>
</div></div>
<h4><a class="header" href="#generating-the-cluster-configuration" id="generating-the-cluster-configuration">Generating the cluster configuration</a></h4>
<p>For the purpose of this tutorial, we’ll name our cluster capi-quickstart.</p>
<div id="tab-clusterctl-config-cluster" class="tabset"><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster-Docker" aria-controls="tab-clusterctl-config-cluster-Docker" checked><label for="tab-clusterctl-config-cluster-Docker">Docker</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- vcluster" aria-controls="tab-clusterctl-config-cluster- vcluster" ><label for="tab-clusterctl-config-cluster- vcluster"> vcluster</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- KubeVirt" aria-controls="tab-clusterctl-config-cluster- KubeVirt" ><label for="tab-clusterctl-config-cluster- KubeVirt"> KubeVirt</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- Azure" aria-controls="tab-clusterctl-config-cluster- Azure" ><label for="tab-clusterctl-config-cluster- Azure"> Azure</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- Other providers..." aria-controls="tab-clusterctl-config-cluster- Other providers..." ><label for="tab-clusterctl-config-cluster- Other providers..."> Other providers...</label><div class="tab-panels">
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-4" id="warning-4">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart --flavor development \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml
</code></pre>
<p>Note: If you want to use MachinePools use flavor <code>development-mp</code>.</p>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">export CLUSTER_NAME=kind
export CLUSTER_NAMESPACE=vcluster
export KUBERNETES_VERSION=1.31.2
export HELM_VALUES=&quot;service:\n  type: NodePort&quot;

kubectl create namespace ${CLUSTER_NAMESPACE}
clusterctl generate cluster ${CLUSTER_NAME} \
    --infrastructure vcluster \
    --kubernetes-version ${KUBERNETES_VERSION} \
    --target-namespace ${CLUSTER_NAMESPACE} | kubectl apply -f -
</code></pre>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>As we described above, in this tutorial, we will use a LoadBalancer service in order to expose the API server of the
workload cluster, so we want to use the load balancer (lb) template (rather than the default one). We’ll use the
clusterctl’s <code>--flavor</code> flag for that:</p>
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --infrastructure=&quot;kubevirt&quot; \
  --flavor lb \
  --kubernetes-version ${CAPK_GUEST_K8S_VERSION} \
  --control-plane-machine-count=1 \
  --worker-machine-count=1 \
  &gt; capi-quickstart.yaml
</code></pre>
</section>
<section id="tab-Azure" class="tab-panel">
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --infrastructure azure \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml

# Cluster templates authenticate with Workload Identity by default. Modify the AzureClusterIdentity for ServicePrincipal authentication.
# See https://capz.sigs.k8s.io/topics/identities for more details.
yq -i &quot;with(. | select(.kind == \&quot;AzureClusterIdentity\&quot;); .spec.type |= \&quot;ServicePrincipal\&quot; | .spec.clientSecret.name |= \&quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAME}\&quot; | .spec.clientSecret.namespace |= \&quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}\&quot;)&quot; capi-quickstart.yaml
</code></pre>
</section>
<section id="tab-Other providers..." class="tab-panel">
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml
</code></pre>
</section>
</div></div>
<p>This creates a YAML file named <code>capi-quickstart.yaml</code> with a predefined list of Cluster API objects; Cluster, Machines,
Machine Deployments, etc.</p>
<p>The file can be eventually modified using your editor of choice.</p>
<p>See <a href="../clusterctl/commands/generate-cluster.html">clusterctl generate cluster</a> for more details.</p>
<h4><a class="header" href="#apply-the-workload-cluster" id="apply-the-workload-cluster">Apply the workload cluster</a></h4>
<p>When ready, run the following command to apply the cluster manifest.</p>
<pre><code class="language-bash">kubectl apply -f capi-quickstart.yaml
</code></pre>
<p>The output is similar to this:</p>
<pre><code class="language-bash">cluster.cluster.x-k8s.io/capi-quickstart created
dockercluster.infrastructure.cluster.x-k8s.io/capi-quickstart created
kubeadmcontrolplane.controlplane.cluster.x-k8s.io/capi-quickstart-control-plane created
dockermachinetemplate.infrastructure.cluster.x-k8s.io/capi-quickstart-control-plane created
machinedeployment.cluster.x-k8s.io/capi-quickstart-md-0 created
dockermachinetemplate.infrastructure.cluster.x-k8s.io/capi-quickstart-md-0 created
kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/capi-quickstart-md-0 created
</code></pre>
<h4><a class="header" href="#accessing-the-workload-cluster" id="accessing-the-workload-cluster">Accessing the workload cluster</a></h4>
<p>The cluster will now start provisioning. You can check status with:</p>
<pre><code class="language-bash">kubectl get cluster
</code></pre>
<p>You can also get an “at glance” view of the cluster and its resources by running:</p>
<pre><code class="language-bash">clusterctl describe cluster capi-quickstart
</code></pre>
<p>and see an output similar to this:</p>
<pre><code class="language-bash">NAME              PHASE         AGE   VERSION
capi-quickstart   Provisioned   8s    v1.35.0
</code></pre>
<p>To verify the first control plane is up:</p>
<pre><code class="language-bash">kubectl get kubeadmcontrolplane
</code></pre>
<p>You should see an output is similar to this:</p>
<pre><code class="language-bash">NAME                    CLUSTER           INITIALIZED   API SERVER AVAILABLE   REPLICAS   READY   UPDATED   UNAVAILABLE   AGE    VERSION
capi-quickstart-g2trk   capi-quickstart   true                                 3                  3         3             4m7s   v1.35.0
</code></pre>
<aside class="note warning">
<h1><a class="header" href="#warning-5" id="warning-5"> Warning </a></h1>
<p>The control plane won’t be <code>Ready</code> until we install a CNI in the next step.</p>
</aside>
<p>After the first control plane node is up and running, we can retrieve the <a href="../reference/glossary.html#workload-cluster">workload cluster</a> Kubeconfig.</p>
<div id="tab-get-kubeconfig" class="tabset"><input type="radio" name="tab-get-kubeconfig" id="tab-get-kubeconfig-Default" aria-controls="tab-get-kubeconfig-Default" checked><label for="tab-get-kubeconfig-Default">Default</label><input type="radio" name="tab-get-kubeconfig" id="tab-get-kubeconfig-Docker" aria-controls="tab-get-kubeconfig-Docker" ><label for="tab-get-kubeconfig-Docker">Docker</label><div class="tab-panels">
</section>
<section id="tab-Default" class="tab-panel">
<pre><code class="language-bash">clusterctl get kubeconfig capi-quickstart &gt; capi-quickstart.kubeconfig
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
For Docker Desktop on macOS, Linux or Windows use kind to retrieve the kubeconfig. Docker Engine for Linux works with the default clusterctl approach.
<pre><code class="language-bash">kind get kubeconfig --name capi-quickstart &gt; capi-quickstart.kubeconfig
</code></pre>
<aside class="note warning">
<p>Note: To use the default clusterctl method to retrieve kubeconfig for a workload cluster created with the Docker provider when using Docker Desktop see <a href="../clusterctl/developers.html#additional-notes-for-the-docker-provider">Additional Notes for the Docker provider</a>.</p>
</aside>
</section>
</div></div>
<h3><a class="header" href="#install-a-cloud-provider" id="install-a-cloud-provider">Install a Cloud Provider</a></h3>
<p>The Kubernetes in-tree cloud provider implementations are being <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2395-removing-in-tree-cloud-providers">removed</a> in favor of external cloud providers (also referred to as “out-of-tree”). This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager. To learn more, see <a href="https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">this blog post</a>.</p>
<div id="tab-install-cloud-provider" class="tabset"><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-Azure" aria-controls="tab-install-cloud-provider-Azure" checked><label for="tab-install-cloud-provider-Azure">Azure</label><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-OpenStack" aria-controls="tab-install-cloud-provider-OpenStack" ><label for="tab-install-cloud-provider-OpenStack">OpenStack</label><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-Scaleway" aria-controls="tab-install-cloud-provider-Scaleway" ><label for="tab-install-cloud-provider-Scaleway">Scaleway</label><div class="tab-panels">
<section id="tab-Azure" class="tab-panel">
<p>Install the official cloud-provider-azure Helm chart on the workload cluster:</p>
<pre><code class="language-bash">helm install --kubeconfig=./capi-quickstart.kubeconfig --repo https://raw.githubusercontent.com/kubernetes-sigs/cloud-provider-azure/master/helm/repo cloud-provider-azure --generate-name --set infra.clusterName=capi-quickstart --set cloudControllerManager.clusterCIDR=&quot;192.168.0.0/16&quot;
</code></pre>
<p>For more information, see the <a href="https://capz.sigs.k8s.io/self-managed/addons.html">CAPZ book</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>Before deploying the OpenStack external cloud provider, configure the <code>cloud.conf</code> file for integration with your OpenStack environment:</p>
<pre><code class="language-bash">cat &gt; cloud.conf &lt;&lt;EOF
[Global]
auth-url=&lt;your_auth_url&gt;
application-credential-id=&lt;your_credential_id&gt;
application-credential-secret=&lt;your_credential_secret&gt;
region=&lt;your_region&gt;
domain-name=&lt;your_domain_name&gt;
EOF
</code></pre>
<p>For more detailed information on configuring the <code>cloud.conf</code> file, see the <a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/using-openstack-cloud-controller-manager.md#config-openstack-cloud-controller-manager">OpenStack Cloud Controller Manager documentation</a>.</p>
<p>Next, create a Kubernetes secret using this configuration to securely store your cloud environment details.
You can create this secret for example with:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig -n kube-system create secret generic cloud-config --from-file=cloud.conf
</code></pre>
<p>Now, you are ready to deploy the external cloud provider!</p>
<pre><code class="language-bash">kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/cloud-controller-manager-roles.yaml
kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml
kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml
</code></pre>
<p>Alternatively, refer to the <a href="https://github.com/kubernetes/cloud-provider-openstack/tree/master/charts/openstack-cloud-controller-manager">helm chart</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<p>Before deploying the Scaleway external cloud provider, you will need:</p>
<ul>
<li>Your Scaleway credentials (access key and secret key)</li>
<li>Your Scaleway project ID</li>
<li>The Scaleway region where your workload cluster is deployed</li>
<li>The Private Network ID of your cluster (optional)</li>
</ul>
<p>First, create the Secret named <code>scaleway-secret</code> in your workload cluster:</p>
<pre><code class="language-bash">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: scaleway-secret
  namespace: kube-system
type: Opaque
stringData:
  SCW_ACCESS_KEY: &quot;xxxxxxxxxxxxxxxx&quot;
  SCW_SECRET_KEY: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;
  SCW_DEFAULT_PROJECT_ID: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx&quot;
  SCW_DEFAULT_REGION: &quot;fr-par&quot;
  SCW_DEFAULT_ZONE: &quot;fr-par-1&quot;
  PN_ID: &quot;&quot; # If your have a private network on your cluster, you may set its ID here.
EOF
</code></pre>
<p>Finally, you can deploy the <code>scaleway-cloud-controller-manager</code>:</p>
<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/scaleway/scaleway-cloud-controller-manager/master/examples/k8s-scaleway-ccm-latest.yml
</code></pre>
<p>For more detailed information on configuring and using the Scaleway external cloud
provider, see the <a href="https://github.com/scaleway/scaleway-cloud-controller-manager">scaleway-cloud-controller-manager repository</a>.</p>
</section>
</div></div>
<h3><a class="header" href="#deploy-a-cni-solution" id="deploy-a-cni-solution">Deploy a CNI solution</a></h3>
<p>Calico is used here as an example.</p>
<div id="tab-deploy-cni" class="tabset"><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-Azure" aria-controls="tab-deploy-cni-Azure" checked><label for="tab-deploy-cni-Azure">Azure</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-vcluster" aria-controls="tab-deploy-cni-vcluster" ><label for="tab-deploy-cni-vcluster">vcluster</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-KubeVirt" aria-controls="tab-deploy-cni-KubeVirt" ><label for="tab-deploy-cni-KubeVirt">KubeVirt</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-Other providers..." aria-controls="tab-deploy-cni-Other providers..." ><label for="tab-deploy-cni-Other providers...">Other providers...</label><div class="tab-panels">
<section id="tab-Azure" class="tab-panel">
<p>Install the official Calico Helm chart on the workload cluster:</p>
<pre><code class="language-bash">helm repo add projectcalico https://docs.tigera.io/calico/charts --kubeconfig=./capi-quickstart.kubeconfig &amp;&amp; \
helm install calico projectcalico/tigera-operator --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-azure/main/templates/addons/calico/values.yaml --namespace tigera-operator --create-namespace
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state,
let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
<pre><code class="language-bash">NAME                                          STATUS   ROLES           AGE    VERSION
capi-quickstart-vs89t-gmbld                   Ready    control-plane   5m33s  v1.35.0
capi-quickstart-vs89t-kf9l5                   Ready    control-plane   6m20s  v1.35.0
capi-quickstart-vs89t-t8cfn                   Ready    control-plane   7m10s  v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-8tq9v   Ready    &lt;none&gt;          6m5s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-glnjd   Ready    &lt;none&gt;          6m9s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-sfzp6   Ready    &lt;none&gt;          6m9s   v1.35.0
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<p>Calico not required for vcluster.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>Before deploying the Calico CNI, make sure the VMs are running:</p>
<pre><code class="language-bash">kubectl get vm
</code></pre>
<p>If our new VMs are running, we should see a response similar to this:</p>
<pre><code class="language-text">NAME                                  AGE    STATUS    READY
capi-quickstart-control-plane-7s945   167m   Running   True
capi-quickstart-md-0-zht5j            164m   Running   True
</code></pre>
<p>We can also read the virtual machine instances:</p>
<pre><code class="language-bash">kubectl get vmi
</code></pre>
<p>The output will be similar to:</p>
<pre><code class="language-text">NAME                                  AGE    PHASE     IP             NODENAME             READY
capi-quickstart-control-plane-7s945   167m   Running   10.244.82.16   kind-control-plane   True
capi-quickstart-md-0-zht5j            164m   Running   10.244.82.17   kind-control-plane   True
</code></pre>
<p>Since our workload cluster is running within the kind cluster, we need to prevent conflicts between the kind
(management) cluster’s CNI, and the workload cluster CNI. The following modifications in the default Calico settings
are enough for these two CNI to work on (actually) the same environment.</p>
<ul>
<li>Change the CIDR to a non-conflicting range</li>
<li>Change the value of the <code>CLUSTER_TYPE</code> environment variable to <code>k8s</code></li>
<li>Change the value of the <code>CALICO_IPV4POOL_IPIP</code> environment variable to <code>Never</code></li>
<li>Change the value of the <code>CALICO_IPV4POOL_VXLAN</code> environment variable to <code>Always</code></li>
<li>Add the <code>FELIX_VXLANPORT</code> environment variable with the value of a non-conflicting port, e.g. <code>&quot;6789&quot;</code>.</li>
</ul>
<p>The following script downloads the Calico manifest and modifies the required field. The CIDR and the port values are examples.</p>
<pre><code class="language-bash">curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/calico.yaml -o calico-workload.yaml

sed -i -E 's|^( +)# (- name: CALICO_IPV4POOL_CIDR)$|\1\2|g;'\
's|^( +)# (  value: )&quot;192.168.0.0/16&quot;|\1\2&quot;10.243.0.0/16&quot;|g;'\
'/- name: CLUSTER_TYPE/{ n; s/( +value: &quot;).+/\1k8s&quot;/g };'\
'/- name: CALICO_IPV4POOL_IPIP/{ n; s/value: &quot;Always&quot;/value: &quot;Never&quot;/ };'\
'/- name: CALICO_IPV4POOL_VXLAN/{ n; s/value: &quot;Never&quot;/value: &quot;Always&quot;/};'\
'/# Set Felix endpoint to host default action to ACCEPT./a\            - name: FELIX_VXLANPORT\n              value: &quot;6789&quot;' \
calico-workload.yaml
</code></pre>
<p>Now, deploy the Calico CNI on the workload cluster:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig create -f calico-workload.yaml
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state, let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
</section>
<section id="tab-Other providers..." class="tab-panel">
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig \
  apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state,
let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
<pre><code class="language-bash">NAME                                          STATUS   ROLES           AGE    VERSION
capi-quickstart-vs89t-gmbld                   Ready    control-plane   5m33s  v1.35.0
capi-quickstart-vs89t-kf9l5                   Ready    control-plane   6m20s  v1.35.0
capi-quickstart-vs89t-t8cfn                   Ready    control-plane   7m10s  v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-8tq9v   Ready    &lt;none&gt;          6m5s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-glnjd   Ready    &lt;none&gt;          6m9s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-sfzp6   Ready    &lt;none&gt;          6m9s   v1.35.0
</code></pre>
</section>
</div></div>
<aside class="note">
<h1><a class="header" href="#troubleshooting" id="troubleshooting">Troubleshooting</a></h1>
<p>If the nodes don’t become ready after a long period, read the pods in the <code>kube-system</code> namespace</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get pod -n kube-system
</code></pre>
<p>If the Calico pods are in image pull error state (<code>ErrImagePull</code>), it’s probably because of the Docker Hub pull rate limit.
We can try to fix that by adding a secret with our Docker Hub credentials, and use it;
see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials">here</a>
for details.</p>
<p>First, create the secret. Please notice the Docker config file path, and adjust it to your local setting.</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig create secret generic docker-creds \
    --from-file=.dockerconfigjson=&lt;YOUR DOCKER CONFIG FILE PATH&gt; \
    --type=kubernetes.io/dockerconfigjson \
    -n kube-system
</code></pre>
<p>Now, if the <code>calico-node</code> pods are with status of <code>ErrImagePull</code>, patch their DaemonSet to make them use the new secret to pull images:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig patch daemonset \
    -n kube-system calico-node \
    -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;imagePullSecrets&quot;:[{&quot;name&quot;:&quot;docker-creds&quot;}]}}}}'
</code></pre>
<p>After a short while, the calico-node pods will be with <code>Running</code> status. Now, if the calico-kube-controllers pod is also
in <code>ErrImagePull</code> status, patch its deployment to fix the problem:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig patch deployment \
    -n kube-system calico-kube-controllers \
    -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;imagePullSecrets&quot;:[{&quot;name&quot;:&quot;docker-creds&quot;}]}}}}'
</code></pre>
<p>Read the pods again</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get pod -n kube-system
</code></pre>
<p>Eventually, all the pods in the kube-system namespace will run, and the result should be similar to this:</p>
<pre><code class="language-text">NAME                                                          READY   STATUS    RESTARTS   AGE
calico-kube-controllers-c969cf844-dgld6                       1/1     Running   0          50s
calico-node-7zz7c                                             1/1     Running   0          54s
calico-node-jmjd6                                             1/1     Running   0          54s
coredns-64897985d-dspjm                                       1/1     Running   0          3m49s
coredns-64897985d-pgtgz                                       1/1     Running   0          3m49s
etcd-capi-quickstart-control-plane-kjjbb                      1/1     Running   0          3m57s
kube-apiserver-capi-quickstart-control-plane-kjjbb            1/1     Running   0          3m57s
kube-controller-manager-capi-quickstart-control-plane-kjjbb   1/1     Running   0          3m57s
kube-proxy-b9g5m                                              1/1     Running   0          3m12s
kube-proxy-p6xx8                                              1/1     Running   0          3m49s
kube-scheduler-capi-quickstart-control-plane-kjjbb            1/1     Running   0          3m57s
</code></pre>
</aside>
<h3><a class="header" href="#clean-up" id="clean-up">Clean Up</a></h3>
<p>Delete workload cluster.</p>
<pre><code class="language-bash">kubectl delete cluster capi-quickstart
</code></pre>
<aside class="note warning">
<p>IMPORTANT: In order to ensure a proper cleanup of your infrastructure you must always delete the cluster object. Deleting the entire cluster template with <code>kubectl delete -f capi-quickstart.yaml</code> might lead to pending resources to be cleaned up manually.</p>
</aside>
<p>Delete management cluster</p>
<pre><code class="language-bash">kind delete cluster
</code></pre>
<h2><a class="header" href="#next-steps" id="next-steps">Next steps</a></h2>
<ul>
<li>Create a second workload cluster. Simply follow the steps outlined above, but remember to provide a different name for your second workload cluster.</li>
<li>Deploy applications to your workload cluster. Use the <a href="quick-start.html#deploy-a-cni-solution">CNI deployment steps</a> for pointers.</li>
<li>See the <a href="../clusterctl/overview.html">clusterctl</a> documentation for more detail about clusterctl supported actions.</li>
</ul>
<!-- links -->
<h1><a class="header" href="#cluster-api-operator-quickstart" id="cluster-api-operator-quickstart">Cluster API Operator Quickstart</a></h1>
<p>This section provides a quickstart guide for using the Cluster API Operator to create a Kubernetes cluster.
To use the <code>clusterctl</code> quickstart path, visit <a href="./quick-start.html">this quickstart guide</a>.</p>
<h1><a class="header" href="#quickstart" id="quickstart">Quickstart</a></h1>
<p>This is a quickstart guide for getting Cluster API Operator up and running on your Kubernetes cluster.</p>
<p>For more detailed information, please refer to the full documentation.</p>
<h2><a class="header" href="#prerequisites" id="prerequisites">Prerequisites</a></h2>
<ul>
<li><a href="https://cluster-api.sigs.k8s.io/user/quick-start#install-andor-configure-a-kubernetes-cluster">Running Kubernetes cluster</a>.</li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> for interacting with the management cluster.</li>
<li><a href="https://helm.sh/docs/intro/install/">Helm</a> for installing operator on the cluster (optional).</li>
</ul>
<h2><a class="header" href="#install-and-configure-cluster-api-operator" id="install-and-configure-cluster-api-operator">Install and configure Cluster API Operator</a></h2>
<h3><a class="header" href="#configuring-credential-for-cloud-providers" id="configuring-credential-for-cloud-providers">Configuring credential for cloud providers</a></h3>
<p>Instead of using environment variables as clusterctl does, Cluster API Operator uses Kubernetes secrets to store credentials for cloud providers. Refer to <a href="https://cluster-api.sigs.k8s.io/user/quick-start#initialization-for-common-providers">provider documentation</a> on which credentials are required.</p>
<p>This example uses AWS provider, but the same approach can be used for other providers.</p>
<pre><code class="language-bash">export CREDENTIALS_SECRET_NAME=&quot;credentials-secret&quot;
export CREDENTIALS_SECRET_NAMESPACE=&quot;default&quot;

kubectl create secret generic &quot;${CREDENTIALS_SECRET_NAME}&quot; --from-literal=AWS_B64ENCODED_CREDENTIALS=&quot;${AWS_B64ENCODED_CREDENTIALS}&quot; --namespace &quot;${CREDENTIALS_SECRET_NAMESPACE}&quot;
</code></pre>
<h3><a class="header" href="#installing-cluster-api-operator" id="installing-cluster-api-operator">Installing Cluster API Operator</a></h3>
<p>Add CAPI Operator &amp; cert manager helm repository:</p>
<pre><code class="language-bash">helm repo add capi-operator https://kubernetes-sigs.github.io/cluster-api-operator
helm repo add jetstack https://charts.jetstack.io --force-update
helm repo update
</code></pre>
<p>Install cert manager:</p>
<pre><code class="language-bash">helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true
</code></pre>
<p>Deploy Cluster API components with docker provider using a single command during operator installation</p>
<aside class="note warning">
<h1><a class="header" href="#warning-6" id="warning-6"> Warning </a></h1>
<p>The <code>--wait</code> flag is REQUIRED for the helm install command to work. If the --wait flag is not used, the helm install command will not wait for the resources to be created and will return immediately. This will cause the helm install command to fail because the webhooks will not be ready in time. The --timeout flag is optional and can be used to specify the amount of time to wait for the resources to be created.</p>
</aside>
<pre><code class="language-bash">helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system --set infrastructure.docker.enabled=true --set configSecret.name=${CREDENTIALS_SECRET_NAME} --set configSecret.namespace=${CREDENTIALS_SECRET_NAMESPACE}  --wait --timeout 90s
</code></pre>
<p>Docker provider can be replaced by any provider supported by <a href="https://cluster-api.sigs.k8s.io/reference/providers.html#infrastructure">clusterctl</a>.</p>
<p>Other options for installing Cluster API Operator are described in <a href="https://github.com/kubernetes-sigs/cluster-api-operator/blob/main/docs/README.md#installation">full documentation</a>.</p>
<h1><a class="header" href="#example-api-usage" id="example-api-usage">Example API Usage</a></h1>
<p>Deploy latest version of core Cluster API components:</p>
<pre><code class="language-yaml">apiVersion: operator.cluster.x-k8s.io/v1alpha2
kind: CoreProvider
metadata:
  name: cluster-api
  namespace: capi-system
</code></pre>
<p>Deploy Cluster API AWS provider with specific version, custom manager options and flags:</p>
<pre><code class="language-yaml">---
apiVersion: operator.cluster.x-k8s.io/v1alpha2
kind: InfrastructureProvider
metadata:
 name: aws
 namespace: capa-system
spec:
 version: v2.1.4
 configSecret:
   name: credentials-secret
</code></pre>
<h1><a class="header" href="#aws-machine-images-for-capa-clusters" id="aws-machine-images-for-capa-clusters">AWS Machine Images for CAPA Clusters</a></h1>
<p>CAPA requires a “machine image” containing pre-installed, matching versions of kubeadm and kubelet.</p>
<h2><a class="header" href="#eks-clusters" id="eks-clusters">EKS Clusters</a></h2>
<p>For an EKS cluster the default behaviour is to retieve the AMI to use from SSM. This is so the recommended Amazon Linux AMI is used (see <a href="https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html">here</a>).</p>
<p>Instead of using the auto resolved AMIs an appropriate custom image ID for the Kubernetes version can be set in <code>AWSMachineTemplate</code> spec.</p>
<h2><a class="header" href="#non-eks-clusters" id="non-eks-clusters">Non-EKS Clusters</a></h2>
<p>By default the machine image is auto-resolved by CAPA to a public AMI that matches the Kubernetes version in <code>KubeadmControlPlane</code> or <code>MachineDeployment</code> spec. These AMIs are published in a community owned AWS account. See <a href="topics/images/built-amis.html">pre-built public AMIs</a> for details of the CAPA project published images.</p>
<blockquote>
<p>IMPORTANT:
The project doesn’t recommend using the public AMIs for production use. Instead its recommended that you build your own AMIs for the Kubernetes versions you want to use. The AMI can then be specified in the <code>AWSMachineTemplate</code> spec. <a href="topics/images/custom-amis.html">Custom images</a> can be created using <a href="https://github.com/kubernetes-sigs/image-builder">image-builder</a> project.</p>
</blockquote>
<h1><a class="header" href="#pre-built-kubernetes-amis" id="pre-built-kubernetes-amis">Pre-built Kubernetes AMIs</a></h1>
<p>New AMIs are built on a best effort basis when a new Kubernetes version is released for each supported OS distribution and then published to supported regions.</p>
<h2><a class="header" href="#ami-publication-policy" id="ami-publication-policy">AMI Publication Policy</a></h2>
<ul>
<li>AMIs should only be used for non-production usage. For production environments we recommend that you build and maintain your own AMIs using the image-builder project.</li>
<li>AMIs will only be published for the latest release series and 2 previous release series. For example, if the current release series is v1.30 then AMIs will only be published for v1.30, v1.29, v1.28.</li>
<li>When there is a new k8s release series then any AMIs no longer covered by the previous point will be deleted. For example, when v1.31.0 is published then any AMIs for the v1.28 release series will be deleted.</li>
<li>Existing AMIs are not updated for security fixes and it is recommended to always use the latest patch version for the Kubernetes version you want to run.</li>
</ul>
<blockquote>
<p>NOTE: As the old community images were located in an AWS account that the project no longer has access to and because those AMIs have been automatically deleted, we have started publishing images again starting from Kubernetes v1.29.9.</p>
</blockquote>
<h2><a class="header" href="#finding-amis" id="finding-amis">Finding AMIs</a></h2>
<p><code>clusterawsadm ami list</code> command lists pre-built reference AMIs by Kubernetes version, OS, or AWS region. See <a href="https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm_ami_list.html">clusterawsadm ami list</a> for details.</p>
<p>If you are using a version of clusterawsadm prior to v2.6.2 then you will need to explicitly specify the owner-id for the community account: <code>clusterawsadm ami list --owner-id 819546954734</code>.</p>
<h2><a class="header" href="#supported-os-distributions" id="supported-os-distributions">Supported OS Distributions</a></h2>
<ul>
<li>Ubuntu (ubuntu-22.04, ubuntu-24.04)</li>
<li>Flatcar (flatcar-stable)</li>
</ul>
<blockquote>
<p>Note: Centos (centos-7) and Amazon Linux 2 (amazon-2) where supported but there are some issues with the AMI build that need fixing. See this <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/5142">issue</a> for details.</p>
</blockquote>
<h2><a class="header" href="#supported-aws-regions" id="supported-aws-regions">Supported AWS Regions</a></h2>
<ul>
<li>ap-northeast-1</li>
<li>ap-northeast-2</li>
<li>ap-south-1</li>
<li>ap-southeast-1</li>
<li>ap-southeast-2</li>
<li>ca-central-1</li>
<li>eu-central-1</li>
<li>eu-west-1</li>
<li>eu-west-2</li>
<li>eu-west-3</li>
<li>sa-east-1</li>
<li>us-east-1</li>
<li>us-east-2</li>
<li>us-west-1</li>
<li>us-west-2</li>
</ul>
<h1><a class="header" href="#custom-kubernetes-amis" id="custom-kubernetes-amis">Custom Kubernetes AMIs</a></h1>
<p>Cluster API uses the Kubernetes <a href="https://github.com/kubernetes-sigs/image-builder">Image Builder</a> tools. You should use the <a href="https://github.com/kubernetes-sigs/image-builder/tree/master/images/capi/packer/ami">AWS images</a> from that project as a starting point for your custom image.</p>
<p><a href="https://image-builder.sigs.k8s.io/capi/capi.html">The Image Builder Book</a> explains how to build the images defined in that repository, with instructions for <a href="https://image-builder.sigs.k8s.io/capi/providers/aws.html">AWS CAPI Images</a> in particular.</p>
<h2><a class="header" href="#operating-system-requirements" id="operating-system-requirements">Operating system requirements</a></h2>
<p>For custom images to work with Cluster API, it must meet the operating system requirements of the bootstrap provider. For example, the default <code>kubeadm</code> bootstrap provider has a set of [<code>preflight checks</code>][kubeadm-preflight-checks] that a VM is expected to pass before it can join the cluster.</p>
<h2><a class="header" href="#kubernetes-version-requirements" id="kubernetes-version-requirements">Kubernetes version requirements</a></h2>
<p>The pre-built public images are each built to support a specific version of Kubernetes. When using custom images, make sure to match the image to the <code>version:</code> field of the <code>KubeadmControlPlane</code> and <code>MachineDeployment</code> in the YAML template for your workload cluster.</p>
<p>To upgrade to a new Kubernetes release with custom images requires this preparation:</p>
<ul>
<li>create a new custom image which supports the Kubernetes release version</li>
<li>copy the existing <code>AWSMachineTemplate</code> and change its <code>ami:</code> section to reference the new custom image</li>
<li>create the new <code>AWSMachineTemplate</code> on the management cluster</li>
<li>modify the existing <code>KubeadmControlPlane</code> and <code>MachineDeployment</code> to reference the new <code>AWSMachineTemplate</code> and update the <code>version:</code> field to match</li>
</ul>
<p>See <a href="https://cluster-api.sigs.k8s.io/tasks/kubeadm-control-plane.html#upgrading-workload-clusters">Upgrading workload clusters</a> for more details.</p>
<h2><a class="header" href="#creating-a-cluster-from-a-custom-image" id="creating-a-cluster-from-a-custom-image">Creating a cluster from a custom image</a></h2>
<p>To use a custom image, it needs to be referenced in an <code>ami:</code> section of your <code>AWSMachineTemplate</code>.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: capa-image-id-example
  namespace: default
spec:
  template:
    spec:
      ami:
        id: ami-09709369c53539c11
      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io
      instanceType: m5.xlarge
      sshKeyName: default
</code></pre>
<h1><a class="header" href="#topics" id="topics">Topics</a></h1>
<!-- NB: This page is meant to be embedded in Cluster API book -->
<h1><a class="header" href="#using-clusterawsadm-to-fulfill-prerequisites" id="using-clusterawsadm-to-fulfill-prerequisites">Using clusterawsadm to fulfill prerequisites</a></h1>
<h2><a class="header" href="#requirements" id="requirements">Requirements</a></h2>
<ul>
<li>Linux or MacOS (Windows isn’t supported at the moment).</li>
<li>AWS credentials.</li>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a></li>
<li><a href="https://stedolan.github.io/jq/download/">jq</a></li>
</ul>
<h2><a class="header" href="#iam-resources" id="iam-resources">IAM resources</a></h2>
<h3><a class="header" href="#with-clusterawsadm" id="with-clusterawsadm">With <code>clusterawsadm</code></a></h3>
<p>Get the latest <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases">clusterawsadm</a>
and place it in your path.</p>
<p>Cluster API Provider AWS ships with clusterawsadm, a utility to help you manage
IAM objects for this project.</p>
<p>In order to use clusterawsadm you must have an administrative user in an AWS account.
Once you have that administrator user you need to set your environment variables:</p>
<ul>
<li><code>AWS_REGION</code></li>
<li><code>AWS_ACCESS_KEY_ID</code></li>
<li><code>AWS_SECRET_ACCESS_KEY</code></li>
<li><code>AWS_SESSION_TOKEN</code> (if you are using Multi-factor authentication)</li>
</ul>
<p>After these are set run this command to get you up and running:</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam create-cloudformation-stack
</code></pre>
<p>Additional policies can be added by creating a configuration file</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  controlPlane:
    extraPolicyAttachments:
      - arn:aws:iam::&lt;AWS_ACCOUNT&gt;:policy/my-policy
      - arn:aws:iam::aws:policy/AmazonEC2FullAccess
  nodes:
    extraPolicyAttachments:
      - arn:aws:iam::&lt;AWS_ACCOUNT&gt;:policy/my-other-policy
</code></pre>
<p>and passing it to clusterawsadm as follows</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap-config.yaml
</code></pre>
<p>These will be added to the control plane and node roles respectively when they are created.</p>
<blockquote>
<p><strong>Note:</strong> If you used the now deprecated <code>clusterawsadm alpha bootstrap</code> 0.5.4 or earlier to create IAM objects for the
Cluster API Provider for AWS, using <code>clusterawsadm bootstrap iam</code> 0.5.5 or later will, by default, remove the bootstrap
user and group. Anything using those credentials to authenticate will start experiencing authentication failures. If you
rely on the bootstrap user and group credentials, specify <code>bootstrapUser.enable = true</code> in the configuration file, like
this:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  bootstrapUser:
    enable: true
</code></pre>
</blockquote>
<h4><a class="header" href="#with-eks-support" id="with-eks-support">With EKS Support</a></h4>
<p>The pre-requisities for EKS are enabled by default. However, if you want to use some of the optional features of EKS (see <a href="topics/eks/enabling.html">here</a> for more information on what these are) then you will need to enable these features via the configuration file. For example:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  eks:
    iamRoleCreation: false # Set to true if you plan to use the EKSEnableIAM feature flag to enable automatic creation of IAM roles
    managedMachinePool:
      disable: false # Set to false to enable creation of the default node role for managed machine pools
    fargate:
      disable: false # Set to false to enable creation of the default role for the fargate profiles
</code></pre>
<p>and then use that configuration file:</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap-config.yaml
</code></pre>
<h4><a class="header" href="#enabling-eventbridge-events" id="enabling-eventbridge-events">Enabling EventBridge Events</a></h4>
<p>To enable EventBridge instance state events, additional permissions must be granted along with enabling the feature-flag.
Additional permissions for events and queue management can be enabled through the configuration file as follows:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  ...
  eventBridge:
    enable: true
  ...
</code></pre>
<h4><a class="header" href="#cross-account-role-assumption" id="cross-account-role-assumption">Cross Account Role Assumption</a></h4>
<p>CAPA, by default, does not provide the necessary permissions to allow cross-account role assumption, which can be used to manage clusters in other environments. This is documented <a href="topics/multitenancy.html#necessary-permissions-for-assuming-a-role">here</a>. The ‘sts:AssumeRole’ permissions can be added via the following configuration on the manager account configuration:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  ...
  allowAssumeRole: true
  ...
</code></pre>
<p>The above will give the controller to have the necessary permissions needed in order for it to manage clusters in other accounts using the AWSClusterRoleIdentity. Please note, the above should only be applied to the account where CAPA is running. To allow CAPA to assume the roles in the managed/target accounts, the following configuration needs to be used:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  ...
  clusterAPIControllers:
    disabled: false
    trustStatements:
    - Action:
      - &quot;sts:AssumeRole&quot;
      Effect: &quot;Allow&quot;
      Principal:
        AWS:
        - &quot;arn:aws:iam::&lt;manager account&gt;:role/controllers.cluster-api-provider-aws.sigs.k8s.io&quot;
  ...
</code></pre>
<h3><a class="header" href="#without-clusterawsadm" id="without-clusterawsadm">Without <code>clusterawsadm</code></a></h3>
<p>This is not a recommended route as the policies are very specific and will
change with new features.</p>
<p>If you do not wish to use the <code>clusteradwsadm</code> tool then you will need to
understand exactly which IAM policies and groups we are expecting. There are
several policies, roles and users that need to be created. Please see our
<a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/0e543e0eb30a7065c967f5df8d6abd872aa4ff0c/pkg/cloud/aws/services/cloudformation/bootstrap.go#L149-L188">controller policy</a> file to understand the permissions that are necessary.</p>
<p>You can use <code>clusteradwadm</code> to print out the needed IAM policies, e.g.</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyControllers --config bootstrap-config.yaml
</code></pre>
<h2><a class="header" href="#ssh-key-pair" id="ssh-key-pair">SSH Key pair</a></h2>
<p>If you plan to use SSH to access the instances created by Cluster API Provider AWS
then you will need to specify the name of an existing SSH key pair within the region
you plan on using. If you don’t have one yet, a new one needs to be created.</p>
<h3><a class="header" href="#create-a-new-key-pair" id="create-a-new-key-pair">Create a new key pair</a></h3>
<pre><code class="language-bash"># Save the output to a secure location
aws ec2 create-key-pair --key-name default --output json | jq .KeyMaterial -r
-----BEGIN RSA PRIVATE KEY-----
[... contents omitted ...]
-----END RSA PRIVATE KEY-----
</code></pre>
<p>If you want to save the private key directly into AWS Systems Manager Parameter
Store with KMS encryption for security, you can use the following command:</p>
<pre><code class="language-bash">aws ssm put-parameter --name &quot;/sigs.k8s.io/cluster-api-provider-aws/ssh-key&quot; \
  --type SecureString \
  --value &quot;$(aws ec2 create-key-pair --key-name default --output json | jq .KeyMaterial -r)&quot;
</code></pre>
<h3><a class="header" href="#adding-an-existing-public-key-to-aws" id="adding-an-existing-public-key-to-aws">Adding an existing public key to AWS</a></h3>
<pre><code class="language-bash"># Replace with your own public key
aws ec2 import-key-pair \
  --key-name default \
  --public-key-material &quot;$(cat ~/.ssh/id_rsa.pub)&quot;
</code></pre>
<blockquote>
<p><strong>NB</strong>: Only RSA keys are supported by AWS.</p>
</blockquote>
<h2><a class="header" href="#setting-up-the-environment" id="setting-up-the-environment">Setting up the environment</a></h2>
<p>The current iteration of the Cluster API Provider AWS relies on credentials
being present in your environment. These then get written into the cluster
manifests for use by the controllers.</p>
<p>E.g.</p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.
</code></pre>
<p><strong>Note</strong>: The credentials used must have the appropriate permissions for use by the controllers.
You can get the required policy statement by using the following command:</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyControllers --config bootstrap-config.yaml
</code></pre>
<blockquote>
<p>To save credentials securely in your environment, <a href="https://github.com/99designs/aws-vault">aws-vault</a> uses
the OS keystore as permanent storage, and offers shell features to securely
expose and setup local AWS environments.</p>
</blockquote>
<h1><a class="header" href="#accessing-cluster-instances" id="accessing-cluster-instances">Accessing cluster instances</a></h1>
<h2><a class="header" href="#overview" id="overview">Overview</a></h2>
<p>After running <code>clusterctl generate cluster</code> to generate the configuration for a new workload cluster (and then redirecting that output to a file for use with <code>kubectl apply</code>, or piping it directly to <code>kubectl apply</code>), the new workload cluster will be deployed. This document explains how to access the new workload cluster’s nodes.</p>
<h2><a class="header" href="#prerequisites-1" id="prerequisites-1">Prerequisites</a></h2>
<ol>
<li><code>clusterctl generate cluster</code> was successfully executed to generate the configuration for a new workload cluster</li>
<li>The configuration for the new workload cluster was applied to the management cluster using <code>kubectl apply</code> and the cluster is up and running in an AWS environment.</li>
<li>The SSH key referenced by <code>clusterctl</code> in step 1 exists in AWS and is stored in the correct location locally for use by SSH (on macOS/Linux systems, this is typically <code>$HOME/.ssh</code>). This document will refer to this key as <code>cluster-api-provider-aws.sigs.k8s.io</code>.</li>
<li><em>(If using AWS Session Manager)</em> The AWS CLI and the Session Manager plugin have been installed and configured.</li>
</ol>
<h2><a class="header" href="#methods-for-accessing-nodes" id="methods-for-accessing-nodes">Methods for accessing nodes</a></h2>
<p>There are two ways to access cluster nodes once the workload cluster is up and running:</p>
<ul>
<li>via SSH</li>
<li>via AWS Session Manager</li>
</ul>
<h3><a class="header" href="#accessing-nodes-via-ssh" id="accessing-nodes-via-ssh">Accessing nodes via SSH</a></h3>
<p>By default, workload clusters created in AWS will <em>not</em> support access via SSH apart from AWS Session Manager (see the section titled “Accessing nodes via AWS Session Manager”). However, the manifest for a workload cluster can be modified to include an SSH bastion host, created and managed by the management cluster, to enable SSH access to cluster nodes. The bastion node is created in a public subnet and provides SSH access from the world. It runs the official Ubuntu Linux image.</p>
<h4><a class="header" href="#enabling-the-bastion-host" id="enabling-the-bastion-host">Enabling the bastion host</a></h4>
<p>To configure the Cluster API Provider for AWS to create an SSH bastion host, add this line to the AWSCluster spec:</p>
<pre><code class="language-yaml">spec:
  bastion:
    enabled: true
</code></pre>
<p>If this field is set and a specific AMI ID is not provided for the bastion (by setting spec.bastion.ami) then by default the latest AMI(Ubuntu 20.04 LTS OS) is looked up from <a href="https://ubuntu.com/server/docs/cloud-images/amazon-ec2">Ubuntu cloud images</a> by CAPA controller and used in bastion host creation.</p>
<h4><a class="header" href="#obtain-public-ip-address-of-the-bastion-node" id="obtain-public-ip-address-of-the-bastion-node">Obtain public IP address of the bastion node</a></h4>
<p>Once the workload cluster is up and running after being configured for an SSH bastion host, you can use the <code>kubectl get awscluster</code> command to look up the public IP address of the bastion host (make sure the <code>kubectl</code> context is set to the management cluster). The output will look something like this:</p>
<pre><code class="language-bash">NAME   CLUSTER   READY   VPC                     BASTION IP
test   test      true    vpc-1739285ed052be7ad   1.2.3.4
</code></pre>
<h4><a class="header" href="#setting-up-the-ssh-key-path" id="setting-up-the-ssh-key-path">Setting up the SSH key path</a></h4>
<p>Assumming that the <code>cluster-api-provider-aws.sigs.k8s.io</code> SSH key is stored in
<code>$HOME/.ssh/cluster-api-provider-aws</code>, use this command to set up an environment variable for use in a later command:</p>
<pre><code class="language-bash">export CLUSTER_SSH_KEY=$HOME/.ssh/cluster-api-provider-aws
</code></pre>
<h4><a class="header" href="#get-private-ip-addresses-of-nodes-in-the-cluster" id="get-private-ip-addresses-of-nodes-in-the-cluster">Get private IP addresses of nodes in the cluster</a></h4>
<p>To get the private IP addresses of nodes in the cluster (nodes may be control plane nodes or worker nodes), use this <code>kubectl</code> command with the context set to the management cluster:</p>
<pre><code class="language-bash">kubectl get nodes -o custom-columns=NAME:.metadata.name,\
IP:&quot;{.status.addresses[?(@.type=='InternalIP')].address}&quot;
</code></pre>
<p>This will produce output that looks like this:</p>
<pre><code class="language-bash">NAME                                         IP
ip-10-0-0-16.us-west-2.compute.internal   10.0.0.16
ip-10-0-0-68.us-west-2.compute.internal   10.0.0.68
</code></pre>
<p>The above command returns IP addresses of the nodes in the cluster. In this
case, the values returned are <code>10.0.0.16</code> and <code>10.0.0.68</code>.</p>
<h3><a class="header" href="#connecting-to-the-nodes-via-ssh" id="connecting-to-the-nodes-via-ssh">Connecting to the nodes via SSH</a></h3>
<p>To access one of the nodes (either a control plane node or a worker node) via the SSH bastion host, use this command if you are using a non-EKS cluster:</p>
<pre><code class="language-bash">ssh -i ${CLUSTER_SSH_KEY} ubuntu@&lt;NODE_IP&gt; \
	-o &quot;ProxyCommand ssh -W %h:%p -i ${CLUSTER_SSH_KEY} ubuntu@${BASTION_HOST}&quot;
</code></pre>
<p>And use this command if you are using a EKS based cluster:</p>
<pre><code class="language-bash">ssh -i ${CLUSTER_SSH_KEY} ec2-user@&lt;NODE_IP&gt; \
	-o &quot;ProxyCommand ssh -W %h:%p -i ${CLUSTER_SSH_KEY} ubuntu@${BASTION_HOST}&quot;
</code></pre>
<p>If the whole document is followed, the value of <code>&lt;NODE_IP&gt;</code> will be either
10.0.0.16 or 10.0.0.68.</p>
<p>Alternately, users can add a configuration stanza to their SSH configuration file (typically found on macOS/Linux systems as <code>$HOME/.ssh/config</code>):</p>
<pre><code class="language-text">Host 10.0.*
  User ubuntu
  IdentityFile &lt;CLUSTER_SSH_KEY&gt;
  ProxyCommand ssh -W %h:%p ubuntu@&lt;BASTION_HOST&gt;
</code></pre>
<h3><a class="header" href="#accessing-nodes-via-aws-session-manager" id="accessing-nodes-via-aws-session-manager">Accessing nodes via AWS Session Manager</a></h3>
<p>All CAPA-published AMIs based on Ubuntu have the AWS SSM Agent pre-installed (as a Snap package; this was added in June 2018 to the base Ubuntu Server image for all 16.04 and later AMIs). This allows users to access cluster nodes directly, without the need for an SSH bastion host, using the AWS CLI and the Session Manager plugin.</p>
<p>To access a cluster node (control plane node or worker node), you’ll need the instance ID. You can retrieve the instance ID using this <code>kubectl</code> command with the context set to the management cluster:</p>
<pre><code class="language-bash">kubectl get awsmachines -o custom-columns=NAME:.metadata.name,INSTANCEID:.spec.providerID
</code></pre>
<p>This will produce output similar to this:</p>
<pre><code class="language-bash">NAME                      INSTANCEID
test-controlplane-52fhh   aws:////i-112bac41a19da1819
test-controlplane-lc5xz   aws:////i-99aaef2381ada9228
</code></pre>
<p>Users can then use the instance ID (everything after the <code>aws:////</code> prefix) to connect to the cluster node with this command:</p>
<pre><code class="language-bash">aws ssm start-session --target &lt;INSTANCE_ID&gt;
</code></pre>
<p>This will log you into the cluster node as the <code>ssm-user</code> user ID.</p>
<h2><a class="header" href="#additional-notes" id="additional-notes">Additional Notes</a></h2>
<h3><a class="header" href="#using-the-aws-cli-instead-of-kubectl" id="using-the-aws-cli-instead-of-kubectl">Using the AWS CLI instead of <code>kubectl</code></a></h3>
<p>It is also possible to use AWS CLI commands instead of <code>kubectl</code> to gather information about the cluster nodes.</p>
<p>For example, to use the AWS CLI to get the public IP address of the SSH bastion host, use this AWS CLI command:</p>
<pre><code class="language-bash">export BASTION_HOST=$(aws ec2 describe-instances --filter='Name=tag:Name,Values=&lt;CLUSTER_NAME&gt;-bastion' \
	| jq '.Reservations[].Instances[].PublicIpAddress' -r)
</code></pre>
<p>You should substitute the correct cluster name for <code>&lt;CLUSTER_NAME&gt;</code> in the above command. (<strong>NOTE</strong>: If <code>make manifests</code> was used to generate manifests, by default the <code>&lt;CLUSTER_NAME&gt;</code> is set to <code>test1</code>.)</p>
<p>Similarly, to obtain the list of private IP addresses of the cluster nodes, use this AWS CLI command:</p>
<pre><code class="language-bash">for type in control-plane node
do
	aws ec2 describe-instances \
    --filter=&quot;Name=tag:sigs.k8s.io/cluster-api-provider-aws/role,\
    Values=${type}&quot; \
		| jq '.Reservations[].Instances[].PrivateIpAddress' -r
done
10.0.0.16
10.0.0.68
</code></pre>
<p>Finally, to obtain AWS instance IDs for cluster nodes, you can use this AWS CLI command:</p>
<pre><code class="language-bash">for type in control-plane node
do
	aws ec2 describe-instances \
    --filter=&quot;Name=tag:sigs.k8s.io/cluster-api-provider-aws/role,\
    Values=${type}&quot; \
		| jq '.Reservations[].Instances[].InstanceId' -r
done
i-112bac41a19da1819
i-99aaef2381ada9228
</code></pre>
<p>Note that your AWS CLI must be configured with credentials that enable you to query the AWS EC2 API.</p>
<h1><a class="header" href="#spot-instances" id="spot-instances">Spot Instances</a></h1>
<p><a href="https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&amp;cards.sort-order=asc&amp;trk=a9b30b20-d23f-4d61-9452-c51a7e407fcd&amp;sc_channel=ps&amp;sc_campaign=acquisition&amp;sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CCompute%7CEC2%20Spot%7CIN%7CEN%7CText&amp;s_kwcid=AL!4422!3!517651795636!e!!g!!amazon%20ec2%20spot&amp;ef_id=Cj0KCQiA95aRBhCsARIsAC2xvfxB17BKyQFcn9UUKZ1GT2sfvxKyhboEKa87gl8wBO37fSrNXmx52cIaAtqwEALw_wcB:G:s&amp;s_kwcid=AL!4422!3!517651795636!e!!g!!amazon%20ec2%20spot">AWS Spot Instances</a> allows user to reduce the costs of their compute resources by utilising AWS spare capacity for a lower price.</p>
<p>Because Spot Instances are tightly integrated with AWS services such as Auto Scaling, ECS and CloudFormation, users can choose how to launch and maintain their applications running on Spot Instances.</p>
<p>Although, with this lower cost, comes the risk of preemption. When capacity within a particular Availability Zone is increased, AWS may need to reclaim Spot instances to satisfy the demand on their data centres.</p>
<h2><a class="header" href="#when-to-use-spot-instances" id="when-to-use-spot-instances">When to use spot instances?</a></h2>
<p>Spot instances are ideal for workloads that can be interrupted. For example, short jobs or stateless services that can be rescheduled quickly, without data loss, and resume operation with limited degradation to a service.</p>
<h2><a class="header" href="#using-spot-instances-with-awsmachine" id="using-spot-instances-with-awsmachine">Using Spot Instances with AWSMachine</a></h2>
<p>To enable AWS Machine to be backed by a Spot Instance, users need to add <code>spotMarketOptions</code> to AWSMachineTemplate:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
spec:
  template:
    spec:
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceType: ${AWS_NODE_MACHINE_TYPE}
      spotMarketOptions:
        maxPrice: &quot;&quot;
      sshKeyName: ${AWS_SSH_KEY_NAME}
</code></pre>
<p>Users may also add a <code>maxPrice</code> to the options to limit the maximum spend for the instance. It is however, recommended not to set a maxPrice as AWS will cap your spending at the on-demand price if this field is left empty, and you will experience fewer interruptions.</p>
<pre><code class="language-yaml">spec:
  template:
    spotMarketOptions:
      maxPrice: 0.02 # Price in USD per hour (up to 5 decimal places)
</code></pre>
<h2><a class="header" href="#using-spot-instances-with-awsmanagedmachinepool" id="using-spot-instances-with-awsmanagedmachinepool">Using Spot Instances with AWSManagedMachinePool</a></h2>
<p>To use spot instance in EKS managed node groups for a EKS cluster, set <code>capacityType</code> to <code>spot</code> in <code>AWSManagedMachinePool</code>.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSManagedMachinePool
metadata:
  name: ${CLUSTER_NAME}-pool-0
spec:
  capacityType: spot
  ...
</code></pre>
<p>See <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">AWS doc</a> for more details.</p>
<h2><a class="header" href="#using-spot-instances-with-awsmachinepool" id="using-spot-instances-with-awsmachinepool">Using Spot Instances with AWSMachinePool</a></h2>
<p>To enable AWSMachinePool to be backed by a Spot Instance, users need to add <code>spotMarketOptions</code> to AWSLaunchTemplate:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachinePool
metadata:
  name: ${CLUSTER_NAME}-mp-0
spec:
  minSize: 1
  maxSize: 4
  awsLaunchTemplate:
    instanceType: &quot;${AWS_CONTROL_PLANE_MACHINE_TYPE}&quot;
    iamInstanceProfile: &quot;nodes.cluster-api-provider-aws.sigs.k8s.io&quot;
    sshKeyName: &quot;${AWS_SSH_KEY_NAME}&quot;
    spotMarketOptions:
       maxPrice: &quot;&quot;
</code></pre>
<blockquote>
<p><strong>IMPORTANT WARNING</strong>: The experimental feature <code>AWSMachinePool</code> supports using spot instances, but the graceful shutdown of machines in <code>AWSMachinePool</code> is not supported and has to be handled externally by users.</p>
</blockquote>
<h1><a class="header" href="#machinepools" id="machinepools">MachinePools</a></h1>
<ul>
<li><strong>Feature status:</strong> Experimental</li>
<li><strong>Feature gate:</strong> MachinePool=true</li>
</ul>
<p>MachinePool allows users to manage many machines as a single entity. Infrastructure providers implement a separate CRD that handles infrastructure side of the feature.</p>
<h2><a class="header" href="#awsmachinepool" id="awsmachinepool">AWSMachinePool</a></h2>
<p>Cluster API Provider AWS (CAPA) has experimental support for <code>MachinePool</code> though the infrastructure type <code>AWSMachinePool</code>. An <code>AWSMachinePool</code> corresponds to an <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">AWS AutoScaling Groups</a>, which provides the cloud provider specific resource for orchestrating a group of EC2 machines.</p>
<p>The AWSMachinePool controller creates and manages an AWS AutoScaling Group using launch templates so users don’t have to manage individual machines. You can use Autoscaling health checks for replacing instances and it will maintain the number of instances specified.</p>
<h3><a class="header" href="#using-clusterctl-to-deploy" id="using-clusterctl-to-deploy">Using <code>clusterctl</code> to deploy</a></h3>
<p>To deploy a MachinePool / AWSMachinePool via <code>clusterctl generate</code> there’s a <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html#flavors">flavor</a> for that.</p>
<p>Make sure to set up your AWS environment as described <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">here</a>.</p>
<pre><code class="language-shell">export EXP_MACHINE_POOL=true
clusterctl init --infrastructure aws
clusterctl generate cluster my-cluster --kubernetes-version v1.25.0 --flavor machinepool &gt; my-cluster.yaml
</code></pre>
<p>The template used for this <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html#flavors">flavor</a> is located <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template-machinepool.yaml">here</a>.</p>
<h2><a class="header" href="#awsmanagedmachinepool" id="awsmanagedmachinepool">AWSManagedMachinePool</a></h2>
<p>Cluster API Provider AWS (CAPA) has experimental support for <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">EKS Managed Node Groups</a> using <code>MachinePool</code> through the infrastructure type <code>AWSManagedMachinePool</code>. An <code>AWSManagedMachinePool</code> corresponds to an <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">AWS AutoScaling Groups</a> that is used for an EKS managed node group. .</p>
<p>The AWSManagedMachinePool controller creates and manages an EKS managed node group which in turn manages an AWS AutoScaling Group of managed EC2 instance types.</p>
<p>To use the managed machine pools certain IAM permissions are needed. The easiest way to ensure the required IAM permissions are in place is to use <code>clusterawsadm</code> to create them. To do this, follow the EKS instructions in <a href="topics/using-clusterawsadm-to-fulfill-prerequisites.html">using clusterawsadm to fulfill prerequisites</a>.</p>
<h3><a class="header" href="#using-clusterctl-to-deploy-1" id="using-clusterctl-to-deploy-1">Using <code>clusterctl</code> to deploy</a></h3>
<p>To deploy an EKS managed node group using AWSManagedMachinePool via <code>clusterctl generate</code> you can use a <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html#flavors">flavor</a>.</p>
<p>Make sure to set up your AWS environment as described <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">here</a>.</p>
<pre><code class="language-shell">export EXP_MACHINE_POOL=true
clusterctl init --infrastructure aws
clusterctl generate cluster my-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool &gt; my-cluster.yaml
</code></pre>
<p>The template used for this <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html#flavors">flavor</a> is located <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template-eks-managedmachinepool.yaml">here</a>.</p>
<h2><a class="header" href="#examples" id="examples">Examples</a></h2>
<h3><a class="header" href="#example-machinepool-awsmachinepool-and-kubeadmconfig-resources" id="example-machinepool-awsmachinepool-and-kubeadmconfig-resources">Example: MachinePool, AWSMachinePool and KubeadmConfig Resources</a></h3>
<p>Below is an example of the resources needed to create a pool of EC2 machines orchestrated with
an AWS Auto Scaling Group.</p>
<pre><code class="language-yaml">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: capa-mp-0
spec:
  clusterName: capa
  replicas: 2
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfig
          name: capa-mp-0
      clusterName: capa
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachinePool
        name: capa-mp-0
      version: v1.25.0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachinePool
metadata:
  name: capa-mp-0
spec:
  minSize: 1
  maxSize: 10
  availabilityZones:
    - &quot;${AWS_AVAILABILITY_ZONE}&quot;
  awsLaunchTemplate:
    instanceType: &quot;${AWS_CONTROL_PLANE_MACHINE_TYPE}&quot;
    sshKeyName: &quot;${AWS_SSH_KEY_NAME}&quot;
  subnets:
    - id : &quot;${AWS_SUBNET_ID}&quot;
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfig
metadata:
  name: capa-mp-0
  namespace: default
spec:
  joinConfiguration:
    nodeRegistration:
      name: '{{ ds.meta_data.local_hostname }}'
      kubeletExtraArgs:
        cloud-provider: aws
</code></pre>
<h2><a class="header" href="#autoscaling" id="autoscaling">Autoscaling</a></h2>
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler"><code>cluster-autoscaler</code></a> can be used to scale MachinePools up and down.
Two providers are possible to use with CAPA MachinePools: <code>clusterapi</code>, or <code>aws</code>.</p>
<p>If the <code>AWS</code> autoscaler provider is used, each MachinePool needs to have an annotation set to prevent scale up/down races between
cluster-autoscaler and cluster-api. Example:</p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: capa-mp-0
  annotations:
    cluster.x-k8s.io/replicas-managed-by: &quot;external-autoscaler&quot;
spec:
  clusterName: capa
  replicas: 2
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfig
          name: capa-mp-0
      clusterName: capa
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachinePool
        name: capa-mp-0
      version: v1.25.0
</code></pre>
<p>When using GitOps, make sure to ignore differences in <code>spec.replicas</code> on MachinePools. Example when using ArgoCD:</p>
<pre><code class="language-yaml">  ignoreDifferences:
    - group: cluster.x-k8s.io
      kind: MachinePool
      jsonPointers:
        - /spec/replicas
</code></pre>
<h2><a class="header" href="#machine-pool-machines" id="machine-pool-machines">Machine pool machines</a></h2>
<p>With the feature gate <code>MachinePoolMachines=true</code>, you can enable creation of <code>Machine</code>/<code>AWSMachine</code> objects for nodes created by a <code>AWSMachinePool</code>. This is experimental and will be used to introduce features such as per-node health checks.</p>
<h1><a class="header" href="#multi-tenancy" id="multi-tenancy">Multi-tenancy</a></h1>
<p>Starting from v0.6.5, single controller multi-tenancy is supported that allows using a different AWS Identity for each workload cluster.
For details, see the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/docs/proposal/20200506-single-controller-multitenancy.md">multi-tenancy proposal</a>.</p>
<p>For multi-tenancy support, a reference field (<code>identityRef</code>) is added to <code>AWSCluster</code>, which informs the controller of which identity to be used when reconciling the cluster.
If the identity provided exists in a different AWS account, this is the mechanism which informs the controller to provision a cluster in a different account.
Identities should have adequate permissions for CAPA to reconcile clusters.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: &quot;test&quot;
  namespace: &quot;test&quot;
spec:
  region: &quot;eu-west-1&quot;
  identityRef:
    kind: &lt;IdentityType&gt;
    name: &lt;IdentityName&gt;
</code></pre>
<p>Identity resources are used to describe IAM identities that will be used during reconciliation.
There are three identity types: AWSClusterControllerIdentity, AWSClusterStaticIdentity, and AWSClusterRoleIdentity.
Once an IAM identity is created in AWS, the corresponding values should be used to create a identity resource.</p>
<h2><a class="header" href="#awsclustercontrolleridentity" id="awsclustercontrolleridentity">AWSClusterControllerIdentity</a></h2>
<p>Before multi-tenancy support, all AWSClusters were being reconciled using the credentials that are used by Cluster API Provider AWS Controllers.
<code>AWSClusterControllerIdentity</code> is used to restrict the usage of controller credentials only to AWSClusters that are in <code>allowedNamespaces</code>.
Since CAPA controllers use a single set of credentials, <code>AWSClusterControllerIdentity</code> is a singleton, and can only be created with <code>name: default</code>.</p>
<p>For backward compatibility, <code>AutoControllerIdentityCreator</code> experimental feature is added, which is responsible to create the <code>AWSClusterControllerIdentity</code> singleton if it does not exist.</p>
<ul>
<li><strong>Feature status:</strong> Experimental</li>
<li><strong>Feature gate:</strong> AutoControllerIdentityCreator=true
<code>AutoControllerIdentityCreator</code> creates <code>AWSClusterControllerIdentity</code> singleton with empty <code>allowedNamespaces</code> (allowedNamespaces: {}) to grant access to the <code>AWSClusterControllerIdentity</code> from all namespaces.</li>
</ul>
<p>Example:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: &quot;test&quot;
  namespace: &quot;test&quot;
spec:
  region: &quot;eu-west-1&quot;
  identityRef:
    kind: AWSClusterControllerIdentity
    name: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
metadata:
  name: &quot;default&quot;
spec:
  allowedNamespaces: {}  # matches all namespaces
</code></pre>
<p><code>AWSClusterControllerIdentity</code> is immutable to avoid any unwanted overrides to the allowed namespaces, especially during upgrading clusters.</p>
<h2><a class="header" href="#awsclusterstaticidentity" id="awsclusterstaticidentity">AWSClusterStaticIdentity</a></h2>
<p><code>AWSClusterStaticIdentity</code> represents static AWS credentials, which are stored in a <code>Secret</code>.</p>
<p>Example: Below, an <code>AWSClusterStaticIdentity</code> is created that allows access to the <code>AWSClusters</code> that are in “test” namespace.
The identity credentials that will be used by “test” AWSCluster are stored in “test-account-creds” secret.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: &quot;test&quot;
  namespace: &quot;test&quot;
spec:
  region: &quot;eu-west-1&quot;
  identityRef:
    kind: AWSClusterStaticIdentity
    name: test-account
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterStaticIdentity
metadata:
  name: &quot;test-account&quot;
spec:
  secretRef: test-account-creds
  allowedNamespaces:
    selector:
      matchLabels:
        cluster.x-k8s.io/ns: &quot;testlabel&quot;
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    cluster.x-k8s.io/ns: &quot;testlabel&quot;
  name: &quot;test&quot;
---
apiVersion: v1
kind: Secret
metadata:
  name: &quot;test-account-creds&quot;
  namespace: capa-system
stringData:
 AccessKeyID: AKIAIOSFODNN7EXAMPLE
 SecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
</code></pre>
<h2><a class="header" href="#awsclusterroleidentity" id="awsclusterroleidentity">AWSClusterRoleIdentity</a></h2>
<p><code>AWSClusterRoleIdentity</code> allows CAPA to assume a role either in the same or another AWS account, using the STS::AssumeRole API.
The assumed role could be used by the AWSClusters that is in the <code>allowedNamespaces</code>.</p>
<p>Example:
Below, an <code>AWSClusterRoleIdentity</code> instance, which will be used by <code>AWSCluster</code> “test”, is created.
This role will be assumed by the source identity at runtime. Source identity can be of any identity type.
Role is assumed in the beginning once and after, whenever the assumed role’s credentials are expired.</p>
<p>This snippet illustrates the connection between <code>AWSCluster</code>and the <code>AWSClusterRoleIdentity</code>, however this is not a working example.
Please view a full example below.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: &quot;test&quot;
  namespace: &quot;test&quot;
spec:
  region: &quot;eu-west-1&quot;
  identityRef:
    kind: AWSClusterRoleIdentity
    name: test-account-role
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterRoleIdentity
metadata:
  name: &quot;test-account-role&quot;
spec:
  allowedNamespaces:
  - &quot;test&quot; # allows only &quot;test&quot; namespace to use this identity
  roleARN: &quot;arn:aws:iam::123456789:role/CAPARole&quot;
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity # use the singleton for root auth
    name: default
</code></pre>
<p>Nested role assumption is also supported.
Example: Below, “multi-tenancy-nested-role” will be assumed by “multi-tenancy-role”, which will be assumed by the “default” <code>AWSClusterControllerIdentity</code></p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterRoleIdentity
metadata:
  name: multi-tenancy-role
spec:
  allowedNamespaces:
    list: []
  durationSeconds: 900 # default and min value is 900 seconds
  roleARN: arn:aws:iam::11122233344:role/multi-tenancy-role
  sessionName: multi-tenancy-role-session
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity
    name: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterRoleIdentity
metadata:
  name: multi-tenancy-nested-role
spec:
  allowedNamespaces:
    list: []
  roleARN: arn:aws:iam::11122233355:role/multi-tenancy-nested-role
  sessionName: multi-tenancy-nested-role-session
  sourceIdentityRef:
    kind: AWSClusterRoleIdentity
    name: multi-tenancy-role
</code></pre>
<h3><a class="header" href="#necessary-permissions-for-assuming-a-role" id="necessary-permissions-for-assuming-a-role">Necessary permissions for assuming a role:</a></h3>
<p>There are multiple AWS assume role permissions that need to be configured in order for the assume role to work:</p>
<ul>
<li>
<p>The source identity (user/role specified in the source identity field) should have IAM policy permissions that enable it to perform sts:AssumeRole operation.</p>
<pre><code class="language-json">{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
</code></pre>
</li>
<li>
<p>The target role (can be in a different AWS account) must be configured to allow the source user/role (or all users in an AWS account) to assume into it by setting a trust policy:</p>
<pre><code class="language-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
  {
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Principal&quot;: {
      &quot;AWS&quot;: &quot;arn:aws:iam::111111111111:root&quot;
        // &quot;AWS&quot;: &quot;arn:aws:iam::111111111111:role/role-used-during-cluster-bootstrap&quot;
    },
    &quot;Action&quot;: &quot;sts:AssumeRole&quot;
  }
]
}
</code></pre>
</li>
</ul>
<p>Both of these permissions can be enabled via clusterawsadm as documented <a href="topics/using-clusterawsadm-to-fulfill-prerequisites.html#cross-account-role-assumption">here</a>.</p>
<h3><a class="header" href="#examples-1" id="examples-1">Examples</a></h3>
<p>This is a deployable example which uses the <code>AWSClusterRoleIdentity</code> “test-account-role” to assume into the <code>arn:aws:iam::123456789:role/CAPARole</code> role in the target account.
This example assumes that the <code>CAPARole</code> has already been configured in the target account.</p>
<p>Finally, we inform the <code>Cluster</code> to use our <code>AWSCluster</code>type to provision a cluster in the target account specified by the <code>identityRef</code> section.</p>
<p><strong>Note</strong></p>
<p>By default the <code>AutoControllerIdentityCreator=true</code> feature gate is set to <code>true</code> <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/412d310654c6b05f1b4bc3d319f6957a07c009c2/feature/feature.go?rgh-link-date=2022-03-23T14%3A57%3A46Z#L81">here</a>.
If this is not enabled for your cluster, you will need to enable the flag, or create your own default <code>AWSClusterControllerIdentity</code>.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
metadata:
  name: &quot;default&quot;
spec:
  allowedNamespaces: {}  # matches all namespaces
</code></pre>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterRoleIdentity
metadata:
  name: &quot;test-account-role&quot;
spec:
  allowedNamespaces: {} # matches all namespaces
  roleARN: &quot;arn:aws:iam::123456789:role/CAPARole&quot;
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity # use the singleton for root auth
    name: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: &quot;test-multi-tenant-workload&quot;
spec:
  region: &quot;eu-west-1&quot;
  identityRef:
    kind: AWSClusterRoleIdentity
    name: test-account-role
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: &quot;test-multi-tenant-workload&quot;
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    name: &quot;test-multi-tenant-workload&quot;
</code></pre>
<p>More specific examples can be referenced from the existing <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/">templates</a> directory.</p>
<p>In order to use the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template.yaml">EC2 template</a> with identity type, you can add the <code>identityRef</code> section to <code>kind: AWSCluster</code> spec section in the template. If you do not, CAPA will automatically add the default identity provider (which is usually your local account credentials).</p>
<p>Similarly, to use the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template-eks.yaml">EKS template</a> with identity type, you can add the <code>identityRef</code> section to <code>kind: AWSManagedControlPlane</code> spec section in the template. If you do not, CAPA will automatically add the default identity provider (which is usually your local account credentials).</p>
<h2><a class="header" href="#secure-access-to-identities" id="secure-access-to-identities">Secure Access to Identities</a></h2>
<p><code>allowedNamespaces</code> field is used to grant access to the namespaces to use Identities.
Only AWSClusters that are created in one of the Identity’s allowed namespaces can use that Identity.
<code>allowedNamespaces</code> are defined by providing either a list of namespaces or label selector to select namespaces.</p>
<h3><a class="header" href="#examples-2" id="examples-2">Examples</a></h3>
<p>An empty <code>allowedNamespaces</code> indicates that the Identity can be used by all namespaces.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
spec:
  allowedNamespaces:{}  # matches all namespaces
</code></pre>
<p>Having a nil <code>list</code> and a nil <code>selector</code> is the same with having an empty <code>allowedNamespaces</code> (Identity can be used by all namespaces).</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
spec:
  allowedNamespaces:
    list: nil
    selector: nil
</code></pre>
<p>A nil <code>allowedNamespaces</code> indicates that the Identity cannot be used from any namespace.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
spec:
  allowedNamespaces:  # this is same with not providing the field at all or allowedNamespaces: null
</code></pre>
<p>The union of namespaces that are matched by <code>selector</code> and the namespaces that are in the <code>list</code> is granted access to the identity.
The namespaces that are not in the list and not matching the selector will not have access.</p>
<p>Nil or empty <code>list</code> matches no namespaces. Nil or empty <code>selector</code> matches no namespaces.
If <code>list</code> is nil and <code>selector</code> is empty OR <code>list</code> is empty and <code>selector</code> is nil, Identity cannot be used from any namespace.
Because in this case, <code>allowedNamespaces</code> is not empty or nil, and neither <code>list</code> nor <code>selector</code> allows any namespaces, so the union is empty.</p>
<pre><code class="language-yaml"># Matches no namespaces
allowedNamespaces:
  list: []
</code></pre>
<pre><code class="language-yaml"># Matches no namespaces
allowedNamespaces:
  selector: {}
</code></pre>
<pre><code class="language-yaml"># Matches no namespaces
allowedNamespaces:
  list: null
  selector: {}
</code></pre>
<pre><code class="language-yaml"># Matches no namespaces
allowedNamespaces:
  list: []
  selector: {}
</code></pre>
<p><strong>Important</strong> The default behaviour of an empty label selector is to match all objects, however here we do not follow that behavior to avoid unintended access to the identities.
This is consistent with core cluster API selectors, e.g., Machine and ClusterResourceSet selectors. The result of matchLabels and matchExpressions are ANDed.</p>
<p>In Kubernetes selectors, <code>matchLabels</code> and <code>matchExpressions</code> are ANDed.
In the example below, list is empty/nil, so does not allow any namespaces and selector matches with only <code>default</code> namespace.
Since <code>list</code> and <code>selector</code> results are ORed, <code>default</code> namespace can use this identity.</p>
<pre><code class="language-yaml">kind: namespace
metadata:
  name: default
  labels:
    environment: dev
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
spec:
  allowedNamespaces:
    list: null # or []
    selector:
      matchLabels:
        namespace: default
      matchExpressions:
        - {key: environment, operator: In, values: [dev]}
</code></pre>
<h1><a class="header" href="#multitenancy-setup-with-eks-and-service-account" id="multitenancy-setup-with-eks-and-service-account">Multitenancy setup with EKS and Service Account</a></h1>
<p>See <a href="topics/./multitenancy.html">multitenancy</a> for more
details on enabling the functionality and the various options you can use.</p>
<p>In this example, we are going to see how to create the following architecture with cluster API:</p>
<pre><code>                                  AWS Account 1
                                 +--------------------+
                                 |                    |
                 +---------------+-&gt;EKS - (Managed)   |
                 |               |                    |
                 |               +--------------------+
 AWS Account 0   |                AWS Account 2
+----------------+---+           +--------------------+
|                |   |           |                    |
|  EKS - (Manager)---+-----------+-&gt;EKS - (Managed)   |
|                |   |           |                    |
+----------------+---+           +--------------------+
                 |                AWS Account 3
                 |               +--------------------+
                 |               |                    |
                 +---------------+-&gt;EKS - (Managed)   |
                                 |                    |
                                 +--------------------+
</code></pre>
<p>And specifically, we will only include:</p>
<ul>
<li>AWS Account 0 (aka Manager account used by management cluster where cluster API controllers reside)</li>
<li>AWS Account 1 (aka Managed account used for EKS-managed workload clusters)</li>
</ul>
<h2><a class="header" href="#prerequisites-2" id="prerequisites-2">Prerequisites</a></h2>
<ul>
<li>A bootstrap cluster (kind)</li>
<li>AWS CLI installed</li>
<li>2 (or more) AWS accounts</li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases">clusterawsadm</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api/releases">clusterctl</a></li>
</ul>
<h2><a class="header" href="#set-variables" id="set-variables">Set variables</a></h2>
<p><strong>Note:</strong> the credentials below are the ones of the manager account</p>
<p>Export the following environment variables:</p>
<ul>
<li>AWS_REGION</li>
<li>AWS_ACCESS_KEY_ID</li>
<li>AWS_SECRET_ACCESS_KEY</li>
<li>AWS_SESSION_TOKEN (if you are using Multi-factor authentication)</li>
<li>AWS_MANAGER_ACCOUNT_ID</li>
<li>AWS_MANAGED_ACCOUNT_ID</li>
<li>OIDC_PROVIDER_ID=”WeWillReplaceThisLater”</li>
</ul>
<h2><a class="header" href="#prepare-the-manager-account" id="prepare-the-manager-account">Prepare the manager account</a></h2>
<p>As explained in the <a href="topics/./eks/prerequisites.html">EKS prerequisites page</a>, we need a couple of roles in the account to build the cluster, <code>clusterawsadm</code> CLI can take care of it.</p>
<p>We know that the CAPA provider in the Manager account should be able to assume roles in the Managed account (AWS Account 1).</p>
<p>We can create a clusterawsadm configuration that adds an inline policy to the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code> role.</p>
<pre><code class="language-bash">envsubst &gt; bootstrap-manager-account.yaml &lt;&lt; EOL
apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  eks: # This section should be changed accordingly to your requirements
    iamRoleCreation: false
    managedMachinePool:
      disable: true
    fargate:
      disable: false
  clusterAPIControllers: # This is the section that really matter
    disabled: false
    extraStatements:
    - Action:
      - &quot;sts:AssumeRole&quot;
      Effect: &quot;Allow&quot;
      Resource: [&quot;arn:aws:iam::${AWS_MANAGED_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io&quot;]
    trustStatements:
    - Action:
      - &quot;sts:AssumeRoleWithWebIdentity&quot;
      Effect: &quot;Allow&quot;
      Principal:
        Federated:
        - &quot;arn:aws:iam::${AWS_MANAGER_ACCOUNT_ID}:oidc-provider/oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}&quot;
      Condition:
        &quot;ForAnyValue:StringEquals&quot;:
          &quot;oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}:sub&quot;:
            - system:serviceaccount:capi-providers:capa-controller-manager
            - system:serviceaccount:capa-eks-control-plane-system:capa-eks-control-plane-controller-manager # Include if also using EKS
EOL
</code></pre>
<p>Let’s provision the Manager role with:</p>
<pre><code>clusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap-manager-account.yaml
</code></pre>
<h2><a class="header" href="#manager-cluster" id="manager-cluster">Manager cluster</a></h2>
<p>The following commands assume you have the AWS credentials for the Manager account exposed, and your kube context is pointing to the bootstrap cluster.</p>
<h3><a class="header" href="#install-cluster-api-provider-in-the-bootstrap-cluster" id="install-cluster-api-provider-in-the-bootstrap-cluster">Install cluster API provider in the bootstrap cluster</a></h3>
<pre><code class="language-bash">export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
export EKS=true
export EXP_MACHINE_POOL=true
clusterctl init --infrastructure aws --target-namespace capi-providers
</code></pre>
<h3><a class="header" href="#generate-the-cluster-configuration" id="generate-the-cluster-configuration">Generate the cluster configuration</a></h3>
<p><strong>NOTE:</strong> You might want to update the Kubernetes and VPC addon versions to one of the available versions when running this command.</p>
<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html">Kubernetes versions</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html">VPC CNI add-on versions</a> don’t forget to add the <code>v</code> prefix</li>
</ul>
<pre><code class="language-bash">export AWS_SSH_KEY_NAME=default
export VPC_ADDON_VERSION=&quot;v1.10.2-eksbuild.1&quot;
clusterctl generate cluster manager --flavor eks-managedmachinepool-vpccni --kubernetes-version v1.20.2 --worker-machine-count=3 &gt; manager-cluster.yaml
</code></pre>
<h3><a class="header" href="#apply-the-cluster-configuration" id="apply-the-cluster-configuration">Apply the cluster configuration</a></h3>
<pre><code class="language-bash">kubectl apply -f manager-cluster.yaml
</code></pre>
<p><strong>WAIT</strong>: time to have a drink, the cluster is creating and we will have to wait for it to be there before continuing.</p>
<h3><a class="header" href="#iam-oidc-identity-provider" id="iam-oidc-identity-provider">IAM OIDC Identity provider</a></h3>
<p>Follow AWS documentation to create an OIDC provider https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html</p>
<h3><a class="header" href="#update-the-truststatement-above" id="update-the-truststatement-above">Update the TrustStatement above</a></h3>
<pre><code class="language-bash">export OIDC_PROVIDER_ID=&lt;OIDC_ID_OF_THE_CLUSTER&gt;
</code></pre>
<p>run the <a href="topics/./full-multitenancy-implementation.html#prepare-the-manager-account">Prepare the manager account</a> step again</p>
<h3><a class="header" href="#get-manager-cluster-credentials" id="get-manager-cluster-credentials">Get manager cluster credentials</a></h3>
<pre><code class="language-bash">kubectl --namespace=default get secret manager-user-kubeconfig \
   -o jsonpath={.data.value} | base64 --decode \
   &gt; manager.kubeconfig
</code></pre>
<h3><a class="header" href="#install-the-capa-provider-in-the-manager-cluster" id="install-the-capa-provider-in-the-manager-cluster">Install the CAPA provider in the manager cluster</a></h3>
<p>Here we install the Cluster API providers into the manager cluster and create a service account to use the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code> role for the Management Components.</p>
<pre><code class="language-bash">export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
export EKS=true
export EXP_MACHINE_POOL=true
export AWS_CONTROLLER_IAM_ROLE=arn:aws:iam::${AWS_MANAGER_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io
clusterctl init --kubeconfig manager.kubeconfig --infrastructure aws --target-namespace capi-providers
</code></pre>
<h2><a class="header" href="#managed-cluster" id="managed-cluster">Managed cluster</a></h2>
<p>Time to build the managed cluster for pivoting the bootstrap cluster.</p>
<h3><a class="header" href="#generate-the-cluster-configuration-1" id="generate-the-cluster-configuration-1">Generate the cluster configuration</a></h3>
<p><strong>NOTE:</strong> As for the manager cluster you might want to update the Kubernetes and VPC addon versions.</p>
<pre><code class="language-bash">export AWS_SSH_KEY_NAME=default
export VPC_ADDON_VERSION=&quot;v1.10.2-eksbuild.1&quot;
clusterctl generate cluster managed --flavor eks-managedmachinepool-vpccni --kubernetes-version v1.20.2 --worker-machine-count=3 &gt; managed-cluster.yaml
</code></pre>
<p>Edit the file and add the following to the <code>AWSManagedControlPlane</code> resource spec to point the controller to the manager account when creating the cluster.</p>
<pre><code class="language-yaml">identityRef:
  kind: AWSClusterRoleIdentity
  name: managed-account
</code></pre>
<h3><a class="header" href="#create-the-identities" id="create-the-identities">Create the identities</a></h3>
<pre><code class="language-bash">envsubst &gt; cluster-role-identity.yaml &lt;&lt; EOL
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterRoleIdentity
metadata:
  name: managed-account
spec:
  allowedNamespaces: {} # This is unsafe since every namespace is allowed to use the role identity
  roleARN: arn:aws:iam::${AWS_MANAGED_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity
    name: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSClusterControllerIdentity
metadata:
  name: default
spec:
  allowedNamespaces: {}
EOL
</code></pre>
<h3><a class="header" href="#prepare-the-managed-account" id="prepare-the-managed-account">Prepare the managed account</a></h3>
<p><strong>NOTE:</strong> Expose the <strong>managed</strong> account credentials before running the following commands.</p>
<p>This configuration is adding the trustStatement in the cluster api controller role to allow the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code> in the manager account to assume it.</p>
<pre><code class="language-bash">envsubst &gt; bootstrap-managed-account.yaml &lt;&lt; EOL
apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  eks:
    iamRoleCreation: false # Set to true if you plan to use the EKSEnableIAM feature flag to enable automatic creation of IAM roles
    managedMachinePool:
      disable: true # Set to false to enable creation of the default node role for managed machine pools
    fargate:
      disable: false # Set to false to enable creation of the default role for the fargate profiles
  clusterAPIControllers:
    disabled: false
    trustStatements:
    - Action:
      - &quot;sts:AssumeRole&quot;
      Effect: &quot;Allow&quot;
      Principal:
        AWS:
        - &quot;arn:aws:iam::${AWS_MANAGER_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io&quot;
EOL
</code></pre>
<p>Let’s provision the Managed account with:</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap-managed-account.yaml
</code></pre>
<h3><a class="header" href="#apply-the-cluster-configuration-1" id="apply-the-cluster-configuration-1">Apply the cluster configuration</a></h3>
<p><strong>Note:</strong> Back to the <strong>manager</strong> account credentials</p>
<pre><code>kubectl --kubeconfig manager.kubeconfig apply -f cluster-role-identity.yaml
kubectl --kubeconfig manager.kubeconfig apply -f managed-cluster.yaml
</code></pre>
<p>Time for another drink, enjoy your multi-tenancy setup.</p>
<h1><a class="header" href="#eks-support-in-the-aws-provider" id="eks-support-in-the-aws-provider">EKS Support in the AWS Provider</a></h1>
<ul>
<li><strong>Feature status:</strong> Stable</li>
<li><strong>Feature gate (required):</strong> EKS=true</li>
<li><strong>Feature gate (optional):</strong> EKSEnableIAM=true,EKSAllowAddRoles=true</li>
</ul>
<h2><a class="header" href="#overview-1" id="overview-1">Overview</a></h2>
<p>The AWS provider supports creating EKS based cluster. Currently the following features are supported:</p>
<ul>
<li>Provisioning/managing an Amazon EKS Cluster</li>
<li>Upgrading the Kubernetes version of the EKS Cluster</li>
<li>Attaching a self-managed machines as nodes to the EKS cluster</li>
<li>Creating a machine pool and attaching it to the EKS cluster. See <a href="topics/eks/../machinepools.html">machine pool docs for details</a>.</li>
<li>Creating a managed machine pool and attaching it to the EKS cluster. See <a href="topics/eks/../machinepools.html">machine pool docs for details</a></li>
<li>Managing “EKS Addons”. See <a href="topics/eks/./addons.html">addons for further details</a></li>
<li>Creating an EKS fargate profile</li>
<li>Managing aws-iam-authenticator configuration</li>
</ul>
<p>Note: machine pools and fargate profiles are still classed as experimental.</p>
<p>The implementation introduces the following CRD kinds:</p>
<ul>
<li>AWSManagedControlPlane - specifies the EKS Cluster in AWS and used by the Cluster API AWS Managed Control plane (MACP)</li>
<li>AWSManagedMachinePool - defines the managed node pool for the cluster</li>
<li>EKSConfig - used by Cluster API bootstrap provider EKS (CABPE)</li>
</ul>
<p>And a number of new templates are available in the templates folder for creating a managed workload cluster.</p>
<h2><a class="header" href="#see-also" id="see-also">SEE ALSO</a></h2>
<ul>
<li><a href="topics/eks/prerequisites.html">Prerequisites</a></li>
<li><a href="topics/eks/enabling.html">Enabling EKS Support</a></li>
<li><a href="topics/eks/disabling.html">Disabling EKS Support</a></li>
<li><a href="topics/eks/creating-a-cluster.html">Creating a cluster</a></li>
<li><a href="topics/eks/eks-console.html">Using EKS Console</a></li>
<li><a href="topics/eks/addons.html">Using EKS Addons</a></li>
<li><a href="topics/eks/encryption.html">Enabling Encryption</a></li>
<li><a href="topics/eks/cluster-upgrades.html">Cluster Upgrades</a></li>
</ul>
<h1><a class="header" href="#prerequisites-3" id="prerequisites-3">Prerequisites</a></h1>
<p>To use EKS you must give the controller the required permissions. The easiest way to do this is by using <code>clusterawsadm</code>. For instructions on how to do this see the <a href="topics/eks/../using-clusterawsadm-to-fulfill-prerequisites.html">prerequisites</a>.</p>
<p>When using <code>clusterawsadm</code> and enabling EKS support a new IAM role will be created for you called <strong>eks-controlplane.cluster-api-provider-aws.sigs.k8s.io</strong>. This role is the IAM role that will be used for the EKS control plane if you don’t specify your own role and if <strong>EKSEnableIAM</strong> isn’t enabled (see the <a href="topics/eks/enabling.html">enabling docs</a> for further information).</p>
<p>Additionally using <code>clusterawsadm</code> will add permissions to the <strong>controllers.cluster-api-provider-aws.sigs.k8s.io</strong> policy for EKS to function properly.</p>
<h1><a class="header" href="#enabling-eks-support" id="enabling-eks-support">Enabling EKS Support</a></h1>
<p>Support for EKS is enabled by default when you use the AWS infrastructure provider. For example:</p>
<pre><code class="language-shell">clusterctl init --infrastructure aws
</code></pre>
<h2><a class="header" href="#enabling-optional-eks-features" id="enabling-optional-eks-features">Enabling optional <strong>EKS</strong> features</a></h2>
<p>There are additional EKS experimental features that are disabled by default. The sections below cover how to enable these features.</p>
<h3><a class="header" href="#machine-pools" id="machine-pools">Machine Pools</a></h3>
<p>To enable support for machine pools the <strong>MachinePool</strong> feature flag must be set to to <strong>true</strong>. This can be done using the <strong>EXP_MACHINE_POOL</strong> environment variable:</p>
<pre><code class="language-shell">export EXP_MACHINE_POOL=true
clusterctl init --infrastructure aws
</code></pre>
<p>See the <a href="topics/eks/../machinepools.html">machine pool documentation</a> for further information.</p>
<p>NOTE: you will need to enable the creation of the default IAM role. The easiest way is using <code>clusterawsadm</code>, for instructions see the <a href="topics/eks/../using-clusterawsadm-to-fulfill-prerequisites.html">prerequisites</a>.</p>
<h3><a class="header" href="#iam-roles-per-cluster" id="iam-roles-per-cluster">IAM Roles Per Cluster</a></h3>
<p>By default EKS clusters will use the same IAM roles (i.e. control plane, node group roles). There is a feature that allows each cluster to have its own IAM roles. This is done by enabling the <strong>EKSEnableIAM</strong> feature flag. This can be done before running <code>clusterctl init</code> by using the the <strong>CAPA_EKS_IAM</strong> environment variable:</p>
<pre><code class="language-shell">export CAPA_EKS_IAM=true
clusterctl init --infrastructure aws
</code></pre>
<p>NOTE: you will need the correct prerequisities for this. The easiest way is using <code>clusterawsadm</code> and setting <code>iamRoleCreation</code> to true, for instructions see the <a href="topics/eks/../using-clusterawsadm-to-fulfill-prerequisites.html">prerequisites</a>.</p>
<h3><a class="header" href="#additional-control-plane-roles" id="additional-control-plane-roles">Additional Control Plane Roles</a></h3>
<p>You can add additional roles to the control plane role that is created for an EKS cluster. To use this you must enable the <strong>EKSAllowAddRoles</strong> feature flag. This can be done before running <code>clusterctl init</code> by using the <strong>CAPA_EKS_ADD_ROLES</strong> environment variable:</p>
<pre><code class="language-shell">export CAPA_EKS_IAM=true
export CAPA_EKS_ADD_ROLES=true
clusterctl init --infrastructure aws
</code></pre>
<p>NOTE: to use this feature you must also enable the <strong>CAPA_EKS_IAM</strong> feature.</p>
<h3><a class="header" href="#eks-fargate-profiles" id="eks-fargate-profiles">EKS Fargate Profiles</a></h3>
<p>You can use Fargate Profiles with EKS. To use this you must enable the <strong>EKSFargate</strong> feature flag. This can be done before running <code>clusterctl init</code> by using the <strong>EXP_EKS_FARGATE</strong> environmnet variable:</p>
<pre><code class="language-shell">export EXP_EKS_FARGATE=true
clusterctl init --infrastructure aws
</code></pre>
<p>NOTE: you will need to enable the creation of the default Fargate IAM role. The easiest way is using <code>clusterawsadm</code> and using the <code>fargate</code> configuration option, for instructions see the <a href="topics/eks/../using-clusterawsadm-to-fulfill-prerequisites.html">prerequisites</a>.</p>
<h1><a class="header" href="#pod-networking" id="pod-networking">Pod Networking</a></h1>
<p>When creating a EKS cluster the Amazon VPC CNI will be used by default for Pod Networking.</p>
<blockquote>
<p>When using the AWS Console to create an EKS cluster with a Kubernetes version of v1.18 or greater you are required to select a specific version of the VPC CNI to use.</p>
</blockquote>
<h2><a class="header" href="#using-the-vpc-cni-addon" id="using-the-vpc-cni-addon">Using the VPC CNI Addon</a></h2>
<p>You can use an explicit version of the Amazon VPC CNI by using the <strong>vpc-cni</strong> EKS addon. See the <a href="topics/eks/./addons.html">addons</a> documentation for further details of how to use addons.</p>
<h2><a class="header" href="#using-custom-vpc-cni-configuration" id="using-custom-vpc-cni-configuration">Using Custom VPC CNI Configuration</a></h2>
<p>If your use case demands <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html">custom networking</a> VPC CNI configuration you might already be familiar with the <a href="https://github.com/aws/amazon-vpc-cni-k8s">helm chart</a> which helps with the process. This gives you access to ENI Configs and you can set Environment Variables on the <code>aws-node</code> DaemonSet where the VPC CNI runs. CAPA is able to tune the same DaemonSet through Kubernetes.</p>
<p>The following example shows how to turn on custom network config and set a <a href="https://github.com/aws/amazon-vpc-cni-k8s#eni_config_label_def">label definition</a>.</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  vpcCni:
    env:
    - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG
      value: &quot;true&quot; 
    - name: ENABLE_PREFIX_DELEGATION
      value: &quot;true&quot;
</code></pre>
<h3><a class="header" href="#increase-node-pod-limit" id="increase-node-pod-limit">Increase node pod limit</a></h3>
<p>You can increase the pod limit per-node as <a href="https://aws.amazon.com/blogs/containers/amazon-vpc-cni-increases-pods-per-node-limits/">per the upstream AWS documentation</a>. You’ll need to enable the <code>vpc-cni</code> plugin addon on your EKS cluster as well as enable prefix assignment mode through the <code>ENABLE_PREFIX_DELEGATION</code> environment variable.</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  vpcCni:
    env:
    - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG
      value: &quot;true&quot; 
    - name: ENABLE_PREFIX_DELEGATION
      value: &quot;true&quot;
  addons:
  - name: vpc-cni
    version: &lt;replace_with_version&gt;
    conflictResolution: overwrite
  associateOIDCProvider: true
  disableVPCCNI: false
</code></pre>
<h3><a class="header" href="#using-secondary-cidrs" id="using-secondary-cidrs">Using Secondary CIDRs</a></h3>
<p>EKS allows users to assign a <a href="https://www.eksworkshop.com/beginner/160_advanced-networking/secondary_cidr/">secondary CIDR range</a> for pods to be  assigned. Below are how to get CAPA to generate ENIConfigs in both the managed and unmanaged VPC configurations. </p>
<blockquote>
<p>Secondary CIDR functionality will not work unless you enable custom network config too.</p>
</blockquote>
<h4><a class="header" href="#managed-dynamic-vpc" id="managed-dynamic-vpc">Managed (dynamic) VPC</a></h4>
<p>Default configuration for CAPA is to manage the VPC and all the subnets for you dynamically. It will create and delete them along with your cluster. In this method all you need to do is set a SecondaryCidrBlock to one of the allowed two IPv4 CIDR blocks: 100.64.0.0/10 and 198.19.0.0/16. CAPA will automatically generate subnets and ENIConfigs for you and the VPC CNI will do the rest.</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  secondaryCidrBlock: 100.64.0.0/10
  vpcCni:
    env:
    - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG
      value: &quot;true&quot; 
  
</code></pre>
<h4><a class="header" href="#unmanaged-static-vpc" id="unmanaged-static-vpc">Unmanaged (static) VPC</a></h4>
<p>In an unmanaged VPC configuration CAPA will create no VPC or subnets and will instead assign the cluster pieces to the IDs you pass. In order to get ENIConfigs to generate you will need to add tags to the subnet you created and want to use as the secondary subnets for your pods. This is done through tagging the subnets with the following tag: <code>sigs.k8s.io/cluster-api-provider-aws/association=secondary</code>.</p>
<blockquote>
<p>Setting <code>SecondaryCidrBlock</code> in this configuration will be ignored and no subnets are created.</p>
</blockquote>
<h2><a class="header" href="#using-an-alternative-cni" id="using-an-alternative-cni">Using an alternative CNI</a></h2>
<p>There may be scenarios where you do not want to use the Amazon VPC CNI. EKS supports a number of alternative CNIs such as Calico, Cilium, and Weave Net (see <a href="https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html">docs</a> for full list).</p>
<p>There are a number of ways to install an alternative CNI into the cluster. One option is to use a <a href="https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-resource-set.html">ClusterResourceSet</a> to apply the required artifacts to a newly provisioned cluster.</p>
<p>When using an alternative CNI you will want to delete the Amazon VPC CNI, especially for a cluster using v1.17 or less. This can be done via the <strong>disableVPCCNI</strong> property of the <strong>AWSManagedControlPlane</strong>:</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  region: &quot;eu-west-2&quot;
  sshKeyName: &quot;capi-management&quot;
  version: &quot;v1.18.0&quot;
  disableVPCCNI: true
</code></pre>
<p>If you are replacing Amazon VPC CNI with your own helm managed instance, you will need to set <code>AWSManagedControlPlane.spec.disableVPCCNI</code> to <code>true</code> and add <code>&quot;aws.cluster.x-k8s.io/prevent-deletion&quot;: &quot;true&quot;</code> label on the Daemonset. This label is needed so <code>aws-node</code> daemonset is not reaped during CNI reconciliation.</p>
<p>The following example shows how to label your aws-node Daemonset.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  annotations:
    ...
  generation: 1
  labels:
    app.kubernetes.io/instance: aws-vpc-cni
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: aws-node
    app.kubernetes.io/version: v1.15.1
    helm.sh/chart: aws-vpc-cni-1.15.1
    aws.cluster.x-k8s.io/prevent-deletion: true
</code></pre>
<blockquote>
<p>You cannot set <strong>disableVPCCNI</strong> to true if you are using the VPC CNI addon.</p>
</blockquote>
<p>Some alternative CNIs provide for the replacement of kube-proxy, such as in <a href="https://projectcalico.docs.tigera.io/maintenance/ebpf/enabling-ebpf#configure-kube-proxy">Calico</a> and <a href="https://docs.cilium.io/en/stable/gettingstarted/kubeproxy-free/">Cilium</a>. When enabling the kube-proxy alternative, the kube-proxy installed by EKS must be deleted. This can be done via the <strong>disable</strong> property of <strong>kubeProxy</strong> in <strong>AWSManagedControlPlane</strong>:</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  region: &quot;eu-west-2&quot;
  sshKeyName: &quot;capi-management&quot;
  version: &quot;v1.18.0&quot;
  disableVPCCNI: true
  kubeProxy:
    disable: true
</code></pre>
<blockquote>
<p>You cannot set <strong>disable</strong> to true in <strong>kubeProxy</strong> if you are using the kube-proxy addon.</p>
</blockquote>
<h2><a class="header" href="#additional-information" id="additional-information">Additional Information</a></h2>
<p>See the <a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html">AWS documentation</a> for further details of EKS pod networking.</p>
<h1><a class="header" href="#creating-a-eks-cluster" id="creating-a-eks-cluster">Creating a EKS cluster</a></h1>
<p>New “eks” cluster templates have been created that you can use with <code>clusterctl</code> to create a EKS cluster. To create a EKS cluster with self-managed nodes (a.k.a machines):</p>
<pre><code class="language-bash">clusterctl generate cluster capi-eks-quickstart --flavor eks --kubernetes-version v1.22.9 --worker-machine-count=3 &gt; capi-eks-quickstart.yaml
</code></pre>
<p>To create a EKS cluster with a managed node group (a.k.a managed machine pool):</p>
<pre><code class="language-bash">clusterctl generate cluster capi-eks-quickstart --flavor eks-managedmachinepool --kubernetes-version v1.22.9 --worker-machine-count=3 &gt; capi-eks-quickstart.yaml
</code></pre>
<p>NOTE: When creating an EKS cluster only the <strong>MAJOR.MINOR</strong> of the <code>-kubernetes-version</code> is taken into consideration.</p>
<p>By default CAPA relies on the default EKS cluster upgrade policy, which at the moment of writing is EXTENDED support.
See more info about <a href="https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html">cluster upgrade policy</a></p>
<h2><a class="header" href="#kubeconfig" id="kubeconfig">Kubeconfig</a></h2>
<p>When creating an EKS cluster 2 kubeconfigs are generated and stored as secrets in the management cluster. This is different to when you create a non-managed cluster using the AWS provider.</p>
<h3><a class="header" href="#user-kubeconfig" id="user-kubeconfig">User kubeconfig</a></h3>
<p>This should be used by users that want to connect to the newly created EKS cluster. The name of the secret that contains the kubeconfig will be <code>[cluster-name]-user-kubeconfig</code> where you need to replace <strong>[cluster-name]</strong> with the name of your cluster. The <strong>-user-kubeconfig</strong> in the name indicates that the kubeconfig is for the user use.</p>
<p>To get the user kubeconfig for a cluster named <code>managed-test</code> you can run a command similar to:</p>
<pre><code class="language-bash">kubectl --namespace=default get secret managed-test-user-kubeconfig \
   -o jsonpath={.data.value} | base64 --decode \
   &gt; managed-test.kubeconfig
</code></pre>
<h3><a class="header" href="#cluster-api-capi-kubeconfig" id="cluster-api-capi-kubeconfig">Cluster API (CAPI) kubeconfig</a></h3>
<p>This kubeconfig is used internally by CAPI and shouldn’t be used outside of the management server. It is used by CAPI to perform operations, such as draining a node. The name of the secret that contains the kubeconfig will be <code>[cluster-name]-kubeconfig</code> where you need to replace <strong>[cluster-name]</strong> with the name of your cluster. Note that there is NO <code>-user</code> in the name.</p>
<p>There are three keys in the CAPI kubeconfig for eks clusters:</p>
<table><thead><tr><th>keys</th><th>purpose</th></tr></thead><tbody>
<tr><td>value</td><td>contains a complete kubeconfig with the cluster admin user and token embedded</td></tr>
<tr><td>relative</td><td>contains a kubeconfig with the cluster admin user, referencing the token file in a relative path - assumes you are mounting all the secret keys in the same dir</td></tr>
<tr><td>single-file</td><td>contains the same token embedded in the complete kubeconfig, it is separated into a single file so that existing APIMachinery can reload the token file when the secret is updated</td></tr>
</tbody></table>
<p>The secret contents are regenerated every <code>sync-period</code> as the token that is embedded in the kubeconfig and token file is only valid for a short period of time. When EKS support is enabled the maximum sync period is 10 minutes. If you try to set <code>--sync-period</code> to greater than 10 minutes then an error will be raised.</p>
<h1><a class="header" href="#eks-console" id="eks-console">EKS Console</a></h1>
<p>To use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/view-workloads.html">Amazon EKS Console</a> to view workloads running in an EKS cluster created using the AWS provider (CAPA) you can do the following:</p>
<ol>
<li>
<p>Create a new policy with the required IAM permissions for the console. This <a href="https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html#policy_example3">example</a> can be used. For example, a policy called <code>EKSViewNodesAndWorkloads</code>.</p>
</li>
<li>
<p>Assign the policy created in step 1) to a IAM user or role for the users of your EKS cluster</p>
</li>
<li>
<p>Map the IAM user or role from step 2) to a Kubernetes user that has the RBAC permissions to view the Kubernetes resources. This needs to be done via the <code>aws-auth</code> configmap (used by <code>aws-iam-authenticator</code>) which is generated by the AWS provider. This mapping can be specified using in the <code>AWSManagedControlPlane</code>, for example:</p>
</li>
</ol>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  region: &quot;eu-west-2&quot;
  sshKeyName: &quot;capi-management&quot;
  version: &quot;v1.18.0&quot;
  iamAuthenticatorConfig:
    mapRoles:
    - username: &quot;kubernetes-admin&quot;
      rolearn: &quot;arn:aws:iam::1234567890:role/AdministratorAccess&quot;
      groups:
      - &quot;system:masters&quot;
</code></pre>
<blockquote>
<p>In the sample above the <strong>arn:aws:iam::1234567890:role/AdministratorAccess</strong> IAM role has the <strong>EKSViewNodesAndWorkloads</strong> policy attached (created in step 1.)</p>
</blockquote>
<h1><a class="header" href="#eks-addons" id="eks-addons">EKS Addons</a></h1>
<p><a href="https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/">EKS Addons</a> can be used with EKS clusters created using Cluster API Provider AWS.</p>
<p>Addons are supported in EKS clusters using Kubernetes v1.18 or greater.</p>
<h2><a class="header" href="#installing-addons" id="installing-addons">Installing addons</a></h2>
<p>To install an addon you need to declare them by specifying the name, version and optionally how conflicts should be resolved in the <code>AWSManagedControlPlane</code>. For example:</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  region: &quot;eu-west-2&quot;
  sshKeyName: &quot;capi-management&quot;
  version: &quot;v1.18.0&quot;
  addons:
    - name: &quot;vpc-cni&quot;
      version: &quot;v1.6.3-eksbuild.1&quot;
      conflictResolution: &quot;overwrite&quot;
</code></pre>
<p>Valid values are:</p>
<pre><code>none
overwrite
preserve
</code></pre>
<p><em>Note</em>: For <code>conflictResolution</code> <code>overwrite</code> is the <strong>default</strong> behaviour. That means, if not otherwise specified, it’s
set to <code>overwrite</code>. Review <a href="https://docs.aws.amazon.com/eks/latest/APIReference/API_CreateAddon.html#AmazonEKS-CreateAddon-request-resolveConflicts">API Documentation</a> for detailed behavior.</p>
<p>Additionally, there is a cluster <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html#flavors">flavor</a>
called <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template-eks-managedmachinepool-vpccni.yaml">eks-managedmachinepool-vpccni</a> that you can use with <strong>clusterctl</strong>:</p>
<pre><code class="language-shell">clusterctl generate cluster my-cluster --kubernetes-version v1.18.0 --flavor eks-managedmachinepool-vpccni &gt; my-cluster.yaml
</code></pre>
<h2><a class="header" href="#updating-addons" id="updating-addons">Updating Addons</a></h2>
<p>To update the version of an addon you need to edit the <code>AWSManagedControlPlane</code> instance and update the version of the addon you want to update. Using the example from the previous section we would do:</p>
<pre><code class="language-yaml">...
  addons:
    - name: &quot;vpc-cni&quot;
      version: &quot;v1.7.5-eksbuild.1&quot;
      conflictResolution: &quot;overwrite&quot;
...
</code></pre>
<p><em>Note</em>: For <code>conflictResolution</code> <code>none</code>, updating may fail if a change was made to the addon that is unexpected by EKS. Review <a href="https://docs.aws.amazon.com/eks/latest/APIReference/API_UpdateAddon.html#AmazonEKS-UpdateAddon-request-resolveConflicts">API Documentation</a> for detailed behavior on conflict resolution.</p>
<h2><a class="header" href="#deleting-addons" id="deleting-addons">Deleting Addons</a></h2>
<p>To delete an addon from a cluster you need to edit the <code>AWSManagedControlPlane</code> instance and remove the entry for the addon you want to delete.</p>
<h2><a class="header" href="#viewing-installed-addons" id="viewing-installed-addons">Viewing installed addons</a></h2>
<p>You can see what addons are installed on your EKS cluster by looking in the <code>Status</code>  of the <code>AWSManagedControlPlane</code> instance.</p>
<p>Additionally you can run the following command:</p>
<pre><code class="language-bash">clusterawsadm eks addons list-installed -n &lt;&lt;eksclustername&gt;&gt;
</code></pre>
<h2><a class="header" href="#viewing-available-addons" id="viewing-available-addons">Viewing available addons</a></h2>
<p>You can see what addons are available to your EKS cluster by running the following command:</p>
<pre><code class="language-bash">clusterawsadm eks addons list-available -n &lt;&lt;eksclustername&gt;&gt;
</code></pre>
<h1><a class="header" href="#enabling-encryption" id="enabling-encryption">Enabling Encryption</a></h1>
<p>To enable encryption when creating a cluster you need to create a new KMS key that has an alias name starting with <code>cluster-api-provider-aws-</code>.</p>
<p>For example, <code>arn:aws:kms:eu-north-1:12345678901:alias/cluster-api-provider-aws-key1</code>.</p>
<p>You then need to specify the <strong>key ARN</strong>  in the <code>encryptionConfig</code> of the <code>AWSManagedControlPlane</code>:</p>
<pre><code class="language-yaml">kind: AWSManagedControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;capi-managed-test-control-plane&quot;
spec:
  ...
  encryptionConfig:
    provider: &quot;arn:aws:kms:eu-north-1:12345678901:key/351f5544-6130-42e4-8786-2c85e546fc2d&quot;
    resources:
    - &quot;secrets&quot;
</code></pre>
<blockquote>
<p>You must use the ARN of the key and not the ARN of the alias.</p>
</blockquote>
<h2><a class="header" href="#custom-kms-alias-prefix" id="custom-kms-alias-prefix">Custom KMS Alias Prefix</a></h2>
<p>If you would like to use a different alias prefix then you can use the <code>kmsAliasPrefix</code> in the optional configuration file for <strong>clusterawsadm</strong>:</p>
<pre><code class="language-bash">clusterawsadm bootstrap iam create-stack --config custom-prefix.yaml

</code></pre>
<p>And the contents of the configuration file:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  eks:
    enable: true
    kmsAliasPrefix: &quot;my-prefix-*

</code></pre>
<h1><a class="header" href="#eks-cluster-upgrades" id="eks-cluster-upgrades">EKS Cluster Upgrades</a></h1>
<h2><a class="header" href="#control-plane-upgrade" id="control-plane-upgrade">Control Plane Upgrade</a></h2>
<p>Upgrading the Kubernetes version of the control plane is supported by the provider. To perform an upgrade you need to update the <code>version</code> in the spec of the <code>AWSManagedControlPlane</code>. Once the version has changed the provider will handle the upgrade for you.</p>
<p>You can only upgrade a EKS cluster by 1 minor version at a time. If you attempt to upgrade the version by more then 1 minor version the provider will ensure the upgrade is done in multiple steps of 1 minor version. For example upgrading from v1.15 to v1.17 would result in your cluster being upgraded v1.15 -&gt; v1.16 first and then v1.16 to v1.17.</p>
<h1><a class="header" href="#rosa-support-in-the-aws-provider" id="rosa-support-in-the-aws-provider">ROSA Support in the AWS Provider</a></h1>
<ul>
<li><strong>Feature status:</strong> Experimental</li>
<li><strong>Feature gate (required):</strong> ROSA=true</li>
</ul>
<h2><a class="header" href="#overview-2" id="overview-2">Overview</a></h2>
<p>The AWS provider supports creating Red Hat OpenShift Service on AWS (<a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/aws">ROSA</a>) based cluster. Currently the following features are supported:</p>
<ul>
<li>Provisioning/Deleting a ROSA cluster with hosted control planes (<a href="https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4">HCP</a>)</li>
</ul>
<p>The implementation introduces the following CRD kinds:</p>
<ul>
<li><code>ROSAControlPlane</code> - specifies the ROSA Cluster in AWS</li>
<li><code>ROSACluster</code> - needed only to satisfy cluster-api contract</li>
</ul>
<p>A new template is available in the templates folder for creating a managed ROSA workload cluster.</p>
<h2><a class="header" href="#see-also-1" id="see-also-1">SEE ALSO</a></h2>
<ul>
<li><a href="topics/rosa/enabling.html">Enabling ROSA Support</a></li>
<li><a href="topics/rosa/creating-a-cluster.html">Creating a cluster</a></li>
<li><a href="topics/rosa/creating-rosa-machinepools.html">Creating MachinePools</a></li>
<li><a href="topics/rosa/upgrades.html">Upgrades</a></li>
<li><a href="topics/rosa/external-auth.html">External Auth Providers</a></li>
<li><a href="topics/rosa/support.html">Support</a></li>
</ul>
<h1><a class="header" href="#enabling-rosa-support" id="enabling-rosa-support">Enabling ROSA Support</a></h1>
<p>To enable support for ROSA HCP clusters, the ROSA feature flag must be set to true. This can be done using the <strong>EXP_ROSA</strong> environment variable.</p>
<p>Make sure to set up your AWS environment first as described <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">here</a>.</p>
<pre><code class="language-shell">export EXP_ROSA=&quot;true&quot;
export EXP_MACHINE_POOL=&quot;true&quot;
clusterctl init --infrastructure aws
</code></pre>
<h2><a class="header" href="#troubleshooting-1" id="troubleshooting-1">Troubleshooting</a></h2>
<p>To check the feature-gates for the Cluster API controller run the following command:</p>
<pre><code class="language-shell">$ kubectl get deploy capi-controller-manager -n capi-system -o yaml
</code></pre>
<p>the feature gate container arg should have <code>MachinePool=true</code> as shown below.</p>
<pre><code class="language-yaml">spec:
  containers:
  - args:
    - --feature-gates=MachinePool=true,ClusterTopology=true,...
</code></pre>
<p>To check the feature-gates for the Cluster API AWS controller run the following command:</p>
<pre><code class="language-shell">$ kubectl get deploy capa-controller-manager -n capa-system -o yaml
</code></pre>
<p>the feature gate arg should have <code>ROSA=true</code> as shown below.</p>
<pre><code class="language-yaml">spec:
  containers:
  - args:
    - --feature-gates=ROSA=true,...
</code></pre>
<h1><a class="header" href="#creating-a-rosa-hcp-cluster" id="creating-a-rosa-hcp-cluster">Creating a ROSA HCP cluster</a></h1>
<h2><a class="header" href="#permissions" id="permissions">Permissions</a></h2>
<h3><a class="header" href="#authentication-using-service-account-credentials" id="authentication-using-service-account-credentials">Authentication using service account credentials</a></h3>
<p>CAPA controller requires service account credentials to be able to provision ROSA HCP clusters:</p>
<ol>
<li>
<p>Visit <a href="https://console.redhat.com/iam/service-accounts">https://console.redhat.com/iam/service-accounts</a> and create a service account. If you already have a service account, you can skip this step.</p>
<p>For every newly created service account, make sure to activate the account using the <a href="https://github.com/openshift/rosa">ROSA command line tool</a>. First, log in using your newly created service account</p>
<pre><code class="language-shell">rosa login --client-id ... --client-secret ...
</code></pre>
<p>Then activate your service account</p>
<pre><code class="language-shell">rosa whoami
</code></pre>
</li>
<li>
<p>Create a new kubernetes secret with the service account credentials to be referenced later by <code>ROSAControlPlane</code></p>
<pre><code class="language-shell">kubectl create secret generic rosa-creds-secret \
  --from-literal=ocmClientID='....' \
  --from-literal=ocmClientSecret='eyJhbGciOiJIUzI1NiIsI....' \
  --from-literal=ocmApiUrl='https://api.openshift.com'
</code></pre>
<p>Note: to consume the secret without the need to reference it from your <code>ROSAControlPlane</code>, name your secret as <code>rosa-creds-secret</code> and create it in the CAPA manager namespace (usually <code>capa-system</code>)</p>
<pre><code class="language-shell">kubectl -n capa-system create secret generic rosa-creds-secret \
  --from-literal=ocmClientID='....' \
  --from-literal=ocmClientSecret='eyJhbGciOiJIUzI1NiIsI....' \
  --from-literal=ocmApiUrl='https://api.openshift.com'
</code></pre>
</li>
</ol>
<h3><a class="header" href="#authentication-using-sso-offline-token-deprecated" id="authentication-using-sso-offline-token-deprecated">Authentication using SSO offline token (DEPRECATED)</a></h3>
<p>The SSO offline token is being deprecated and it is recommended to use service account credentials instead, as described above.</p>
<ol>
<li>
<p>Visit https://console.redhat.com/openshift/token to retrieve your SSO offline authentication token</p>
</li>
<li>
<p>Create a credentials secret within the target namespace with the token to be referenced later by <code>ROSAControlePlane</code></p>
<pre><code class="language-shell">    kubectl create secret generic rosa-creds-secret \
        --from-literal=ocmToken='eyJhbGciOiJIUzI1NiIsI....' \
    --from-literal=ocmApiUrl='https://api.openshift.com'
</code></pre>
<p>Alternatively, you can edit the CAPA controller deployment to provide the credentials</p>
<pre><code class="language-shell">    kubectl edit deployment -n capa-system capa-controller-manager
</code></pre>
<p>and add the following environment variables to the manager container</p>
<pre><code class="language-yaml">    env:
      - name: OCM_TOKEN
        value: &quot;&lt;token&gt;&quot;
      - name: OCM_API_URL
        value: &quot;https://api.openshift.com&quot; # or https://api.stage.openshift.com
</code></pre>
</li>
</ol>
<h3><a class="header" href="#migration-from-offline-token-to-service-account-authentication" id="migration-from-offline-token-to-service-account-authentication">Migration from offline token to service account authentication</a></h3>
<ol>
<li>
<p>Visit <a href="https://console.redhat.com/iam/service-accounts">https://console.redhat.com/iam/service-accounts</a> and create a new service account.</p>
</li>
<li>
<p>If you previously used kubernetes secret to specify the OCM credentials secret, edit the secret:</p>
<pre><code class="language-shell">    kubectl edit secret rosa-creds-secret
</code></pre>
<p>where you will remove the <code>ocmToken</code> credentials and add base64 encoded <code>ocmClientID</code> and <code>ocmClientSecret</code> credentials like so:</p>
<pre><code class="language-yaml">apiVersion: v1
 data:
   ocmApiUrl: aHR0cHM6Ly9hcGkub3BlbnNoaWZ0LmNvbQ==
   ocmClientID: Y2xpZW50X2lk...
   ocmClientSecret: Y2xpZW50X3NlY3JldA==...
 kind: Secret
 type: Opaque
</code></pre>
</li>
<li>
<p>If you previously used capa manager deployment to specify the OCM offline token as environment variable, edit the manager deployment</p>
<pre><code class="language-shell">    kubectl -n capa-system edit deployment capa-controller-manager
</code></pre>
<p>and remove the <code>OCM_TOKEN</code> and <code>OCM_API_URL</code> variables, followed by <code>kubectl -n capa-system rollout restart deploy capa-controller-manager</code>. Then create the new default secret in the <code>capa-system</code> namespace with</p>
<pre><code class="language-shell">    kubectl -n capa-system create secret generic rosa-creds-secret \
      --from-literal=ocmClientID='....' \
      --from-literal=ocmClientSecret='eyJhbGciOiJIUzI1NiIsI....' \
      --from-literal=ocmApiUrl='https://api.openshift.com'
</code></pre>
</li>
</ol>
<h2><a class="header" href="#prerequisites-4" id="prerequisites-4">Prerequisites</a></h2>
<p>Follow the guide <a href="https://docs.aws.amazon.com/ROSA/latest/userguide/getting-started-hcp.html">here</a> up until <a href="https://docs.aws.amazon.com/ROSA/latest/userguide/getting-started-hcp.html#create-hcp-cluster-cli">“Create a ROSA with HCP Cluster”</a> to install the required tools and setup the prerequisite infrastructure. Once Step 3 is done, you will be ready to proceed with creating a ROSA HCP cluster using cluster-api.</p>
<p>Note; Skip the “Create the required IAM roles and OpenID Connect configuration” step from the prerequisites url above and use the templates/cluster-template-rosa-role-config.yaml to generate a ROSARoleConfig CR to create the required account roles, operator roles &amp; managed OIDC provider.</p>
<h2><a class="header" href="#creating-the-cluster" id="creating-the-cluster">Creating the cluster</a></h2>
<ol>
<li>
<p>Prepare the environment:</p>
<pre><code class="language-bash">export OPENSHIFT_VERSION=&quot;4.19.0&quot;
export AWS_REGION=&quot;us-west-2&quot;
export AWS_AVAILABILITY_ZONE=&quot;us-west-2a&quot;
export AWS_ACCOUNT_ID=&quot;&lt;account_id&gt;&quot;
export AWS_CREATOR_ARN=&quot;&lt;user_arn&gt;&quot; # can be retrieved e.g. using `aws sts get-caller-identity`

# Note: if using templates/cluster-template-rosa.yaml set the below env variables
export OIDC_CONFIG_ID=&quot;&lt;oidc_id&gt;&quot; # OIDC config id creating previously with `rosa create oidc-config`
export ACCOUNT_ROLES_PREFIX=&quot;ManagedOpenShift-HCP&quot; # prefix used to create account IAM roles with `rosa create account-roles`
export OPERATOR_ROLES_PREFIX=&quot;capi-rosa-quickstart&quot;  # prefix used to create operator roles with `rosa create operator-roles --prefix &lt;PREFIX_NAME&gt;`

# Note: if using templates/cluster-template-rosa-role-config.yaml set the below env variables
export ACCOUNT_ROLES_PREFIX=&quot;capa&quot; # prefix can be change to preferable prefix with max 4 chars
export OPERATOR_ROLES_PREFIX=&quot;capa&quot;  # prefix can be change to preferable prefix with max 4 chars

# subnet IDs created earlier
export PUBLIC_SUBNET_ID=&quot;subnet-0b54a1111111111111&quot;
export PRIVATE_SUBNET_ID=&quot;subnet-05e72222222222222&quot;
</code></pre>
</li>
<li>
<p>Render the cluster manifest using the ROSA HCP cluster template:</p>
<p>a. Using templates/cluster-template-rosa.yaml</p>
<p>Note: The AWS role name must be no more than 64 characters in length. Otherwise an error will be returned. Truncate values exceeding 64 characters.</p>
<pre><code class="language-shell">clusterctl generate cluster &lt;cluster-name&gt; --from templates/cluster-template-rosa.yaml &gt; rosa-capi-cluster.yaml
</code></pre>
<p>b. Using templates/cluster-template-rosa-role-config.yaml</p>
<pre><code class="language-shell">clusterctl generate cluster &lt;cluster-name&gt; --from templates/cluster-template-rosa-role-config.yaml &gt; rosa-capi-cluster.yaml
</code></pre>
</li>
<li>
<p>If a credentials secret was created earlier, edit <code>ROSAControlPlane</code> to reference it:</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: ROSAControlPlane
metadata:
  name: &quot;capi-rosa-quickstart-control-plane&quot;
spec:
  credentialsSecretRef:
    name: rosa-creds-secret
...
</code></pre>
</li>
<li>
<p>Provide an AWS identity reference</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: ROSAControlPlane
metadata:
  name: &quot;capi-rosa-quickstart-control-plane&quot;
spec:
  identityRef:
    kind: &lt;IdentityType&gt;
    name: &lt;IdentityName&gt;
...
</code></pre>
<p>Otherwise, make sure the following <code>AWSClusterControllerIdentity</code> singleton exists in your management cluster:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSClusterControllerIdentity
metadata:
  name: &quot;default&quot;
spec:
  allowedNamespaces: {}  # matches all namespaces
</code></pre>
<p>see <a href="topics/rosa/../multitenancy.html">Multi-tenancy</a> for more details</p>
</li>
<li>
<p>Finally apply the manifest to create your ROSA cluster:</p>
<pre><code class="language-shell">kubectl apply -f rosa-capi-cluster.yaml
</code></pre>
</li>
</ol>
<p>see <a href="https://cluster-api-aws.sigs.k8s.io/crd/#controlplane.cluster.x-k8s.io/v1beta2.ROSAControlPlane">ROSAControlPlane CRD Reference</a> for all possible configurations.</p>
<h1><a class="header" href="#creating-machinepools" id="creating-machinepools">Creating MachinePools</a></h1>
<p>Cluster API Provider AWS (CAPA) has experimental support for managed ROSA MachinePools through the infrastructure type <code>ROSAMachinePool</code>. A <code>ROSAMachinePool</code> is responsible for orchestrating and bootstraping a group of EC2 machines into kubernetes nodes.</p>
<h3><a class="header" href="#using-clusterctl-to-deploy-2" id="using-clusterctl-to-deploy-2">Using <code>clusterctl</code> to deploy</a></h3>
<p>To deploy a MachinePool / ROSAMachinePool via <code>clusterctl generate</code> use the template located <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/templates/cluster-template-rosa-machinepool.yaml">here</a>.</p>
<p>Make sure to set up your environment as described <a href="topics/rosa/./creating-a-cluster.html#creating-the-cluster">here</a>.</p>
<pre><code class="language-shell">clusterctl generate cluster my-cluster --from templates/cluster-template-rosa-machinepool &gt; my-cluster.yaml
</code></pre>
<h2><a class="header" href="#example" id="example">Example</a></h2>
<p>Below is an example of the resources needed to create a ROSA MachinePool.</p>
<pre><code class="language-yaml">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: &quot;${CLUSTER_NAME}-pool-0&quot;
spec:
  clusterName: &quot;${CLUSTER_NAME}&quot;
  replicas: 1
  template:
    spec:
      clusterName: &quot;${CLUSTER_NAME}&quot;
      bootstrap:
        dataSecretName: &quot;&quot;
      infrastructureRef:
        name: &quot;${CLUSTER_NAME}-pool-0&quot;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: ROSAMachinePool
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ROSAMachinePool
metadata:
  name: &quot;${CLUSTER_NAME}-pool-0&quot;
spec:
  nodePoolName: &quot;nodepool-0&quot;
  instanceType: &quot;m5.xlarge&quot;
  subnet: &quot;${PRIVATE_SUBNET_ID}&quot;
  version: &quot;${OPENSHIFT_VERSION}&quot;
</code></pre>
<p>see <a href="https://cluster-api-aws.sigs.k8s.io/crd/#infrastructure.cluster.x-k8s.io/v1beta2.ROSAMachinePool">ROSAMachinePool CRD Reference</a> for all possible configurations.</p>
<h1><a class="header" href="#upgrades" id="upgrades">Upgrades</a></h1>
<h2><a class="header" href="#control-plane-upgrade-1" id="control-plane-upgrade-1">Control Plane Upgrade</a></h2>
<p>Upgrading the OpenShift version of the control plane is supported by the provider. To perform an upgrade you need to update the <code>version</code> in the spec of the <code>ROSAControlPlane</code>. Once the version has changed the provider will handle the upgrade for you.</p>
<p>Upgrading y-stream version ex; v4.16.x to v4.17.x required the version gate acknowledgement. By default the versionGate is set to WaitForAcknowledge in the <code>ROSAControlPlane</code> CR. When upgrading to y-stream version the versionGate should be set to Acknowledge or AlwaysAcknowledge.</p>
<h5><a class="header" href="#note-1" id="note-1">Note:</a></h5>
<p>When the versionGate is set to ‘Acknowledge’, it will revert to ‘WaitForAcknowledge’ once the upgrade is successfully completed. However, if the versionGate is set to ‘AlwaysAcknowledge’, it will remain set to ‘AlwaysAcknowledge’ after the upgrade is successfully completed.</p>
<p>The available upgrades versions for the <code>ROSAControlPlane</code> will be listed under <code>ROSAControlPlane.status.availableUpgrades</code></p>
<p>The version channel group <code>ROSAControlPlane.spec.channelGroup</code> defaults to stable. However, it can be set to eus, fast, candidate, or nightly. Changing the version channel group will change the <code>ROSAControlPlane.status.availableUpgrades</code> accordingly. Note that the use of channel groups other than stable may require additional permissions.</p>
<p>The Upgrade state can be checked in the conditions under <code>ROSAControlPlane.status</code>.</p>
<h2><a class="header" href="#machinepool-upgrade" id="machinepool-upgrade">MachinePool Upgrade</a></h2>
<p>Upgrading the OpenShift version of the MachinePools is supported by the provider and can be performed independently from the Control Plane upgrades. To perform an upgrade you need to update the <code>version</code> in the spec of the <code>ROSAMachinePool</code>. Once the version has changed the provider will handle the upgrade for you.</p>
<p>The available upgrades versions for the <code>ROSAMachinePool</code> will be listed under <code>ROSAMachinePool.status.availableUpgrades</code></p>
<p>The Upgrade state can be checked in the conditions under <code>ROSAMachinePool.status</code>.</p>
<p>The version of the ROSAMachinePool can’t be greater than its ROSAControlPlane version.</p>
<h1><a class="header" href="#external-auth-providers-byoi" id="external-auth-providers-byoi">External Auth Providers (BYOI)</a></h1>
<p>ROSA HCP allows you to Bring Your Own Identity (BYOI) to manage and authenticate cluster users.</p>
<h2><a class="header" href="#enabling" id="enabling">Enabling</a></h2>
<p>To enable this feature, <code>enableExternalAuthProviders</code> field should be set to <code>true</code> on cluster creation. Changing this field afterwards will have no effect:</p>
<pre><code class="language-yaml">---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: ROSAControlPlane
metadata:
  name: &quot;capi-rosa-quickstart-control-plane&quot;
spec:
  enableExternalAuthProviders: true
  ....
</code></pre>
<p>Note: This feature requires OpenShift version <code>4.15.5</code> or newer.</p>
<h2><a class="header" href="#usage" id="usage">Usage</a></h2>
<p>After creating and configuring your OIDC provider of choice, the next step is to configure ROSAControlPlane <code>externalAuthProviders</code> as follows:</p>
<pre><code class="language-yaml">---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: ROSAControlPlane
metadata:
  name: &quot;capi-rosa-quickstart-control-plane&quot;
spec:
  enableExternalAuthProviders: true
  externalAuthProviders:
  - name: my-oidc-provider
    issuer:
      issuerURL: https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0 # e.g. if using Microsoft Entra ID
      audiences:  # audiences that will be trusted by the kube-apiserver
      - &quot;audience1&quot; # usually the client ID
    claimMappings:
      username:
        claim: email
        prefixPolicy: &quot;&quot;
      groups:
        claim: groups
  ....
</code></pre>
<p>Note: <code>oidcProviders</code> only accepts one entry at the moment.</p>
<h2><a class="header" href="#accessing-the-cluster" id="accessing-the-cluster">Accessing the cluster</a></h2>
<h3><a class="header" href="#setting-up-rbac" id="setting-up-rbac">Setting up RBAC</a></h3>
<p>When <code>enableExternalAuthProviders</code> is set to <code>true</code>, ROSA provider will generate a temporary admin kubeconfig secret in the same namespace named <code>&lt;cluster-name&gt;-bootstrap-kubeconfig</code>. This kubeconfig can be used to access the cluster to setup RBAC for OIDC users/groups.</p>
<p>The following example binds the <code>cluster-admin</code> role to an OIDC group, giving all users in that group admin permissions.</p>
<pre><code class="language-shell">kubectl get secret &lt;cluster-name&gt;-bootstrap-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; /tmp/capi-admin-kubeconfig
export KUBECONFIG=/tmp/capi-admin-kubeconfig

kubectl create clusterrolebinding oidc-cluster-admins --clusterrole cluster-admin --group &lt;group-id&gt;
</code></pre>
<p>Note: The generated bootstrap kubeconfig is only valid for 24h, and will not be usable afterwards. However, users can opt to manually delete the secret object to trigger the generation of a new one which will be valid for another 24h.</p>
<h3><a class="header" href="#login-using-the-cli" id="login-using-the-cli">Login using the cli</a></h3>
<p>The <a href="https://github.com/int128/kubelogin/tree/master">kubelogin kubectl plugin</a> can be used to login with OIDC credentials using the cli. </p>
<h3><a class="header" href="#configuring-openshift-console" id="configuring-openshift-console">Configuring OpenShift Console</a></h3>
<p>The OpenShift Console needs to be configured before it can be used to authenticate and login to the cluster. </p>
<ol>
<li>
<p>Setup a new client in your OIDC provider with the following Redirect URL: <code>&lt;console-url&gt;/auth/callback</code>. You can find the console URL in the status field of the <code>ROSAControlPlane</code> once the cluster is ready:</p>
<pre><code class="language-shell">kubectl get rosacontrolplane &lt;control-plane-name&gt; -o jsonpath='{.status.consoleURL}'
</code></pre>
</li>
<li>
<p>Create a new client secret in your OIDC provider and store the value in a kubernetes secret in the same namespace as your cluster:</p>
<pre><code class="language-shell">kubectl create secret generic console-client-secret --from-literal=clientSecret='&lt;client-secret-value&gt;' 
</code></pre>
</li>
<li>
<p>Configure <code>ROSAControlPlane</code> external auth provider with the created client:</p>
<pre><code class="language-yaml">---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: ROSAControlPlane
metadata:
  name: &quot;capi-rosa-quickstart-control-plane&quot;
spec:
  enableExternalAuthProviders: true
  externalAuthProviders:
  - name: my-oidc-provider
    issuer:
      issuerURL: https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0 # e.g. if using Microsoft Entra ID
      audiences:  # audiences that will be trusted by the kube-apiserver
      - &quot;audience1&quot;
      - &lt;console-client-id&gt; # &lt;----New
    claimMappings:
      username:
        claim: email
        prefixPolicy: &quot;&quot;
      groups:
        claim: groups
    oidcClients:  # &lt;----New
      - componentName: console
        componentNamespace: openshift-console
        clientID: &lt;console-client-id&gt;
        clientSecret:
          name: console-client-secret # secret name created in step 2
  ....
</code></pre>
</li>
</ol>
<p>see <a href="https://cluster-api-aws.sigs.k8s.io/crd/#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ROSAControlPlane CRD Reference</a> for all possible configurations.</p>
<h1><a class="header" href="#create-issue-for-rosa" id="create-issue-for-rosa">Create issue for ROSA</a></h1>
<p>When creating issue for ROSA HCP cluster, include the logs for the capa-controller-manager and capi-controller-manager deployment pods. The logs can be saved to text file using the commands below. Also include the yaml files for all the resources used to create the ROSA HCP cluster:</p>
<ul>
<li><code>Cluster</code></li>
<li><code>ROSAControlPlane</code></li>
<li><code>MachinePool</code></li>
<li><code>ROSAMachinePool</code></li>
</ul>
<pre><code class="language-shell">$ kubectl get pod -n capa-system 
NAME                                      READY   STATUS    RESTARTS   AGE
capa-controller-manager-77f5b946b-sddcg   1/1     Running   1          3d3h

$ kubectl logs -n capa-system capa-controller-manager-77f5b946b-sddcg &gt; capa-controller-manager-logs.txt

$ kubectl get pod -n capi-system 
NAME                                       READY   STATUS    RESTARTS   AGE
capi-controller-manager-78dc897784-f8gpn   1/1     Running   18         26d

$ kubectl logs -n capi-system capi-controller-manager-78dc897784-f8gpn &gt; capi-controller-manager-logs.txt
</code></pre>
<h1><a class="header" href="#bring-your-own-aws-infrastructure" id="bring-your-own-aws-infrastructure">Bring Your Own AWS Infrastructure</a></h1>
<p>Normally, Cluster API will create infrastructure on AWS when standing up a new workload cluster. However, it is possible to have Cluster API re-use external AWS infrastructure instead of creating its own infrastructure. </p>
<p>There are two possible ways to do this:</p>
<ul>
<li>By consuming existing AWS infrastructure</li>
<li>By using externally managed AWS infrastructure</li>
</ul>
<blockquote>
<p><strong>IMPORTANT NOTE</strong>: This externally managed AWS infrastructure should not be confused with EKS-managed clusters.</p>
</blockquote>
<p>Follow the instructions below to configure Cluster API to consume existing AWS infrastructure.</p>
<h2><a class="header" href="#consuming-existing-aws-infrastructure" id="consuming-existing-aws-infrastructure">Consuming Existing AWS Infrastructure</a></h2>
<h3><a class="header" href="#overview-3" id="overview-3">Overview</a></h3>
<p>CAPA supports using existing AWS resources while creating AWS Clusters which gives flexibility to the users to bring their own existing resources into the cluster instead of creating new resources again.</p>
<p>Follow the instructions below to configure Cluster API to consume existing AWS infrastructure.</p>
<h3><a class="header" href="#prerequisites-5" id="prerequisites-5">Prerequisites</a></h3>
<p>In order to have Cluster API consume existing AWS infrastructure, you will need to have already created the following resources:</p>
<ul>
<li>A VPC</li>
<li>One or more private subnets (subnets that do not have a route to an Internet gateway)</li>
<li>A NAT gateway for each private subnet, along with associated Elastic IP addresses (only needed if the nodes require access to the Internet, i.e. pulling public images)
<ul>
<li>A public subnet in the same Availability Zone (AZ) for each private subnet (this is required for NAT gateways to function properly)</li>
</ul>
</li>
<li>An Internet gateway for all public subnets (only required if the workload cluster is set to use an Internet facing load balancer or one or more NAT gateways exist in the VPC)</li>
<li>Route table associations that provide connectivity to the Internet through a NAT gateway (for private subnets) or the Internet gateway (for public subnets)</li>
<li>VPC endpoints for <code>ec2</code>, <code>elasticloadbalancing</code>, <code>secretsmanager</code> an <code>autoscaling</code> (if using MachinePools) when the private Subnets do not have a NAT gateway</li>
</ul>
<p>You will need the ID of the VPC and subnet IDs that Cluster API should use. This information is available via the AWS Management Console or the AWS CLI.</p>
<p>Note that there is no need to create an Elastic Load Balancer (ELB), security groups, or EC2 instances; Cluster API will take care of these items.</p>
<p>If you want to use existing security groups, these can be specified and new ones will not be created.</p>
<p>If you want to use an existing control load load balancer, specify its name.</p>
<h3><a class="header" href="#tagging-aws-resources" id="tagging-aws-resources">Tagging AWS Resources</a></h3>
<p>Cluster API itself does tag AWS resources it creates. The <code>sigs.k8s.io/cluster-api-provider-aws/cluster/&lt;cluster-name&gt;</code> (where <code>&lt;cluster-name&gt;</code> matches the <code>metadata.name</code> field of the Cluster object) tag, with a value of <code>owned</code>, tells Cluster API that it has ownership of the resource. In this case, Cluster API will modify and manage the lifecycle of the resource.</p>
<p>When consuming existing AWS infrastructure, the Cluster API AWS provider does not require any tags to be present. The absence of the tags on an AWS resource indicates to Cluster API that it should not modify the resource or attempt to manage the lifecycle of the resource.</p>
<p>However, the built-in Kubernetes AWS cloud provider <em>does</em> require certain tags in order to function properly. Specifically, all subnets where Kubernetes nodes reside should have the <code>kubernetes.io/cluster/&lt;cluster-name&gt;</code> tag present. Private subnets should also have the <code>kubernetes.io/role/internal-elb</code> tag with a value of 1, and public subnets should have the <code>kubernetes.io/role/elb</code> tag with a value of 1. These latter two tags help the cloud provider understand which subnets to use when creating load balancers.</p>
<p>Finally, if the controller manager isn’t started with the <code>--configure-cloud-routes: &quot;false&quot;</code> parameter, the route table(s) will also need the <code>kubernetes.io/cluster/&lt;cluster-name&gt;</code> tag. (This parameter can be added by customizing the <code>KubeadmConfigSpec</code> object of the <code>KubeadmControlPlane</code> object.)</p>
<blockquote>
<p><strong>Note</strong>: All the tagging of resources should be the responsibility of the users and are not managed by CAPA controllers.</p>
</blockquote>
<h3><a class="header" href="#configuring-the-awscluster-specification" id="configuring-the-awscluster-specification">Configuring the AWSCluster Specification</a></h3>
<p>Specifying existing infrastructure for Cluster API to use takes place in the specification for the AWSCluster object. Specifically, you will need to add an entry with the VPC ID and the IDs of all applicable subnets into the <code>network</code> field. Here is an example:</p>
<p>For EC2</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: AWSCluster
</code></pre>
<p>For EKS</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: AWSManagedControlPlane
</code></pre>
<pre><code class="language-yaml">spec:
  network:
    vpc:
      id: vpc-0425c335226437144
    subnets:
    - id: subnet-0261219d564bb0dc5
    - id: subnet-0fdcccba78668e013
</code></pre>
<p>When you use <code>kubectl apply</code> to apply the Cluster and AWSCluster specifications to the management cluster, Cluster API will use the specified VPC ID and subnet IDs, and will not create a new VPC, new subnets, or other associated resources. It <em>will</em>, however, create a new ELB and new security groups.</p>
<h3><a class="header" href="#placing-ec2-instances-in-specific-azs" id="placing-ec2-instances-in-specific-azs">Placing EC2 Instances in Specific AZs</a></h3>
<p>To distribute EC2 instances across multiple AZs, you can add information to the Machine specification. This is optional and only necessary if control over AZ placement is desired.</p>
<p>To tell Cluster API that an EC2 instance should be placed in a particular AZ but allow Cluster API to select which subnet in that AZ can be used, add this to the Machine specification:</p>
<pre><code class="language-yaml">spec:
  failureDomain: &quot;us-west-2a&quot;
</code></pre>
<p>If using a MachineDeployment, specify AZ placement like so:</p>
<pre><code class="language-yaml">spec:
  template:
    spec:
      failureDomain: &quot;us-west-2b&quot;
</code></pre>
<p>Note that all replicas within a MachineDeployment will reside in the same AZ.</p>
<h3><a class="header" href="#placing-ec2-instances-in-specific-subnets" id="placing-ec2-instances-in-specific-subnets">Placing EC2 Instances in Specific Subnets</a></h3>
<p>To specify that an EC2 instance should be placed in a specific subnet, add this to the AWSMachine specification:</p>
<pre><code class="language-yaml">spec:
  subnet:
    id: subnet-0a3507a5ad2c5c8c3
</code></pre>
<p>When using MachineDeployments, users can control subnet selection by adding information to the AWSMachineTemplate associated with that MachineDeployment, like this:</p>
<pre><code class="language-yaml">spec:
  template:
    spec:
      subnet:
        id: subnet-0a3507a5ad2c5c8c3
</code></pre>
<p>Users may either specify <code>failureDomain</code> on the Machine or MachineDeployment objects, <em>or</em> users may explicitly specify subnet IDs on the AWSMachine or AWSMachineTemplate objects. If both are specified, the subnet ID is used and the <code>failureDomain</code> is ignored.</p>
<h3><a class="header" href="#placing-ec2-instances-in-specific-external-vpcs" id="placing-ec2-instances-in-specific-external-vpcs">Placing EC2 Instances in Specific External VPCs</a></h3>
<p>CAPA clusters are deployed within a single VPC, but it’s possible to place machines that live in external VPCs. For this kind of configuration, we assume that all the VPCs have the ability to communicate, either through external peering, a transit gateway, or some other mechanism already established outside of CAPA. CAPA will not create a tunnel or manage the network configuration for any secondary VPCs.</p>
<p>The AWSMachineTemplate <code>subnet</code> field allows specifying filters or specific subnet ids for worker machine placement. If the filters or subnet id is specified in a secondary VPC, CAPA will place the machine in that VPC and subnet.</p>
<pre><code class="language-yaml">spec:
  template:
    spec:
      subnet:
        filters:
          name: &quot;vpc-id&quot;
          values:
            - &quot;secondary-vpc-id&quot;
      securityGroupOverrides:
        node: sg-04e870a3507a5ad2c5c8c2
        node-eks-additional: sg-04e870a3507a5ad2c5c8c1
</code></pre>
<h4><a class="header" href="#caveatsnotes" id="caveatsnotes">Caveats/Notes</a></h4>
<p>CAPA helpfully creates security groups for various roles in the cluster and automatically attaches them to workers. However, security groups are tied to a specific VPC, so workers placed in a VPC outside of the cluster will need to have these security groups created by some external process first and set in the <code>securityGroupOverrides</code> field, otherwise the ec2 creation will fail.</p>
<h3><a class="header" href="#security-groups" id="security-groups">Security Groups</a></h3>
<p>To use existing security groups for instances for a cluster, add this to the AWSCluster specification:</p>
<pre><code class="language-yaml">spec:
  network:
    securityGroupOverrides:
      bastion: sg-0350a3507a5ad2c5c8c3
      controlplane: sg-0350a3507a5ad2c5c8c3
      apiserver-lb: sg-0200a3507a5ad2c5c8c3
      node: sg-04e870a3507a5ad2c5c8c3
      lb: sg-00a3507a5ad2c5c8c3
</code></pre>
<p>Any additional security groups specified in an AWSMachineTemplate will be applied in addition to these overriden security groups.</p>
<p>To specify additional security groups for the control plane load balancer for a cluster, add this to the AWSCluster specification:</p>
<pre><code class="language-yaml">spec:
  controlPlaneLoadBalancer:
    additionalSecurityGroups:
    - sg-0200a3507a5ad2c5c8c3
    - ...
</code></pre>
<p>It’s also possible to override the cluster security groups for an individual AWSMachine or AWSMachineTemplate:</p>
<pre><code class="language-yaml">spec:
  SecurityGroupOverrides:
    node: sg-04e870a3507a5ad2c5c8c2
    node-eks-additional: sg-04e870a3507a5ad2c5c8c1
</code></pre>
<h3><a class="header" href="#control-plane-load-balancer" id="control-plane-load-balancer">Control Plane Load Balancer</a></h3>
<p>The cluster control plane is accessed through a Classic ELB. By default, Cluster API creates the Classic ELB. To use an existing Classic ELB, add its name to the AWSCluster specification:</p>
<pre><code class="language-yaml">spec:
  controlPlaneLoadBalancer:
    name: my-classic-elb-name
</code></pre>
<p>As control plane instances are added or removed, Cluster API will register and deregister them, respectively, with the Classic ELB.</p>
<p>It’s also possible to specify custom ingress rules for the control plane load balancer. To do so, add this to the AWSCluster specification:</p>
<pre><code class="language-yaml">spec:
  controlPlaneLoadBalancer:
    ingressRules:
      - description: &quot;example ingress rule&quot;
        protocol: &quot;-1&quot; # all
        fromPort: 7777
        toPort: 7777
</code></pre>
<blockquote>
<p><strong>WARNING:</strong> Using an existing Classic ELB is an advanced feature. <strong>If you use an existing Classic ELB, you must correctly configure it, and attach subnets to it.</strong></p>
<p>An incorrectly configured Classic ELB can easily lead to a non-functional cluster. We strongly recommend you let Cluster API create the Classic ELB.</p>
</blockquote>
<h3><a class="header" href="#control-plane-ingress-rules" id="control-plane-ingress-rules">Control Plane ingress rules</a></h3>
<p>It’s possible to specify custom ingress rules for the control plane itself. To do so, add this to the AWSCluster specification:</p>
<pre><code class="language-yaml">spec:
  network:
    additionalControlPlaneIngressRules:
    - description: &quot;example ingress rule&quot;
      protocol: &quot;-1&quot; # all
      fromPort: 7777
      toPort: 7777
</code></pre>
<h3><a class="header" href="#caveatsnotes-1" id="caveatsnotes-1">Caveats/Notes</a></h3>
<ul>
<li>When both public and private subnets are available in an AZ, CAPI will choose the private subnet in the AZ over the public subnet for placing EC2 instances.</li>
<li>If you configure CAPI to use existing infrastructure as outlined above, CAPI will <em>not</em> create an SSH bastion host. Combined with the previous bullet, this means you must make sure you have established some form of connectivity to the instances that CAPI will create.</li>
</ul>
<h2><a class="header" href="#using-externally-managed-aws-clusters" id="using-externally-managed-aws-clusters">Using Externally managed AWS Clusters</a></h2>
<h3><a class="header" href="#overview-4" id="overview-4">Overview</a></h3>
<p>Alternatively, CAPA supports externally managed cluster infrastructure which is useful for scenarios where a different persona is managing the cluster infrastructure out-of-band(external system) while still wanting to use CAPI for automated machine management.
Users can make use of existing AWSCluster CRDs in their externally managed clusters.</p>
<h3><a class="header" href="#how-to-use-externally-managed-clusters" id="how-to-use-externally-managed-clusters">How to use externally managed clusters?</a></h3>
<p>Users have to use <code>cluster.x-k8s.io/managed-by: &quot;&lt;name-of-system&gt;&quot;</code> annotation to depict that AWS resources are managed externally. If CAPA controllers come across this annotation in any of the AWS resources while reconciliation, then it will ignore the resource and not perform any reconciliation(including creating/modifying any of the AWS resources, or it’s status).</p>
<p>A predicate <code>ResourceIsNotExternallyManaged</code> is exposed by Cluster API which allows CAPA controllers to differentiate between externally managed vs CAPA managed resources. For example:</p>
<pre><code class="language-go">c, err := ctrl.NewControllerManagedBy(mgr).
        For(&amp;providerv1.InfraCluster{}).
        Watches(...).
        WithOptions(options).
        WithEventFilter(predicates.ResourceIsNotExternallyManaged(mgr.GetScheme(),logger.FromContext(ctx))).
        Build(r)
if err != nil {
	return errors.Wrap(err, &quot;failed setting up with a controller manager&quot;)
}
</code></pre>
<p>The external system must provide all required fields within the spec of the AWSCluster and must adhere to the CAPI provider contract and set the AWSCluster status to be ready when it is appropriate to do so.</p>
<blockquote>
<p><strong>IMPORTANT NOTE</strong>: Users should take care of skipping reconciliation in external controllers within mapping function while enqueuing requests. For example:</p>
<pre><code class="language-go">err := c.Watch(
  	&amp;source.Kind{Type: &amp;infrav1.AWSCluster{}},
  	handler.EnqueueRequestsFromMapFunc(func(a client.Object) []reconcile.Request {
  	   if annotations.IsExternallyManaged(awsCluster) {
  		    log.Info(&quot;AWSCluster is externally managed, skipping mapping.&quot;)
  		    return nil
  	   }
           return []reconcile.Request{
             {
  	         NamespacedName: client.ObjectKey{Namespace: c.Namespace, Name: c.Spec.InfrastructureRef.Name},
             },
          }}))
if err != nil {
   // handle it
}
</code></pre>
</blockquote>
<h3><a class="header" href="#caveats" id="caveats">Caveats</a></h3>
<p>Once the user has created externally managed AWSCluster, it is not allowed to convert it to CAPA managed cluster. However, converting from managed to externally managed is allowed.</p>
<p>User should only use this feature if their cluster infrastructure lifecycle management has constraints that the reference implementation does not support. See <a href="https://github.com/kubernetes-sigs/cluster-api/blob/10d89ceca938e4d3d94a1d1c2b60515bcdf39829/docs/proposals/20210203-externally-managed-cluster-infrastructure.md#user-stories">user stories</a> for more details.</p>
<h2><a class="header" href="#bring-your-own-byo-public-ipv4-addresses" id="bring-your-own-byo-public-ipv4-addresses">Bring your own (BYO) Public IPv4 addresses</a></h2>
<p>Cluster API provides a mechanism to allocate Elastic IPs from an existing Public IPv4 Pool that you brought to AWS[1].</p>
<p>Bringing your own Public IPv4 Pool (BYOIPv4) can serve as an alternative to purchasing Public IPs from AWS, especially considering the updated pricing model introduced in February 2024[2].</p>
<h3><a class="header" href="#supported-resources-for-byo-public-ipv4-pool" id="supported-resources-for-byo-public-ipv4-pool">Supported Resources for BYO Public IPv4 Pool</a></h3>
<p>The following resources can consume IPs from a BYO Public IPv4 Pool:</p>
<ul>
<li>NAT Gateways</li>
<li>Network Load Balancer for the API server</li>
<li>Machines</li>
</ul>
<p>Use <code>BYO Public IPv4</code> when you have custom IPv4 CIDR blocks advertised to AWS and want the cluster to automatically use IPs from the custom pool instead of Amazon-provided pools.</p>
<h3><a class="header" href="#prerequisites-and-limitations-for-byo-public-ipv4-pool" id="prerequisites-and-limitations-for-byo-public-ipv4-pool">Prerequisites and Limitations for BYO Public IPv4 Pool</a></h3>
<ul>
<li><strong>Regional Availability</strong>: BYOIPv4 is limited to selected AWS regions. Refer to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html#byoip-reg-avail">AWS Documentation for Regional Availability</a>.</li>
<li><strong>Provisioning and Advertising</strong>: IPv4 addresses must be provisioned and advertised to the AWS account before the cluster is installed.</li>
<li><strong>Network Border Group</strong>: Public IPv4 addresses are restricted to the network border group where the CIDR block has been advertised[3][4]. The <code>NetworkSpec.ElasticIpPool.PublicIpv4Pool</code> must match the cluster’s installation location.</li>
<li><strong>Resource Scope</strong>: Only NAT Gateways and the Network Load Balancer for the API server will consume IPs from the IPv4 pool defined in the network scope.</li>
<li><strong>Machine Assignment</strong>: Each machine must be assigned to the public IPv4 pool to consume IPs from the custom pool.</li>
</ul>
<h3><a class="header" href="#steps-to-configure-byo-public-ipv4-pool-for-core-infrastructure" id="steps-to-configure-byo-public-ipv4-pool-for-core-infrastructure">Steps to Configure BYO Public IPv4 Pool for Core Infrastructure</a></h3>
<p>CAPA supports BYO Public IPv4 for core components, including NAT Gateways and the Network Load Balancer for the internet-facing API server.</p>
<p>To specify a Public IPv4 Pool for core components, set the <code>spec.elasticIpPool</code> in the <code>AWSCluster</code> object:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: aws-cluster-localzone
spec:
  region: us-east-1
  networkSpec:
    vpc:
      elasticIpPool:
        publicIpv4Pool: ipv4pool-ec2-0123456789abcdef0 # Custom IPv4 pool ID
        publicIpv4PoolFallbackOrder: amazon-pool       # Fallback to AWS-provided pool
</code></pre>
<p>All Elastic IPs will be created by consuming from the pool <code>ipv4pool-ec2-0123456789abcdef0</code>.</p>
<h3><a class="header" href="#steps-to-configure-byo-public-ipv4-pool-for-machines" id="steps-to-configure-byo-public-ipv4-pool-for-machines">Steps to Configure BYO Public IPv4 Pool for Machines</a></h3>
<p>To configure a machine to consume IPs from a custom Public IPv4 Pool, specify the pool ID in the <code>AWSMachine</code> spec and set <code>PublicIP</code> to <code>true</code>:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachine
metadata:
  name: byoip-s55p4-bootstrap
spec:
  elasticIpPool:
    publicIpv4Pool: ipv4pool-ec2-0123456789abcdef0 # Custom IPv4 pool ID
    publicIpv4PoolFallbackOrder: amazon-pool       # Fallback to AWS-provided pool
  publicIP: true
</code></pre>
<h3><a class="header" href="#references" id="references">References</a></h3>
<p>[1] <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html">AWS BYOIPv4 Documentation</a>
[2] <a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/">AWS Blog: Public IPv4 Address Charges</a>
[3] <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html#byoip-onboard">AWS BYOIPv4 Onboarding Guide</a>
[4] <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/advertise-byoip-cidr.html">AWS CLI: Advertise BYOIPv4 CIDR</a></p>
<h1><a class="header" href="#specifying-the-iam-role-to-use-for-management-components" id="specifying-the-iam-role-to-use-for-management-components">Specifying the IAM Role to use for Management Components</a></h1>
<h2><a class="header" href="#prerequisites-6" id="prerequisites-6">Prerequisites</a></h2>
<p>To be able to specify the IAM role that the management components should run as your cluster must be set up with the ability to assume IAM roles using one of the following solutions:</p>
<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">IAM Roles for Service Accounts</a></li>
<li><a href="https://github.com/uswitch/kiam">Kiam</a></li>
<li><a href="https://github.com/jtblin/kube2iam">Kube2iam</a></li>
</ul>
<h2><a class="header" href="#setting-iam-role" id="setting-iam-role">Setting IAM Role</a></h2>
<p>Set the <code>AWS_CONTROLLER_IAM_ROLE</code> environment variable to the ARN of the IAM role to use when performing the <code>clusterctl init</code> command.</p>
<p>For example:</p>
<pre><code class="language-bash">export AWS_CONTROLLER_IAM_ROLE=arn:aws:iam::1234567890:role/capa-management-components
clusterctl init --infrastructure=aws
</code></pre>
<h2><a class="header" href="#iam-role-trust-policy" id="iam-role-trust-policy">IAM Role Trust Policy</a></h2>
<h3><a class="header" href="#iam-roles-for-service-accounts" id="iam-roles-for-service-accounts">IAM Roles for Service Accounts</a></h3>
<p>When creating the IAM role, the following trust policy will need to be used with the <code>AWS_ACCOUNT_ID</code>, <code>AWS_REGION</code> and <code>OIDC_PROVIDER_ID</code> environment variables replaced.</p>
<pre><code class="language-json">{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Sid&quot;: &quot;&quot;,
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Principal&quot;: {
        &quot;Federated&quot;: &quot;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}&quot;
      },
      &quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;,
      &quot;Condition&quot;: {
        &quot;ForAnyValue:StringEquals&quot;: {
          &quot;oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}:sub&quot;: [
            &quot;system:serviceaccount:capa-system:capa-controller-manager&quot;,
            &quot;system:serviceaccount:capi-system:capi-controller-manager&quot;,
            &quot;system:serviceaccount:capa-eks-control-plane-system:capa-eks-control-plane-controller-manager&quot;,
            &quot;system:serviceaccount:capa-eks-bootstrap-system:capa-eks-bootstrap-controller-manager&quot;,
          ]
        }
      }
    }
  ]
}
</code></pre>
<p>If you plan to use the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code> role created by clusterawsadm then you’ll need to add the following to your AWSIAMConfiguration:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  clusterAPIControllers:
    disabled: false
    trustStatements:
    - Action:
      - &quot;sts:AssumeRoleWithWebIdentity&quot;
      Effect: &quot;Allow&quot;
      Principal:
        Federated:
        - &quot;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}&quot;
      Condition:
        &quot;ForAnyValue:StringEquals&quot;:
          &quot;oidc.eks.${AWS_REGION}.amazonaws.com/id/${OIDC_PROVIDER_ID}:sub&quot;:
            - system:serviceaccount:capa-system:capa-controller-manager
            - system:serviceaccount:capa-eks-control-plane-system:capa-eks-control-plane-controller-manager # Include if also using EKS
</code></pre>
<p>With this you can then set <code>AWS_CONTROLLER_IAM_ROLE</code> to <code>arn:aws:iam::${AWS_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io</code></p>
<h3><a class="header" href="#kiam--kube2iam" id="kiam--kube2iam">Kiam / kube2iam</a></h3>
<p>When creating the IAM role, you will need to apply the <code>kubernetes.io/cluster/${CLUSTER_NAME}/role&quot;: &quot;enabled&quot;</code> tag to the role and use the following trust policy with the <code>AWS_ACCOUNT_ID</code> and <code>CLUSTER_NAME</code> environment variables correctly replaced.</p>
<pre><code class="language-json">{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Sid&quot;: &quot;&quot;,
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Principal&quot;: {
        &quot;Service&quot;: &quot;ec2.amazonaws.com&quot;
      },
      &quot;Action&quot;: &quot;sts:AssumeRole&quot;
    },
    {
      &quot;Sid&quot;: &quot;&quot;,
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Principal&quot;: {
        &quot;AWS&quot;: &quot;arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}.worker-node-role&quot;
      },
      &quot;Action&quot;: &quot;sts:AssumeRole&quot;
    }
  ]
}
</code></pre>
<p>If you plan to use the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code> role created by clusterawsadm then you’ll need to add the following to your AWSIAMConfiguration:</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  clusterAPIControllers:
    disabled: false
    trustStatements:
      - Action:
        - &quot;sts:AssumeRole&quot;
        Effect: &quot;Allow&quot;
        Principal:
          Service:
          - &quot;ec2.amazonaws.com&quot;
      - Action:
        - &quot;sts:AssumeRole&quot;
        Effect: &quot;Allow&quot;
        Principal:
          AWS:
          - &quot;arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}.worker-node-role&quot;
</code></pre>
<p>With this you can then set <code>AWS_CONTROLLER_IAM_ROLE</code> to <code>arn:aws:iam::${AWS_ACCOUNT_ID}:role/controllers.cluster-api-provider-aws.sigs.k8s.io</code></p>
<h1><a class="header" href="#external-aws-cloud-provider-and-aws-csi-driver" id="external-aws-cloud-provider-and-aws-csi-driver">External AWS Cloud Provider and AWS CSI Driver</a></h1>
<h2><a class="header" href="#overview-5" id="overview-5">Overview</a></h2>
<p>The support for in-tree cloud providers and the CSI drivers is coming to an end and CAPA supports various upgrade paths
to use <a href="https://github.com/kubernetes/cloud-provider-aws">external cloud provider (Cloud Controller Manager - CCM) </a> and external CSI drivers.
This document explains how to create a CAPA cluster with external CSI/CCM plugins and how to upgrade existing clusters that rely on in-tree providers.</p>
<h2><a class="header" href="#creating-clusters-with-external-csiccm-and-validating" id="creating-clusters-with-external-csiccm-and-validating">Creating clusters with external CSI/CCM and validating</a></h2>
<p>For clusters that will use external CCM, <code>cloud-provider: external</code> flag needs to be set in KubeadmConfig resources in both <code>KubeadmControlPlane</code> and <code>MachineDeployment</code> resources.</p>
<pre><code>clusterConfiguration:
  apiServer:
    extraArgs:
      cloud-provider: external
  controllerManager:
    extraArgs:
      cloud-provider: external
initConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: external
joinConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: external
</code></pre>
<p>External CCM and EBS CSI driver can be installed manually or using ClusterResourceSets (CRS) onto the CAPA workload cluster.
To install them with CRS, create a CRS resource on the management cluster with labels, for example <code>csi: external</code> and <code>ccm: external</code> labels.
Then, when creating <code>Cluster</code> objects for workload clusters that should have this CSR applied, create them with matching labels <code>csi: external</code> and <code>ccm: external</code> for CSI and CCM, respectively.</p>
<p>Manifests for installing the AWS CCM and the AWS EBS CSI driver are available from their respective
GitHub repositories (see <a href="https://github.com/kubernetes/cloud-provider-aws">here for the AWS CCM</a> and
<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">here for the AWS EBS CSI driver</a>).</p>
<p>An example of a workload cluster manifest with labels assigned for matching to a CRS can be found
<a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/tree/main/templates/cluster-template.yaml">here</a>.</p>
<h3><a class="header" href="#verifying-dynamically-provisioned-volumes-with-csi-driver" id="verifying-dynamically-provisioned-volumes-with-csi-driver">Verifying dynamically provisioned volumes with CSI driver</a></h3>
<p>Once you have the cluster with external CCM and CSI controller running successfully, you can test the CSI driver functioning with following steps after switching to workload cluster:</p>
<ol>
<li>Create a service (say,<code>nginx</code>)</li>
</ol>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
spec:
  clusterIP: None
  ports:
    - name: nginx-web
      port: 80
  selector:
    app: nginx
</code></pre>
<ol start="2">
<li>Create a storageclass and statefulset for the service created above with the persistent volume assigned to the storageclass:</li>
</ol>
<pre><code class="language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-ebs-volumes
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  csi.storage.k8s.io/fstype: xfs
  type: io1
  iopsPerGB: &quot;100&quot;
allowedTopologies:
  - matchLabelExpressions:
      - key: topology.ebs.csi.aws.com/zone
        values:
          - us-east-1a
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-statefulset
spec:
  serviceName: &quot;nginx-svc&quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: registry.k8s.io/nginx-slim:0.8
          ports:
            - name: nginx-web
              containerPort: 80
          volumeMounts:
            - name: nginx-volumes
              mountPath: /usr/share/nginx/html
      volumes:
        - name: nginx-volumes
          persistentVolumeClaim:
            claimName: nginx-volumes
  volumeClaimTemplates:
    - metadata:
        name: nginx-volumes
      spec:
        storageClassName: &quot;aws-ebs-volumes&quot;
        accessModes: [ &quot;ReadWriteOnce&quot; ]
        resources:
          requests:
            storage: 4Gi
</code></pre>
<ol start="3">
<li>Once you apply the above manifest, the EBS volumes will be created and attached to the worker nodes.</li>
</ol>
<blockquote>
<p><strong>IMPORTANT WARNING:</strong> The CRDs from the AWS EBS CSI driver and AWS external cloud provider gives issue while installing the respective controllers on the AWS Cluster, it doesn’t allow statefulsets to create the volume on existing EC2 instance.
We need the CSI controller deployment and CCM pinned to the control plane which has right permissions to create, attach
and mount the volumes to EC2 instances. To achieve this, you should add the node affinity rules to the CSI driver controller deployment and CCM DaemonSet manifests.</p>
<pre><code class="language-yaml">tolerations:
- key: node-role.kubernetes.io/master
  effect: NoSchedule
- effect: NoSchedule
  key: node-role.kubernetes.io/control-plane
affinity:
  nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
      - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
      - matchExpressions:
          - key: node-role.kubernetes.io/master
            operator: Exists
</code></pre>
</blockquote>
<h2><a class="header" href="#validated-upgrade-paths-for-existing-clusters" id="validated-upgrade-paths-for-existing-clusters">Validated upgrade paths for existing clusters</a></h2>
<p>From Kubernetes 1.23 onwards, <code>CSIMigrationAWS</code> flag is enabled by default, which requires the installation of <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">external CSI driver</a>, unless <code>CSIMigrationAWS</code> is disabled by the user.
For installing external CSI/CCM in the upgraded cluster, CRS can be used, see the section above for details.</p>
<p>CCM and CSI do not need to be migrated to use external plugins at the same time,
external CSI drivers works with in-tree CCM (Warning: using in-tree CSI with external CCM does not work).</p>
<p><strong>Following 3 upgrade paths are validated:</strong></p>
<ul>
<li>Scenario 1: During upgrade to v1.23.x, disabling <code>CSIMigrationAWS</code> flag and keep using in-tree CCM and CSI.</li>
<li>Scenario 2: During upgrade to v1.23.x, enabling <code>CSIMigrationAWS</code> flag and using in-tree CCM with external CSI.</li>
<li>Scenario 3: During upgrade to v1.23.x, enabling <code>CSIMigrationAWS</code> flag and using external CCM and CSI.</li>
</ul>
<table><thead><tr><th></th><th>CSI</th><th>CCM</th><th>feature-gate CSIMigrationAWS</th><th>external-cloud-volume-plugin</th></tr></thead><tbody>
<tr><td><strong>Scenario 1</strong></td><td></td><td></td><td></td><td></td></tr>
<tr><td>From Kubernetes  &lt; v1.23</td><td>in-tree</td><td>in-tree</td><td>off</td><td>NA</td></tr>
<tr><td>To Kubernetes    &gt;= v1.23</td><td>in-tree</td><td>in-tree</td><td>off</td><td>NA</td></tr>
<tr><td><strong>Scenario 2</strong></td><td></td><td></td><td></td><td></td></tr>
<tr><td>From Kubernetes  &lt; v1.23</td><td>in-tree</td><td>in-tree</td><td>off</td><td>NA</td></tr>
<tr><td>To Kubernetes    &gt;= v1.23</td><td>external</td><td>in-tree</td><td>on</td><td>NA</td></tr>
<tr><td><strong>Scenario 3</strong></td><td></td><td></td><td></td><td></td></tr>
<tr><td>From Kubernetes  &lt; v1.23</td><td>in-tree</td><td>in-tree</td><td>off</td><td>NA</td></tr>
<tr><td>To Kubernetes    &gt;= v1.23</td><td>external</td><td>external</td><td>on</td><td>aws</td></tr>
</tbody></table>
<p><strong>KubeadmConfig in the upgraded cluster for scenario 1:</strong></p>
<pre><code>clusterConfiguration:
  apiServer:
    extraArgs:
      cloud-provider: aws
  controllerManager:
    extraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=false
initConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=false
    name: '{{ ds.meta_data.local_hostname }}'
joinConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=false
</code></pre>
<p><strong>KubeadmConfig in the upgraded cluster for scenario 2:</strong></p>
<p>When <code>CSIMigrationAWS=true</code>, installed external CSI driver will be used while relying on in-tree CCM.</p>
<pre><code>clusterConfiguration:
  apiServer:
    extraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=true   // Set only if Kubernetes version &lt; 1.23.x, otherwise this flag is enabled by default.
  controllerManager:
    extraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=true   // Set only if Kubernetes version &lt; 1.23.x, otherwise this flag is enabled by default.
initConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=true   // Set only if Kubernetes version &lt; 1.23.x, otherwise this flag is enabled by default.
joinConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: aws
      feature-gates: CSIMigrationAWS=true   // Set only if Kubernetes version &lt; 1.23.x, otherwise this flag is enabled by default.
</code></pre>
<p><strong>KubeadmConfig in the upgraded cluster for scenario 3:</strong></p>
<p><code>external-cloud-volume-plugin</code> flag needs to be set for old Kubelets to keep talking to in-tree CCM and upgrade fails without this is set.</p>
<pre><code>clusterConfiguration:
  apiServer:
    extraArgs:
      cloud-provider: external
  controllerManager:
    extraArgs:
      cloud-provider: external
      external-cloud-volume-plugin: aws
initConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: external
joinConfiguration:
  nodeRegistration:
    kubeletExtraArgs:
      cloud-provider: external
</code></pre>
<h1><a class="header" href="#restricting-cluster-api-to-certain-namespaces----omit-in-toc---" id="restricting-cluster-api-to-certain-namespaces----omit-in-toc---">Restricting Cluster API to certain namespaces <!-- omit in toc --></a></h1>
<p>Cluster-api-provider-aws controllers by default, reconcile cluster-api objects
across all namespaces in the cluster. However, it is possible to restrict
reconciliation to a single namespace and this document tells you how.</p>
<h2><a class="header" href="#contents----omit-in-toc---" id="contents----omit-in-toc---">Contents <!-- omit in toc --></a></h2>
<ul>
<li><a href="topics/restricting-cluster-api-to-certain-namespaces.html#use-cases">Use cases</a></li>
<li><a href="topics/restricting-cluster-api-to-certain-namespaces.html#configuring-cluster-api-provider-aws-controllers">Configuring <code>cluster-api-provider-aws</code> controllers</a></li>
</ul>
<h2><a class="header" href="#use-cases" id="use-cases">Use cases</a></h2>
<ul>
<li>Grouping clusters into a namespace based on the AWS account will allow
managing clusters across multiple AWS accounts. This will require each
<code>cluster-api-provider-aws</code> controller to have credentials to their respective
AWS accounts. These credentials can be created as kubernetes secret and be
mounted in the pod at <code>/home/.aws</code> or as environment variables.</li>
<li>Grouping clusters into a namespace based on their environment, (test,
qualification, canary, production) will allow a phased rolling out of
<code>cluster-api-provider-aws</code> releases.</li>
<li>Grouping clusters into a namespace based on the infrastructure provider will
allow running multiple cluster-api provider implementations side-by-side and
manage clusters across infrastructure providers.</li>
</ul>
<h2><a class="header" href="#configuring-cluster-api-provider-aws-controllers" id="configuring-cluster-api-provider-aws-controllers">Configuring <code>cluster-api-provider-aws</code> controllers</a></h2>
<ul>
<li>Create the namespace that <code>cluster-api-provider-aws</code> controller will watch for
cluster-api objects</li>
</ul>
<pre><code class="language-(bash)">cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: my-pet-clusters #edit if necessary
EOF
</code></pre>
<ul>
<li>Deploy/edit <code>aws-provider-controller-manager</code> controller statefulset</li>
</ul>
<p>Specifically, edit the container spec for <code>cluster-api-aws-controller</code>, in the
<code>aws-provider-controller-manager</code> statefulset, to pass a value to the <code>namespace</code>
CLI flag.</p>
<pre><code class="language-(bash)">        - -namespace=my-pet-clusters # edit this if necessary
</code></pre>
<p>Once the <code>aws-provider-controller-manager-0</code> pod restarts,
<code>cluster-api-provider-aws</code> controllers will only reconcile the cluster-api
objects in the <code>my-pet-clusters</code> namespace.</p>
<h1><a class="header" href="#using-iam-roles-in-management-cluster-instead-of-aws-credentials" id="using-iam-roles-in-management-cluster-instead-of-aws-credentials">Using IAM roles in management cluster instead of AWS credentials</a></h1>
<h2><a class="header" href="#overview-6" id="overview-6">Overview</a></h2>
<p>Sometimes users might want to use IAM roles to deploy management clusters. If the user already has a management cluster which was created using the AWS credentials, CAPA provides a way to use IAM roles instead of using these credentials.</p>
<h2><a class="header" href="#pre-requisites" id="pre-requisites">Pre-requisites</a></h2>
<p>User has a bootstrap cluster created with AWS credentials. These credentials can be temporary as well.
To create temporary credentials, please follow <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html">this doc</a>.</p>
<p>We can verify whether this bootstrap cluster is using AWS credentials by checking the <code>capa-manager-bootstrap-credentials</code> secret created in <code>capa-system</code> namespace:</p>
<pre><code class="language-bash">kubectl get secret -n capa-system capa-manager-bootstrap-credentials -o=jsonpath='{.data.credentials}' | { base64 -d 2&gt;/dev/null || base64 -D; }
</code></pre>
<p>which will give output similar to below:</p>
<pre><code class="language-bash">[default]
aws_access_key_id = &lt;your-access-key&gt;
aws_secret_access_key = &lt;your-secret-access-key&gt;
region = us-east-1

aws_session_token = &lt;session-token&gt;
</code></pre>
<h2><a class="header" href="#goal" id="goal">Goal</a></h2>
<p>Create a management cluster which uses instance profiles (IAM roles) attached to EC2 instance.</p>
<h2><a class="header" href="#steps-for-capa-managed-clusters" id="steps-for-capa-managed-clusters">Steps for CAPA-managed clusters</a></h2>
<ol>
<li>Create a workload cluster on existing bootstrap cluster. Refer <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">quick start guide</a> for more details.
Since only control-plane nodes have the required IAM roles attached, CAPA deployment should have the necessary tolerations for master (control-plane) node and node selector for master.</li>
</ol>
<blockquote>
<p><strong>Note:</strong> A cluster with a single control plane node won’t be sufficient here due to the <code>NoSchedule</code> taint.</p>
</blockquote>
<ol start="2">
<li>
<p>Get the kubeconfig for the new target management cluster(created in previous step) once it is up and running.</p>
</li>
<li>
<p>Zero the credentials CAPA controller started with, such that target management cluster uses empty credentials and not the previous credentials used to create bootstrap cluster using:</p>
<pre><code class="language-bash">clusterawsadm controller zero-credentials --namespace=capa-system
</code></pre>
<p>For more details, please refer <a href="https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm_controller_zero-credentials.html">zero-credentials doc</a>.</p>
</li>
<li>
<p>Rollout and restart on capa-controller-manager deployment using:</p>
<pre><code class="language-bash">clusterawsadm controller rollout-controller --kubeconfig=kubeconfig --namespace=capa-system
</code></pre>
<p>For more details, please refer <a href="https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm_controller_rollout-controller.html">rollout-controller doc</a>.</p>
</li>
<li>
<p>Use <code>clusterctl init</code> with the new cluster’s kubeconfig to install the provider components. For more details on preparing for init, please refer <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/init.html">clusterctl init doc</a>.</p>
</li>
<li>
<p>Use <code>clusterctl move</code> to move the Cluster API resources from the bootstrap cluster to the target management cluster. For more details on preparing for move, please refer <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">clusterctl move doc</a>.</p>
</li>
<li>
<p>Once the resources are moved to target management cluster successfully, <code>capa-manager-bootstrap-credentials</code> will be created as nil, and hence CAPA controllers will fall back to use the attached instance profiles.</p>
</li>
<li>
<p>Delete the bootstrap cluster with the AWS credentials.</p>
</li>
</ol>
<h1><a class="header" href="#failure-domains" id="failure-domains">Failure Domains</a></h1>
<p>A failure domain in the AWS provider corresponds to an availability zone within an AWS region. </p>
<p>In AWS, Availability Zones are distinct locations within an AWS Region that are engineered to be isolated from failures in other Availability Zones. They provide inexpensive, low-latency network connectivity to other Availability Zones in the same AWS Region, to ensure a cluster (or any application) is resilient to failure. </p>
<p>If a zone goes down, your cluster will continue to run as the other 2 zones are physically separated and can continue to run.</p>
<p>More details of availability zones and regions can be found in the <a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/">AWS docs</a>.</p>
<p>The usage of failure domains for control-plane and worker nodes can be found below in detail:</p>
<ul>
<li><a href="topics/failure-domains/control-planes.html">Control Plane</a></li>
<li><a href="topics/failure-domains/worker-nodes.html">Worker nodes</a></li>
</ul>
<h1><a class="header" href="#failure-domains-in-control-plane-nodes" id="failure-domains-in-control-plane-nodes">Failure domains in control-plane nodes</a></h1>
<p>By default, the control plane of a workload cluster created by CAPA will span multiple availability zones (AZs) (also referred to as “failure domains”) when using multiple control plane nodes. This is because CAPA will, by default, create public and private subnets in all the AZs of a region (up to a maximum of 3 AZs by default). If a region has more than 3 AZs then CAPA will pick 3 AZs to use.</p>
<h2><a class="header" href="#configuring-capa-to-use-specific-azs" id="configuring-capa-to-use-specific-azs">Configuring CAPA to Use Specific AZs</a></h2>
<p>The Cluster API controller will look for the <strong>FailureDomain</strong> status field and will set the <strong>FailureDomain</strong> field in a <code>Machine</code> if a value hasn’t already been explicitly set. It will try to ensure that the machines are spread across all the failure domains.</p>
<p>The <code>AWSMachine</code> controller looks for a failure domain (i.e. Availability Zone) first in the <code>Machine</code> before checking in the <code>network</code> specification of <code>AWSMachine</code>. This failure domain is then used when provisioning the <code>AWSMachine</code>.</p>
<h3><a class="header" href="#using-failuredomain-in-machinemachinedeployment-spec" id="using-failuredomain-in-machinemachinedeployment-spec">Using FailureDomain in Machine/MachineDeployment spec</a></h3>
<p>To control the placement of <code>AWSMachine</code> into a failure domain (i.e. Availability Zones), we can explicitly state the failure domain in <code>Machine</code>. The best way is to specify this using the <strong>FailureDomain</strong> field within the <code>Machine</code> (or <code>MachineDeployment</code>) spec.</p>
<p>For example:</p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: Machine
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: my-cluster
    cluster.x-k8s.io/control-plane: &quot;true&quot;
  name: controlplane-0
  namespace: default
spec:
  version: &quot;v1.22.1&quot;
  clusterName: my-cluster
  failureDomain: &quot;1&quot;
  bootstrap:
    configRef:
        apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
        kind: KubeadmConfigTemplate
        name: my-cluster-md-0
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSMachineTemplate
    name: my-cluster-md-0
</code></pre>
<blockquote>
<p><strong>IMPORTANT WARNING:</strong> All the replicas within a <code>MachineDeployment</code> will reside in the same Availability Zone.</p>
</blockquote>
<h3><a class="header" href="#using-failuredomain-in-network-object-of-awsmachine" id="using-failuredomain-in-network-object-of-awsmachine">Using FailureDomain in network object of AWSMachine</a></h3>
<p>Another way to explicitly instruct CAPA to create resources in specific AZs (and not by random), users can add a <code>network</code> object to the AWSCluster specification. Here is an example <code>network</code> that creates resources across three AZs in the “us-west-2” region:</p>
<pre><code class="language-yaml">spec:
  network:
    vpc:
      cidrBlock: 10.50.0.0/16
    subnets:
    - availabilityZone: us-west-2a
      cidrBlock: 10.50.0.0/20
      isPublic: true
    - availabilityZone: us-west-2a
      cidrBlock: 10.50.16.0/20
    - availabilityZone: us-west-2b
      cidrBlock: 10.50.32.0/20
      isPublic: true
    - availabilityZone: us-west-2b
      cidrBlock: 10.50.48.0/20
    - availabilityZone: us-west-2c
      cidrBlock: 10.50.64.0/20
      isPublic: true
    - availabilityZone: us-west-2c
      cidrBlock: 10.50.80.0/20
</code></pre>
<blockquote>
<p>Note: This method can also be used with worker nodes as well.</p>
</blockquote>
<p>Specifying the CIDR block alone for the VPC is not enough; users must also supply a list of subnets that provides the desired AZ, the CIDR for the subnet, and whether the subnet is public (has a route to an Internet gateway) or is private (does not have a route to an Internet gateway).</p>
<p>Note that CAPA insists that there must be a public subnet (and associated Internet gateway), even if no public load balancer is requested for the control plane. Therefore, for every AZ where a control plane node should be placed, the <code>network</code> object must define both a public and private subnet.</p>
<p>Once CAPA is provided with a <code>network</code> that spans multiple AZs, the KubeadmControlPlane controller will automatically distribute control plane nodes across multiple AZs. No further configuration from the user is required.</p>
<blockquote>
<p>Note: This method can also be used if you do not want to split your EC2 instances across multiple AZs.</p>
</blockquote>
<h2><a class="header" href="#changing-az-defaults" id="changing-az-defaults">Changing AZ defaults</a></h2>
<p>When creating default subnets by default a maximum of 3 AZs will be used. If you are creating a cluster in a region that has more than 3 AZs then 3 AZs will be picked based on alphabetical from that region.</p>
<p>If this default behavior for maximum number of AZs and ordered selection method doesn’t suit your requirements you can use the following to change the behaviour:</p>
<ul>
<li><code>availabilityZoneUsageLimit</code> - specifies the maximum number of availability zones (AZ) that should be used in a region when automatically creating subnets.</li>
<li><code>availabilityZoneSelection</code> - specifies how AZs should be selected if there are more AZs in a region than specified by availabilityZoneUsageLimit. There are 2 selection schemes:
<ul>
<li><code>Ordered</code> - selects based on alphabetical order</li>
<li><code>Random</code> - selects AZs randomly in a region</li>
</ul>
</li>
</ul>
<p>For example if you wanted have a maximum of 2 AZs using a random selection scheme:</p>
<pre><code class="language-yaml">spec:
  network:
    vpc:
      availabilityZoneUsageLimit: 2
      availabilityZoneSelection: Random
</code></pre>
<h2><a class="header" href="#caveats-1" id="caveats-1">Caveats</a></h2>
<p>Deploying control plane nodes across multiple AZs is not a panacea to cure all availability concerns. The sizing and overall utilization of the cluster will greatly affect the behavior of the cluster and the workloads hosted there in the event of an AZ failure. Careful planning is needed to maximize the availability of the cluster even in the face of an AZ failure. There are also other considerations, like cross-AZ traffic charges, that should be taken into account.</p>
<h1><a class="header" href="#failure-domains-in-worker-nodes" id="failure-domains-in-worker-nodes">Failure domains in worker nodes</a></h1>
<p>To ensure that the worker machines are spread across failure domains, we need to create N <code>MachineDeployment</code> for your N failure domains, scaling them independently. Resiliency to failures comes from having multiple <code>MachineDeployment</code>.
For example:</p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
      failureDomain: &quot;1&quot;
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-1
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-1
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachineTemplate
        name: ${CLUSTER_NAME}-md-1
      version: ${KUBERNETES_VERSION}
      failureDomain: &quot;2&quot;
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-2
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-2
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachineTemplate
        name: ${CLUSTER_NAME}-md-2
      version: ${KUBERNETES_VERSION}
      failureDomain: &quot;3&quot;
</code></pre>
<blockquote>
<p><strong>IMPORTANT WARNING:</strong> All the replicas within a <code>MachineDeployment</code> will reside in the same Availability Zone.</p>
</blockquote>
<h3><a class="header" href="#using-awsmachinepool" id="using-awsmachinepool">Using AWSMachinePool</a></h3>
<p>You can use an <code>AWSMachinePool</code> object which automatically distributes worker machines across the configured availability zones.
Set the <strong>FailureDomains</strong> field to the list of availability zones that you want to use. Be aware that not all regions have the same availability zones.</p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: my-cluster
  name: ${CLUSTER_NAME}-mp-0
  namespace: default
spec:
  clusterName: my-cluster
  failureDomains:
    - &quot;1&quot;
    - &quot;3&quot;
  replicas: 3
  template:
    spec:
      clusterName: my-cluster
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-mp-0
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachinePool
        name: ${CLUSTER_NAME}-mp-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachinePool
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: my-cluster
  name: ${CLUSTER_NAME}-mp-0
  namespace: default
spec:
  minSize: 1
  maxSize: 4
  awsLaunchTemplate:
    instanceType: ${AWS_NODE_MACHINE_TYPE}
    iamInstanceProfile: &quot;nodes.cluster-api-provider-aws.sigs.k8s.io&quot;
    sshKeyName: ${AWS_SSH_KEY_NAME}
</code></pre>
<h1><a class="header" href="#userdata-privacy" id="userdata-privacy">Userdata Privacy</a></h1>
<p>Cluster API Provider AWS bootstraps EC2 instances to create and join Kubernetes clusters using instance user data.
Because Kubernetes clusters are secured using TLS using multiple Certificate Authorities, these are generated by
Cluster API and injected into the user data. It is important to note that without the configuring of host firewalls, processes can
retrieve instance userdata from http://169.254.169.254/latest/api/token</p>
<h2><a class="header" href="#requirements-1" id="requirements-1">Requirements</a></h2>
<ul>
<li>An AMI that includes the AWS CLI</li>
<li>AMIs using CloudInit</li>
<li>A working <code>/bin/bash</code> shell</li>
<li>LFS directory layout (i.e. <code>/etc</code> exists and is readable by CloudInit)</li>
</ul>
<p><a href="topics/./images/built-amis.html">Listed AMIs</a> on 1.16 and up should include the AWS CLI.</p>
<h2><a class="header" href="#how-cluster-api-secures-tls-secrets" id="how-cluster-api-secures-tls-secrets">How Cluster API secures TLS secrets</a></h2>
<p>Since v0.5.x, Cluster API Provider AWS has used <a href="https://aws.amazon.com/secrets-manager/">AWS Secrets Manager</a>
as a limited-time secret store, storing the userdata using KMS encryption at rest in AWS.
The EC2 IMDS userdata will contain a boot script to download the encrypted userdata secret
using instance profile permissions, then immediately delete it from AWS Secrets Manager, and then execute it.</p>
<p>To avoid guessing keys in the AWS Secrets Manager key-value store and to prevent collisions, the key is an encoding the
Kubernetes namespace, cluster name and instance name, with a random string appended, providing ~256-bits of entropy.</p>
<p>Cluster API Provider AWS also stores the secret ARN in the AWSMachine spec, and will delete the secret if it isn’t already deleted and
the machine has registered successfully against the workload cluster API server as a node.
Cluster API Provider AWS will also attempt deletion of the secret if the AWSMachine is otherwise deleted or the EC2 instance
is terminated or failed.</p>
<p>This method is only compatible with operating systems and distributions using
<a href="https://cloudinit.readthedocs.io/en/latest/topics/format.html#mime-multi-part-archive">cloud-init</a>. If you are using a different bootstrap
process, you will need to co-ordinate this externally and set the following in the specification of the AWSMachine types to disable the use
of a cloud-init boothook:</p>
<pre><code class="language-yaml">cloudInit:
  insecureSkipSecretsManager: true
</code></pre>
<h2><a class="header" href="#troubleshooting-2" id="troubleshooting-2">Troubleshooting</a></h2>
<h3><a class="header" href="#script-errors" id="script-errors">Script errors</a></h3>
<p>cloud-init does not print boothook script errors to the systemd journal. Logs for the script, if it errored can be found in
<code>/var/log/cloud-init-output.log</code></p>
<h3><a class="header" href="#warning-messages" id="warning-messages">Warning messages</a></h3>
<p>Because cloud-init will attempt to read the final file at start, cloud-init will always print a <code>/etc/secret-userdata.txt cannot be found</code>
message. This can be safely ignored.</p>
<h3><a class="header" href="#secrets-manager-console" id="secrets-manager-console">Secrets manager console</a></h3>
<p>The AWS secrets manager console should show secrets being created and deleted, with a lifetime of around a minute. No plaintext secret
data will appear in the console as Cluster API Provider AWS stores the userdata as fragments of a gzipped data stream.</p>
<h1><a class="header" href="#troubleshooting-3" id="troubleshooting-3">Troubleshooting</a></h1>
<h2><a class="header" href="#resources-arent-being-created" id="resources-arent-being-created">Resources aren’t being created</a></h2>
<p>TODO</p>
<h2><a class="header" href="#target-clusters-control-plane-machine-is-up-but-target-clusters-apiserver-not-working-as-expected" id="target-clusters-control-plane-machine-is-up-but-target-clusters-apiserver-not-working-as-expected">Target cluster’s control plane machine is up but target cluster’s apiserver not working as expected</a></h2>
<p>If <code>aws-provider-controller-manager-0</code> logs did not help, you might want to look into cloud-init logs, <code>/var/log/cloud-init-output.log</code>, on the controller host.
Verifying kubelet status and logs may also provide hints:</p>
<pre><code class="language-bash">journalctl -u kubelet.service
systemctl status kubelet
</code></pre>
<p>For reaching controller host from your local machine:</p>
<pre><code class="language-bash"> ssh -i &lt;private-key&gt; -o &quot;ProxyCommand ssh -W %h:%p -i &lt;private-key&gt; ubuntu@&lt;bastion-IP&gt;&quot; ubuntu@&lt;controller-host-IP&gt;
</code></pre>
<p><code>private-key</code> is the private key from the key-pair discussed in the <code>ssh key pair</code> section above.</p>
<h2><a class="header" href="#kubelet-on-the-control-plane-host-failing-with-error-nocredentialproviders" id="kubelet-on-the-control-plane-host-failing-with-error-nocredentialproviders">kubelet on the control plane host failing with error: NoCredentialProviders</a></h2>
<pre><code class="language-bash">failed to run Kubelet: could not init cloud provider &quot;aws&quot;: error finding instance i-0c276f2a1f1c617b2: &quot;error listing AWS instances: \&quot;NoCredentialProviders: no valid providers in chain. Deprecated.\\n\\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors\&quot;&quot;
</code></pre>
<p>This error can occur if <code>CloudFormation</code> stack is not created properly and IAM instance profile is missing appropriate roles. Run following command to inspect IAM instance profile:</p>
<pre><code class="language-bash">$ aws iam get-instance-profile --instance-profile-name control-plane.cluster-api-provider-aws.sigs.k8s.io --output json
{
    &quot;InstanceProfile&quot;: {
        &quot;InstanceProfileId&quot;: &quot;AIPAJQABLZS4A3QDU576Q&quot;,
        &quot;Roles&quot;: [
            {
                &quot;AssumeRolePolicyDocument&quot;: {
                    &quot;Version&quot;: &quot;2012-10-17&quot;,
                    &quot;Statement&quot;: [
                        {
                            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
                            &quot;Effect&quot;: &quot;Allow&quot;,
                            &quot;Principal&quot;: {
                                &quot;Service&quot;: &quot;ec2.amazonaws.com&quot;
                            }
                        }
                    ]
                },
                &quot;RoleId&quot;: &quot;AROAJQABLZS4A3QDU576Q&quot;,
                &quot;CreateDate&quot;: &quot;2019-05-13T16:45:12Z&quot;,
                &quot;RoleName&quot;: &quot;control-plane.cluster-api-provider-aws.sigs.k8s.io&quot;,
                &quot;Path&quot;: &quot;/&quot;,
                &quot;Arn&quot;: &quot;arn:aws:iam::123456789012:role/control-plane.cluster-api-provider-aws.sigs.k8s.io&quot;
            }
        ],
        &quot;CreateDate&quot;: &quot;2019-05-13T16:45:28Z&quot;,
        &quot;InstanceProfileName&quot;: &quot;control-plane.cluster-api-provider-aws.sigs.k8s.io&quot;,
        &quot;Path&quot;: &quot;/&quot;,
        &quot;Arn&quot;: &quot;arn:aws:iam::123456789012:instance-profile/control-plane.cluster-api-provider-aws.sigs.k8s.io&quot;
    }
}

</code></pre>
<p>If instance profile does not look as expected, you may try recreating the CloudFormation stack using <code>clusterawsadm</code> as explained in the above sections.</p>
<h2><a class="header" href="#recover-a-management-cluster-after-losing-the-api-server-load-balancer" id="recover-a-management-cluster-after-losing-the-api-server-load-balancer">Recover a management cluster after losing the api server load balancer</a></h2>
<p>These steps outline the process for recovering a management cluster after losing the load balancer for the api server. These steps are needed because AWS load balancers have dynamically generated DNS names. This means that when a load balancer is deleted CAPA will recreate the load balancer but it will have a different DNS name that does not match the original, so we need to update some resources as well as the certs to match the new name to make the cluster healthy again. There are a few different scenarios which this could happen.</p>
<ul>
<li>The load balancer gets deleted by some external process or user.</li>
<li>If a cluster is created with the same name as the management cluster in a different namespace and then deleted it will delete the existing load balancer. This is due to ownership of AWS resources being managed by tags. See this <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/969#issuecomment-519121056">issue</a> for reference.</li>
</ul>
<h3><a class="header" href="#access-the-api-server-locally" id="access-the-api-server-locally"><strong>Access the api server locally</strong></a></h3>
<ol>
<li>
<p>ssh to a control plane node and modify the <code>/etc/kubernetes/admin.conf</code></p>
<ul>
<li>
<p>Replace the <code>server</code> with <code>server: https://localhost:6443</code></p>
</li>
<li>
<p>Add <code>insecure-skip-tls-verify: true</code></p>
</li>
<li>
<p>Comment out <code>certificate-authority-data:</code></p>
</li>
</ul>
</li>
<li>
<p>Export the kubeconfig and ensure you can connect</p>
<pre><code class="language-bash">export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl get nodes
</code></pre>
</li>
</ol>
<h3><a class="header" href="#get-rid-of-the-lingering-duplicate-cluster" id="get-rid-of-the-lingering-duplicate-cluster"><strong>Get rid of the lingering duplicate cluster</strong></a></h3>
<p><strong>This step is only needed in the scenario that duplicate cluster was created and deleted which caused the API server load balancer to be deleted.</strong></p>
<ol>
<li>
<p>since there is a duplicate cluster that is trying to be deleted and can’t due to some resources being unable to cleanup since they are in use we need to stop the conflicting reconciliation process. Edit the duplicate aws cluster object and remove the <code>finalizers</code></p>
<pre><code class="language-bash">kubectl edit awscluster &lt;clustername&gt;
</code></pre>
</li>
<li>
<p>next run <code>kubectl describe awscluster &lt;clustername&gt;</code> to validate that the finalizers have been removed</p>
</li>
<li>
<p><code>kubectl get clusters</code> to verify the cluster is gone</p>
</li>
</ol>
<h3><a class="header" href="#make-at-least-one-node-ready" id="make-at-least-one-node-ready"><strong>Make at least one node <code>Ready</code></strong></a></h3>
<ol>
<li>
<p>Right now all endpoints are down due to nodes not being ready. this is problematic for coredns adn cni pods in particular. let’s get one control plane node back healthy. on the control plane node we logged into edit the <code>/etc/kubernetes/kubelet.conf</code></p>
<ul>
<li>
<p>Replace the <code>server</code> with <code>server: https://localhost:6443</code></p>
</li>
<li>
<p>Add <code>insecure-skip-tls-verify: true</code></p>
</li>
<li>
<p>Comment out <code>certificate-authority-data:</code></p>
</li>
<li>
<p>Restart the kubelet <code>systemctl restart kubelet</code></p>
</li>
</ul>
</li>
<li>
<p><code>kubectl get nodes</code> and validate that the node is in a  ready state.</p>
</li>
<li>
<p>After a few minutes most things should start scheduling themselves on the new node. The pods that did not restart on their own that were causing issues were core-dns,kube-proxy, and cni pods.Those should be restart manually.</p>
</li>
<li>
<p>(optional) tail the capa logs to see the load balancer start to reconcile</p>
<pre><code class="language-bash">kubectl logs -f -n capa-system deployments.apps/capa-controller-manager`
</code></pre>
</li>
</ol>
<h3><a class="header" href="#update-the-control-plane-nodes-with-new-lb-settings" id="update-the-control-plane-nodes-with-new-lb-settings"><strong>Update the control plane nodes with new LB settings</strong></a></h3>
<ol>
<li>
<p>To be safe we will do this on all CP nodes rather than having them recreate to avoid potential data loss issues. Follow the following steps for <strong>each</strong> CP node.</p>
</li>
<li>
<p>Regenrate the certs for the api server using the new name. Make sure to update your service cidr and endpoint in the below command.</p>
<pre><code class="language-bash">rm /etc/kubernetes/pki/apiserver.crt
rm /etc/kubernetes/pki/apiserver.key

kubeadm init phase certs apiserver --control-plane-endpoint=&quot;mynewendpoint.com&quot; --service-cidr=100.64.0.0/13 -v10
</code></pre>
</li>
<li>
<p>Update settings in <code>/etc/kubernetes/admin.conf</code></p>
<ul>
<li>
<p>Replace the <code>server</code> with <code>server: https://&lt;your-new-lb.com&gt;:6443</code></p>
</li>
<li>
<p>Remove <code>insecure-skip-tls-verify: true</code></p>
</li>
<li>
<p>Uncomment <code>certificate-authority-data:</code></p>
</li>
<li>
<p>Export the kubeconfig and ensure you can connect </p>
<pre><code class="language-bash">export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl get nodes
</code></pre>
</li>
</ul>
</li>
<li>
<p>Update the settings in <code>/etc/kubernetes/kubelet.conf</code></p>
<ul>
<li>
<p>Replace the <code>server</code> with <code>server: https://your-new-lb.com:6443</code></p>
</li>
<li>
<p>Remove <code>insecure-skip-tls-verify: true</code></p>
</li>
<li>
<p>Uncomment <code>certificate-authority-data:</code></p>
</li>
<li>
<p>restart the kubelet <code>systemctl restart kubelet</code></p>
</li>
</ul>
</li>
<li>
<p>Just as we did before we need new pods to pick up api server cache changes so  you will want to force restart pods like cni pods, kube-proxy, core-dns , etc.</p>
</li>
</ol>
<h3><a class="header" href="#update-capi-settings-for-new-lb-dns-name" id="update-capi-settings-for-new-lb-dns-name">Update capi settings for new LB DNS name</a></h3>
<ol>
<li>
<p>Update the control plane endpoint on the <code>awscluster</code> and <code>cluster</code> objects. To do this we need to disable the validatingwebhooks. We will back them up and then delete so we can apply later.</p>
<pre><code class="language-bash">kubectl get validatingwebhookconfigurations capa-validating-webhook-configuration -o yaml &gt; capa-webhook &amp;&amp; kubectl delete validatingwebhookconfigurations capa-validating-webhook-configuration

kubectl get validatingwebhookconfigurations capi-validating-webhook-configuration -o yaml &gt; capi-webhook &amp;&amp; kubectl delete validatingwebhookconfigurations capi-validating-webhook-configuration
</code></pre>
</li>
<li>
<p>Edit the <code>spec.controlPlaneEndpoint.host</code> field on both <code>awscluster</code> and <code>cluster</code> to have the new endpoint</p>
</li>
<li>
<p>Re-apply your webhooks</p>
<pre><code class="language-bash">kubectl apply -f capi-webhook
kubectl apply -f capa-webhook
</code></pre>
</li>
<li>
<p>Update the following config maps and replace the old control plane name with the new one.</p>
<pre><code class="language-bash">kubectl edit cm -n kube-system kubeadm-config
kubectl edit cm -n kube-system kube-proxy
kubectl edit cm -n kube-public cluster-info
</code></pre>
</li>
<li>
<p>Edit the cluster kubeconfig secret that capi uses to talk to the management cluster. You will need to decode teh secret, replace the endpoint and re-encode and save.</p>
<pre><code class="language-bash">kubectl edit secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig`
</code></pre>
</li>
<li>
<p>At this point things should start to reconcile on their own, but we can use the commands in the next step to force it. </p>
</li>
</ol>
<h3><a class="header" href="#roll-all-of-the-nodes-to-make-sure-everything-is-fresh" id="roll-all-of-the-nodes-to-make-sure-everything-is-fresh">Roll all of the nodes to make sure everything is fresh</a></h3>
<ol>
<li>
<pre><code class="language-bash">kubectl patch kcp &lt;clusternamekcp&gt; -n namespace --type merge -p &quot;{\&quot;spec\&quot;:{\&quot;rolloutAfter\&quot;:\&quot;`date +'%Y-%m-%dT%TZ'`\&quot;}}&quot;
</code></pre>
</li>
<li>
<pre><code class="language-bash"> kubectl patch machinedeployment CLUSTER_NAME-md-0 -n namespace --type merge -p &quot;{\&quot;spec\&quot;:{\&quot;template\&quot;:{\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{\&quot;date\&quot;:\&quot;`date +'%s'`\&quot;}}}}}&quot;
</code></pre>
</li>
</ol>
<h1><a class="header" href="#iam-permissions" id="iam-permissions">IAM Permissions</a></h1>
<h2><a class="header" href="#required-to-use-clusterawsadm-to-provision-iam-roles-via-cloudformation" id="required-to-use-clusterawsadm-to-provision-iam-roles-via-cloudformation">Required to use clusterawsadm to provision IAM roles via CloudFormation</a></h2>
<p>If using <code>clusterawsadm</code> to automate deployment of IAM roles via CloudFormation,
you must have IAM administrative access as <code>clusterawsadm</code> will provision IAM
roles and policies.</p>
<h2><a class="header" href="#required-by-cluster-api-provider-aws-controllers" id="required-by-cluster-api-provider-aws-controllers">Required by Cluster API Provider AWS controllers</a></h2>
<p>The Cluster API Provider AWS controller requires permissions to use EC2, ELB
Autoscaling and optionally EKS. If provisioning IAM roles using <code>clusterawsadm</code>,
these will be set up as the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code>
IAM Policy, and attached to the <code>controllers.cluster-api-provider-aws.sigs.k8s.io</code>
and <code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code> IAM roles.</p>
<h3><a class="header" href="#ec2-provisioned-kubernetes-clusters" id="ec2-provisioned-kubernetes-clusters">EC2 Provisioned Kubernetes Clusters</a></h3>
<pre><code class="language-json">{
  &quot;Type&quot;: &quot;AWS::IAM::ManagedPolicy&quot;,
  &quot;Properties&quot;: {
    &quot;Description&quot;: &quot;For the Kubernetes Cluster API Provider AWS Controllers&quot;,
    &quot;ManagedPolicyName&quot;: &quot;controllers.cluster-api-provider-aws.sigs.k8s.io&quot;,
    &quot;PolicyDocument&quot;: {
      &quot;Version&quot;: &quot;2012-10-17&quot;,
      &quot;Statement&quot;: [
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;ec2:DescribeIpamPools&quot;,
            &quot;ec2:AllocateIpamPoolCidr&quot;,
            &quot;ec2:AttachNetworkInterface&quot;,
            &quot;ec2:DetachNetworkInterface&quot;,
            &quot;ec2:AllocateAddress&quot;,
            &quot;ec2:AssignIpv6Addresses&quot;,
            &quot;ec2:AssignPrivateIpAddresses&quot;,
            &quot;ec2:UnassignPrivateIpAddresses&quot;,
            &quot;ec2:AssociateRouteTable&quot;,
            &quot;ec2:AssociateVpcCidrBlock&quot;,
            &quot;ec2:AttachInternetGateway&quot;,
            &quot;ec2:AuthorizeSecurityGroupIngress&quot;,
            &quot;ec2:CreateCarrierGateway&quot;,
            &quot;ec2:CreateInternetGateway&quot;,
            &quot;ec2:CreateEgressOnlyInternetGateway&quot;,
            &quot;ec2:CreateNatGateway&quot;,
            &quot;ec2:CreateNetworkInterface&quot;,
            &quot;ec2:CreateRoute&quot;,
            &quot;ec2:CreateRouteTable&quot;,
            &quot;ec2:CreateSecurityGroup&quot;,
            &quot;ec2:CreateSubnet&quot;,
            &quot;ec2:CreateTags&quot;,
            &quot;ec2:CreateVpc&quot;,
            &quot;ec2:CreateVpcEndpoint&quot;,
            &quot;ec2:DisassociateVpcCidrBlock&quot;,
            &quot;ec2:ModifyVpcAttribute&quot;,
            &quot;ec2:ModifyVpcEndpoint&quot;,
            &quot;ec2:DeleteCarrierGateway&quot;,
            &quot;ec2:DeleteInternetGateway&quot;,
            &quot;ec2:DeleteEgressOnlyInternetGateway&quot;,
            &quot;ec2:DeleteNatGateway&quot;,
            &quot;ec2:DeleteRouteTable&quot;,
            &quot;ec2:ReplaceRoute&quot;,
            &quot;ec2:DeleteSecurityGroup&quot;,
            &quot;ec2:DeleteSubnet&quot;,
            &quot;ec2:DeleteTags&quot;,
            &quot;ec2:DeleteVpc&quot;,
            &quot;ec2:DeleteVpcEndpoints&quot;,
            &quot;ec2:DescribeAccountAttributes&quot;,
            &quot;ec2:DescribeAddresses&quot;,
            &quot;ec2:DescribeAvailabilityZones&quot;,
            &quot;ec2:DescribeCarrierGateways&quot;,
            &quot;ec2:DescribeInstances&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeInternetGateways&quot;,
            &quot;ec2:DescribeEgressOnlyInternetGateways&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeImages&quot;,
            &quot;ec2:DescribeNatGateways&quot;,
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeNetworkInterfaceAttribute&quot;,
            &quot;ec2:DescribeRouteTables&quot;,
            &quot;ec2:DescribeSecurityGroups&quot;,
            &quot;ec2:DescribeSubnets&quot;,
            &quot;ec2:DescribeVpcs&quot;,
            &quot;ec2:DescribeDhcpOptions&quot;,
            &quot;ec2:DescribeVpcAttribute&quot;,
            &quot;ec2:DescribeVpcEndpoints&quot;,
            &quot;ec2:DescribeVolumes&quot;,
            &quot;ec2:DescribeTags&quot;,
            &quot;ec2:DetachInternetGateway&quot;,
            &quot;ec2:DisassociateRouteTable&quot;,
            &quot;ec2:DisassociateAddress&quot;,
            &quot;ec2:ModifyInstanceAttribute&quot;,
            &quot;ec2:ModifyNetworkInterfaceAttribute&quot;,
            &quot;ec2:ModifySubnetAttribute&quot;,
            &quot;ec2:ReleaseAddress&quot;,
            &quot;ec2:RevokeSecurityGroupEgress&quot;,
            &quot;ec2:RevokeSecurityGroupIngress&quot;,
            &quot;ec2:RunInstances&quot;,
            &quot;ec2:TerminateInstances&quot;,
            &quot;ec2:GetSecurityGroupsForVpc&quot;,
            &quot;tag:GetResources&quot;,
            &quot;elasticloadbalancing:AddTags&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancer&quot;,
            &quot;elasticloadbalancing:ConfigureHealthCheck&quot;,
            &quot;elasticloadbalancing:DeleteLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeleteTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancers&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:DescribeTargetGroups&quot;,
            &quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&quot;,
            &quot;elasticloadbalancing:SetSecurityGroups&quot;,
            &quot;elasticloadbalancing:DescribeTags&quot;,
            &quot;elasticloadbalancing:ModifyLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&quot;,
            &quot;elasticloadbalancing:RemoveTags&quot;,
            &quot;elasticloadbalancing:SetSubnets&quot;,
            &quot;elasticloadbalancing:ModifyTargetGroupAttributes&quot;,
            &quot;elasticloadbalancing:CreateTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeListeners&quot;,
            &quot;elasticloadbalancing:CreateListener&quot;,
            &quot;elasticloadbalancing:DescribeTargetHealth&quot;,
            &quot;elasticloadbalancing:RegisterTargets&quot;,
            &quot;elasticloadbalancing:DeregisterTargets&quot;,
            &quot;elasticloadbalancing:DeleteListener&quot;,
            &quot;autoscaling:DescribeAutoScalingGroups&quot;,
            &quot;autoscaling:DescribeInstanceRefreshes&quot;,
            &quot;autoscaling:DeleteLifecycleHook&quot;,
            &quot;autoscaling:DescribeLifecycleHooks&quot;,
            &quot;autoscaling:PutLifecycleHook&quot;,
            &quot;ec2:CreateLaunchTemplate&quot;,
            &quot;ec2:CreateLaunchTemplateVersion&quot;,
            &quot;ec2:DescribeLaunchTemplates&quot;,
            &quot;ec2:DescribeLaunchTemplateVersions&quot;,
            &quot;ec2:DeleteLaunchTemplate&quot;,
            &quot;ec2:DeleteLaunchTemplateVersions&quot;,
            &quot;ec2:DescribeKeyPairs&quot;,
            &quot;ec2:ModifyInstanceMetadataOptions&quot;,
            &quot;eks:CreateAccessEntry&quot;,
            &quot;eks:DeleteAccessEntry&quot;,
            &quot;eks:DescribeAccessEntry&quot;,
            &quot;eks:UpdateAccessEntry&quot;,
            &quot;eks:ListAccessEntries&quot;,
            &quot;eks:AssociateAccessPolicy&quot;,
            &quot;eks:DisassociateAccessPolicy&quot;,
            &quot;eks:ListAssociatedAccessPolicies&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;autoscaling:CancelInstanceRefresh&quot;,
            &quot;autoscaling:CreateAutoScalingGroup&quot;,
            &quot;autoscaling:UpdateAutoScalingGroup&quot;,
            &quot;autoscaling:CreateOrUpdateTags&quot;,
            &quot;autoscaling:StartInstanceRefresh&quot;,
            &quot;autoscaling:DeleteAutoScalingGroup&quot;,
            &quot;autoscaling:DeleteTags&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:autoscaling:*:*:autoScalingGroup:*:autoScalingGroupName/*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;autoscaling.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;elasticloadbalancing.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;spot.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:PassRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/*.cluster-api-provider-aws.sigs.k8s.io&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;secretsmanager:CreateSecret&quot;,
            &quot;secretsmanager:DeleteSecret&quot;,
            &quot;secretsmanager:TagResource&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:secretsmanager:*:*:secret:aws.cluster.x-k8s.io/*&quot;
          ]
        }
      ]
    },
    &quot;Roles&quot;: [
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sbGVycyIgfQ==&quot;,
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sUGxhbmUiIH0=&quot;
    ]
  }
}
</code></pre>
<h3><a class="header" href="#with-eks-support-1" id="with-eks-support-1">With EKS Support</a></h3>
<pre><code class="language-json">{
  &quot;Type&quot;: &quot;AWS::IAM::ManagedPolicy&quot;,
  &quot;Properties&quot;: {
    &quot;Description&quot;: &quot;For the Kubernetes Cluster API Provider AWS Controllers&quot;,
    &quot;ManagedPolicyName&quot;: &quot;controllers.cluster-api-provider-aws.sigs.k8s.io&quot;,
    &quot;PolicyDocument&quot;: {
      &quot;Version&quot;: &quot;2012-10-17&quot;,
      &quot;Statement&quot;: [
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;ec2:DescribeIpamPools&quot;,
            &quot;ec2:AllocateIpamPoolCidr&quot;,
            &quot;ec2:AttachNetworkInterface&quot;,
            &quot;ec2:DetachNetworkInterface&quot;,
            &quot;ec2:AllocateAddress&quot;,
            &quot;ec2:AssignIpv6Addresses&quot;,
            &quot;ec2:AssignPrivateIpAddresses&quot;,
            &quot;ec2:UnassignPrivateIpAddresses&quot;,
            &quot;ec2:AssociateRouteTable&quot;,
            &quot;ec2:AssociateVpcCidrBlock&quot;,
            &quot;ec2:AttachInternetGateway&quot;,
            &quot;ec2:AuthorizeSecurityGroupIngress&quot;,
            &quot;ec2:CreateCarrierGateway&quot;,
            &quot;ec2:CreateInternetGateway&quot;,
            &quot;ec2:CreateEgressOnlyInternetGateway&quot;,
            &quot;ec2:CreateNatGateway&quot;,
            &quot;ec2:CreateNetworkInterface&quot;,
            &quot;ec2:CreateRoute&quot;,
            &quot;ec2:CreateRouteTable&quot;,
            &quot;ec2:CreateSecurityGroup&quot;,
            &quot;ec2:CreateSubnet&quot;,
            &quot;ec2:CreateTags&quot;,
            &quot;ec2:CreateVpc&quot;,
            &quot;ec2:CreateVpcEndpoint&quot;,
            &quot;ec2:DisassociateVpcCidrBlock&quot;,
            &quot;ec2:ModifyVpcAttribute&quot;,
            &quot;ec2:ModifyVpcEndpoint&quot;,
            &quot;ec2:DeleteCarrierGateway&quot;,
            &quot;ec2:DeleteInternetGateway&quot;,
            &quot;ec2:DeleteEgressOnlyInternetGateway&quot;,
            &quot;ec2:DeleteNatGateway&quot;,
            &quot;ec2:DeleteRouteTable&quot;,
            &quot;ec2:ReplaceRoute&quot;,
            &quot;ec2:DeleteSecurityGroup&quot;,
            &quot;ec2:DeleteSubnet&quot;,
            &quot;ec2:DeleteTags&quot;,
            &quot;ec2:DeleteVpc&quot;,
            &quot;ec2:DeleteVpcEndpoints&quot;,
            &quot;ec2:DescribeAccountAttributes&quot;,
            &quot;ec2:DescribeAddresses&quot;,
            &quot;ec2:DescribeAvailabilityZones&quot;,
            &quot;ec2:DescribeCarrierGateways&quot;,
            &quot;ec2:DescribeInstances&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeInternetGateways&quot;,
            &quot;ec2:DescribeEgressOnlyInternetGateways&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeImages&quot;,
            &quot;ec2:DescribeNatGateways&quot;,
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeNetworkInterfaceAttribute&quot;,
            &quot;ec2:DescribeRouteTables&quot;,
            &quot;ec2:DescribeSecurityGroups&quot;,
            &quot;ec2:DescribeSubnets&quot;,
            &quot;ec2:DescribeVpcs&quot;,
            &quot;ec2:DescribeDhcpOptions&quot;,
            &quot;ec2:DescribeVpcAttribute&quot;,
            &quot;ec2:DescribeVpcEndpoints&quot;,
            &quot;ec2:DescribeVolumes&quot;,
            &quot;ec2:DescribeTags&quot;,
            &quot;ec2:DetachInternetGateway&quot;,
            &quot;ec2:DisassociateRouteTable&quot;,
            &quot;ec2:DisassociateAddress&quot;,
            &quot;ec2:ModifyInstanceAttribute&quot;,
            &quot;ec2:ModifyNetworkInterfaceAttribute&quot;,
            &quot;ec2:ModifySubnetAttribute&quot;,
            &quot;ec2:ReleaseAddress&quot;,
            &quot;ec2:RevokeSecurityGroupEgress&quot;,
            &quot;ec2:RevokeSecurityGroupIngress&quot;,
            &quot;ec2:RunInstances&quot;,
            &quot;ec2:TerminateInstances&quot;,
            &quot;ec2:GetSecurityGroupsForVpc&quot;,
            &quot;tag:GetResources&quot;,
            &quot;elasticloadbalancing:AddTags&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancer&quot;,
            &quot;elasticloadbalancing:ConfigureHealthCheck&quot;,
            &quot;elasticloadbalancing:DeleteLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeleteTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancers&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:DescribeTargetGroups&quot;,
            &quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&quot;,
            &quot;elasticloadbalancing:SetSecurityGroups&quot;,
            &quot;elasticloadbalancing:DescribeTags&quot;,
            &quot;elasticloadbalancing:ModifyLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&quot;,
            &quot;elasticloadbalancing:RemoveTags&quot;,
            &quot;elasticloadbalancing:SetSubnets&quot;,
            &quot;elasticloadbalancing:ModifyTargetGroupAttributes&quot;,
            &quot;elasticloadbalancing:CreateTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeListeners&quot;,
            &quot;elasticloadbalancing:CreateListener&quot;,
            &quot;elasticloadbalancing:DescribeTargetHealth&quot;,
            &quot;elasticloadbalancing:RegisterTargets&quot;,
            &quot;elasticloadbalancing:DeregisterTargets&quot;,
            &quot;elasticloadbalancing:DeleteListener&quot;,
            &quot;autoscaling:DescribeAutoScalingGroups&quot;,
            &quot;autoscaling:DescribeInstanceRefreshes&quot;,
            &quot;autoscaling:DeleteLifecycleHook&quot;,
            &quot;autoscaling:DescribeLifecycleHooks&quot;,
            &quot;autoscaling:PutLifecycleHook&quot;,
            &quot;ec2:CreateLaunchTemplate&quot;,
            &quot;ec2:CreateLaunchTemplateVersion&quot;,
            &quot;ec2:DescribeLaunchTemplates&quot;,
            &quot;ec2:DescribeLaunchTemplateVersions&quot;,
            &quot;ec2:DeleteLaunchTemplate&quot;,
            &quot;ec2:DeleteLaunchTemplateVersions&quot;,
            &quot;ec2:DescribeKeyPairs&quot;,
            &quot;ec2:ModifyInstanceMetadataOptions&quot;,
            &quot;eks:CreateAccessEntry&quot;,
            &quot;eks:DeleteAccessEntry&quot;,
            &quot;eks:DescribeAccessEntry&quot;,
            &quot;eks:UpdateAccessEntry&quot;,
            &quot;eks:ListAccessEntries&quot;,
            &quot;eks:AssociateAccessPolicy&quot;,
            &quot;eks:DisassociateAccessPolicy&quot;,
            &quot;eks:ListAssociatedAccessPolicies&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;autoscaling:CancelInstanceRefresh&quot;,
            &quot;autoscaling:CreateAutoScalingGroup&quot;,
            &quot;autoscaling:UpdateAutoScalingGroup&quot;,
            &quot;autoscaling:CreateOrUpdateTags&quot;,
            &quot;autoscaling:StartInstanceRefresh&quot;,
            &quot;autoscaling:DeleteAutoScalingGroup&quot;,
            &quot;autoscaling:DeleteTags&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:autoscaling:*:*:autoScalingGroup:*:autoScalingGroupName/*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;autoscaling.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;elasticloadbalancing.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;spot.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:PassRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/*.cluster-api-provider-aws.sigs.k8s.io&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;secretsmanager:CreateSecret&quot;,
            &quot;secretsmanager:DeleteSecret&quot;,
            &quot;secretsmanager:TagResource&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:secretsmanager:*:*:secret:aws.cluster.x-k8s.io/*&quot;
          ]
        }
      ]
    },
    &quot;Roles&quot;: [
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sbGVycyIgfQ==&quot;,
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sUGxhbmUiIH0=&quot;
    ]
  }
}
</code></pre>
<h3><a class="header" href="#with-s3-support" id="with-s3-support">With S3 Support</a></h3>
<pre><code class="language-json">{
  &quot;Type&quot;: &quot;AWS::IAM::ManagedPolicy&quot;,
  &quot;Properties&quot;: {
    &quot;Description&quot;: &quot;For the Kubernetes Cluster API Provider AWS Controllers&quot;,
    &quot;ManagedPolicyName&quot;: &quot;controllers.cluster-api-provider-aws.sigs.k8s.io&quot;,
    &quot;PolicyDocument&quot;: {
      &quot;Version&quot;: &quot;2012-10-17&quot;,
      &quot;Statement&quot;: [
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;ec2:DescribeIpamPools&quot;,
            &quot;ec2:AllocateIpamPoolCidr&quot;,
            &quot;ec2:AttachNetworkInterface&quot;,
            &quot;ec2:DetachNetworkInterface&quot;,
            &quot;ec2:AllocateAddress&quot;,
            &quot;ec2:AssignIpv6Addresses&quot;,
            &quot;ec2:AssignPrivateIpAddresses&quot;,
            &quot;ec2:UnassignPrivateIpAddresses&quot;,
            &quot;ec2:AssociateRouteTable&quot;,
            &quot;ec2:AssociateVpcCidrBlock&quot;,
            &quot;ec2:AttachInternetGateway&quot;,
            &quot;ec2:AuthorizeSecurityGroupIngress&quot;,
            &quot;ec2:CreateCarrierGateway&quot;,
            &quot;ec2:CreateInternetGateway&quot;,
            &quot;ec2:CreateEgressOnlyInternetGateway&quot;,
            &quot;ec2:CreateNatGateway&quot;,
            &quot;ec2:CreateNetworkInterface&quot;,
            &quot;ec2:CreateRoute&quot;,
            &quot;ec2:CreateRouteTable&quot;,
            &quot;ec2:CreateSecurityGroup&quot;,
            &quot;ec2:CreateSubnet&quot;,
            &quot;ec2:CreateTags&quot;,
            &quot;ec2:CreateVpc&quot;,
            &quot;ec2:CreateVpcEndpoint&quot;,
            &quot;ec2:DisassociateVpcCidrBlock&quot;,
            &quot;ec2:ModifyVpcAttribute&quot;,
            &quot;ec2:ModifyVpcEndpoint&quot;,
            &quot;ec2:DeleteCarrierGateway&quot;,
            &quot;ec2:DeleteInternetGateway&quot;,
            &quot;ec2:DeleteEgressOnlyInternetGateway&quot;,
            &quot;ec2:DeleteNatGateway&quot;,
            &quot;ec2:DeleteRouteTable&quot;,
            &quot;ec2:ReplaceRoute&quot;,
            &quot;ec2:DeleteSecurityGroup&quot;,
            &quot;ec2:DeleteSubnet&quot;,
            &quot;ec2:DeleteTags&quot;,
            &quot;ec2:DeleteVpc&quot;,
            &quot;ec2:DeleteVpcEndpoints&quot;,
            &quot;ec2:DescribeAccountAttributes&quot;,
            &quot;ec2:DescribeAddresses&quot;,
            &quot;ec2:DescribeAvailabilityZones&quot;,
            &quot;ec2:DescribeCarrierGateways&quot;,
            &quot;ec2:DescribeInstances&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeInternetGateways&quot;,
            &quot;ec2:DescribeEgressOnlyInternetGateways&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ec2:DescribeImages&quot;,
            &quot;ec2:DescribeNatGateways&quot;,
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeNetworkInterfaceAttribute&quot;,
            &quot;ec2:DescribeRouteTables&quot;,
            &quot;ec2:DescribeSecurityGroups&quot;,
            &quot;ec2:DescribeSubnets&quot;,
            &quot;ec2:DescribeVpcs&quot;,
            &quot;ec2:DescribeDhcpOptions&quot;,
            &quot;ec2:DescribeVpcAttribute&quot;,
            &quot;ec2:DescribeVpcEndpoints&quot;,
            &quot;ec2:DescribeVolumes&quot;,
            &quot;ec2:DescribeTags&quot;,
            &quot;ec2:DetachInternetGateway&quot;,
            &quot;ec2:DisassociateRouteTable&quot;,
            &quot;ec2:DisassociateAddress&quot;,
            &quot;ec2:ModifyInstanceAttribute&quot;,
            &quot;ec2:ModifyNetworkInterfaceAttribute&quot;,
            &quot;ec2:ModifySubnetAttribute&quot;,
            &quot;ec2:ReleaseAddress&quot;,
            &quot;ec2:RevokeSecurityGroupEgress&quot;,
            &quot;ec2:RevokeSecurityGroupIngress&quot;,
            &quot;ec2:RunInstances&quot;,
            &quot;ec2:TerminateInstances&quot;,
            &quot;ec2:GetSecurityGroupsForVpc&quot;,
            &quot;tag:GetResources&quot;,
            &quot;elasticloadbalancing:AddTags&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancer&quot;,
            &quot;elasticloadbalancing:ConfigureHealthCheck&quot;,
            &quot;elasticloadbalancing:DeleteLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeleteTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancers&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:DescribeTargetGroups&quot;,
            &quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&quot;,
            &quot;elasticloadbalancing:SetSecurityGroups&quot;,
            &quot;elasticloadbalancing:DescribeTags&quot;,
            &quot;elasticloadbalancing:ModifyLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&quot;,
            &quot;elasticloadbalancing:RemoveTags&quot;,
            &quot;elasticloadbalancing:SetSubnets&quot;,
            &quot;elasticloadbalancing:ModifyTargetGroupAttributes&quot;,
            &quot;elasticloadbalancing:CreateTargetGroup&quot;,
            &quot;elasticloadbalancing:DescribeListeners&quot;,
            &quot;elasticloadbalancing:CreateListener&quot;,
            &quot;elasticloadbalancing:DescribeTargetHealth&quot;,
            &quot;elasticloadbalancing:RegisterTargets&quot;,
            &quot;elasticloadbalancing:DeregisterTargets&quot;,
            &quot;elasticloadbalancing:DeleteListener&quot;,
            &quot;autoscaling:DescribeAutoScalingGroups&quot;,
            &quot;autoscaling:DescribeInstanceRefreshes&quot;,
            &quot;autoscaling:DeleteLifecycleHook&quot;,
            &quot;autoscaling:DescribeLifecycleHooks&quot;,
            &quot;autoscaling:PutLifecycleHook&quot;,
            &quot;ec2:CreateLaunchTemplate&quot;,
            &quot;ec2:CreateLaunchTemplateVersion&quot;,
            &quot;ec2:DescribeLaunchTemplates&quot;,
            &quot;ec2:DescribeLaunchTemplateVersions&quot;,
            &quot;ec2:DeleteLaunchTemplate&quot;,
            &quot;ec2:DeleteLaunchTemplateVersions&quot;,
            &quot;ec2:DescribeKeyPairs&quot;,
            &quot;ec2:ModifyInstanceMetadataOptions&quot;,
            &quot;eks:CreateAccessEntry&quot;,
            &quot;eks:DeleteAccessEntry&quot;,
            &quot;eks:DescribeAccessEntry&quot;,
            &quot;eks:UpdateAccessEntry&quot;,
            &quot;eks:ListAccessEntries&quot;,
            &quot;eks:AssociateAccessPolicy&quot;,
            &quot;eks:DisassociateAccessPolicy&quot;,
            &quot;eks:ListAssociatedAccessPolicies&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;autoscaling:CancelInstanceRefresh&quot;,
            &quot;autoscaling:CreateAutoScalingGroup&quot;,
            &quot;autoscaling:UpdateAutoScalingGroup&quot;,
            &quot;autoscaling:CreateOrUpdateTags&quot;,
            &quot;autoscaling:StartInstanceRefresh&quot;,
            &quot;autoscaling:DeleteAutoScalingGroup&quot;,
            &quot;autoscaling:DeleteTags&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:autoscaling:*:*:autoScalingGroup:*:autoScalingGroupName/*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;autoscaling.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;elasticloadbalancing.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:CreateServiceLinkedRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot&quot;
          ],
          &quot;Condition&quot;: {
            &quot;StringLike&quot;: {
              &quot;iam:AWSServiceName&quot;: &quot;spot.amazonaws.com&quot;
            }
          }
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;iam:PassRole&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:iam::*:role/*.cluster-api-provider-aws.sigs.k8s.io&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;secretsmanager:CreateSecret&quot;,
            &quot;secretsmanager:DeleteSecret&quot;,
            &quot;secretsmanager:TagResource&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:secretsmanager:*:*:secret:aws.cluster.x-k8s.io/*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;s3:CreateBucket&quot;,
            &quot;s3:DeleteBucket&quot;,
            &quot;s3:DeleteObject&quot;,
            &quot;s3:GetObject&quot;,
            &quot;s3:ListBucket&quot;,
            &quot;s3:PutBucketPolicy&quot;,
            &quot;s3:PutBucketTagging&quot;,
            &quot;s3:PutLifecycleConfiguration&quot;,
            &quot;s3:PutObject&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:s3:::cluster-api-provider-aws-*&quot;
          ]
        }
      ]
    },
    &quot;Roles&quot;: [
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sbGVycyIgfQ==&quot;,
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sUGxhbmUiIH0=&quot;
    ]
  }
}
</code></pre>
<h2><a class="header" href="#required-by-the-kubernetes-aws-cloud-provider" id="required-by-the-kubernetes-aws-cloud-provider">Required by the Kubernetes AWS Cloud Provider</a></h2>
<p>These permissions are used by the Kubernetes AWS Cloud Provider. If you are
running with the in-tree cloud provider, this will typically be used by the
<code>controller-manager</code> pod in the <code>kube-system</code> namespace.</p>
<p>If provisioning IAM roles using <code>clusterawsadm</code>,
these will be set up as the <code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code>
IAM Policy, and attached to the <code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code>
IAM role.</p>
<pre><code class="language-json">{
  &quot;Type&quot;: &quot;AWS::IAM::ManagedPolicy&quot;,
  &quot;Properties&quot;: {
    &quot;Description&quot;: &quot;For the Kubernetes Cloud Provider AWS Control Plane&quot;,
    &quot;ManagedPolicyName&quot;: &quot;control-plane.cluster-api-provider-aws.sigs.k8s.io&quot;,
    &quot;PolicyDocument&quot;: {
      &quot;Version&quot;: &quot;2012-10-17&quot;,
      &quot;Statement&quot;: [
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;autoscaling:DescribeAutoScalingGroups&quot;,
            &quot;autoscaling:DescribeLaunchConfigurations&quot;,
            &quot;autoscaling:DescribeTags&quot;,
            &quot;ec2:AssignIpv6Addresses&quot;,
            &quot;ec2:DescribeInstances&quot;,
            &quot;ec2:DescribeImages&quot;,
            &quot;ec2:DescribeRegions&quot;,
            &quot;ec2:DescribeRouteTables&quot;,
            &quot;ec2:DescribeSecurityGroups&quot;,
            &quot;ec2:DescribeSubnets&quot;,
            &quot;ec2:DescribeVolumes&quot;,
            &quot;ec2:CreateSecurityGroup&quot;,
            &quot;ec2:CreateTags&quot;,
            &quot;ec2:CreateVolume&quot;,
            &quot;ec2:ModifyInstanceAttribute&quot;,
            &quot;ec2:ModifyVolume&quot;,
            &quot;ec2:AttachVolume&quot;,
            &quot;ec2:AuthorizeSecurityGroupIngress&quot;,
            &quot;ec2:CreateRoute&quot;,
            &quot;ec2:DeleteRoute&quot;,
            &quot;ec2:DeleteSecurityGroup&quot;,
            &quot;ec2:DeleteVolume&quot;,
            &quot;ec2:DetachVolume&quot;,
            &quot;ec2:RevokeSecurityGroupIngress&quot;,
            &quot;ec2:DescribeVpcs&quot;,
            &quot;elasticloadbalancing:AddTags&quot;,
            &quot;elasticloadbalancing:AttachLoadBalancerToSubnets&quot;,
            &quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&quot;,
            &quot;elasticloadbalancing:SetSecurityGroups&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancer&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancerPolicy&quot;,
            &quot;elasticloadbalancing:CreateLoadBalancerListeners&quot;,
            &quot;elasticloadbalancing:ConfigureHealthCheck&quot;,
            &quot;elasticloadbalancing:DeleteLoadBalancer&quot;,
            &quot;elasticloadbalancing:DeleteLoadBalancerListeners&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancers&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:DetachLoadBalancerFromSubnets&quot;,
            &quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&quot;,
            &quot;elasticloadbalancing:ModifyLoadBalancerAttributes&quot;,
            &quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer&quot;,
            &quot;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&quot;,
            &quot;elasticloadbalancing:CreateListener&quot;,
            &quot;elasticloadbalancing:CreateTargetGroup&quot;,
            &quot;elasticloadbalancing:DeleteListener&quot;,
            &quot;elasticloadbalancing:DeleteTargetGroup&quot;,
            &quot;elasticloadbalancing:DeregisterTargets&quot;,
            &quot;elasticloadbalancing:DescribeListeners&quot;,
            &quot;elasticloadbalancing:DescribeLoadBalancerPolicies&quot;,
            &quot;elasticloadbalancing:DescribeTargetGroups&quot;,
            &quot;elasticloadbalancing:DescribeTargetHealth&quot;,
            &quot;elasticloadbalancing:ModifyListener&quot;,
            &quot;elasticloadbalancing:ModifyTargetGroup&quot;,
            &quot;elasticloadbalancing:RegisterTargets&quot;,
            &quot;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&quot;,
            &quot;iam:CreateServiceLinkedRole&quot;,
            &quot;kms:DescribeKey&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        }
      ]
    },
    &quot;Roles&quot;: [
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sUGxhbmUiIH0=&quot;
    ]
  }
}
</code></pre>
<h2><a class="header" href="#required-by-all-nodes" id="required-by-all-nodes">Required by all nodes</a></h2>
<p>All nodes require these permissions in order to run, and are used by the AWS
cloud provider run by kubelet.</p>
<p>If provisioning IAM roles using <code>clusterawsadm</code>,
these will be set up as the <code>nodes.cluster-api-provider-aws.sigs.k8s.io</code>
IAM Policy, and attached to the <code>nodes.cluster-api-provider-aws.sigs.k8s.io</code>
IAM role.</p>
<pre><code class="language-json">{
  &quot;Type&quot;: &quot;AWS::IAM::ManagedPolicy&quot;,
  &quot;Properties&quot;: {
    &quot;Description&quot;: &quot;For the Kubernetes Cloud Provider AWS nodes&quot;,
    &quot;ManagedPolicyName&quot;: &quot;nodes.cluster-api-provider-aws.sigs.k8s.io&quot;,
    &quot;PolicyDocument&quot;: {
      &quot;Version&quot;: &quot;2012-10-17&quot;,
      &quot;Statement&quot;: [
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;ec2:AssignIpv6Addresses&quot;,
            &quot;ec2:DescribeInstances&quot;,
            &quot;ec2:DescribeRegions&quot;,
            &quot;ec2:CreateTags&quot;,
            &quot;ec2:DescribeTags&quot;,
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeInstanceTypes&quot;,
            &quot;ecr:GetAuthorizationToken&quot;,
            &quot;ecr:BatchCheckLayerAvailability&quot;,
            &quot;ecr:GetDownloadUrlForLayer&quot;,
            &quot;ecr:GetRepositoryPolicy&quot;,
            &quot;ecr:DescribeRepositories&quot;,
            &quot;ecr:ListImages&quot;,
            &quot;ecr:BatchGetImage&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;secretsmanager:DeleteSecret&quot;,
            &quot;secretsmanager:GetSecretValue&quot;
          ],
          &quot;Resource&quot;: [
            &quot;arn:*:secretsmanager:*:*:secret:aws.cluster.x-k8s.io/*&quot;
          ]
        },
        {
          &quot;Effect&quot;: &quot;Allow&quot;,
          &quot;Action&quot;: [
            &quot;ssm:UpdateInstanceInformation&quot;,
            &quot;ssmmessages:CreateControlChannel&quot;,
            &quot;ssmmessages:CreateDataChannel&quot;,
            &quot;ssmmessages:OpenControlChannel&quot;,
            &quot;ssmmessages:OpenDataChannel&quot;,
            &quot;s3:GetEncryptionConfiguration&quot;
          ],
          &quot;Resource&quot;: [
            &quot;*&quot;
          ]
        }
      ]
    },
    &quot;Roles&quot;: [
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVDb250cm9sUGxhbmUiIH0=&quot;,
      &quot;eyAiUmVmIjogIkFXU0lBTVJvbGVOb2RlcyIgfQ==&quot;
    ]
  }
}
</code></pre>
<p>When using EKS, the <code>AmazonEKSWorkerNodePolicy</code> and <code>AmazonEKS_CNI_Policy</code>
AWS managed policies will also be attached to
<code>nodes.cluster-api-provider-aws.sigs.k8s.io</code> IAM role.</p>
<h1><a class="header" href="#ignition-support" id="ignition-support">Ignition support</a></h1>
<ul>
<li><strong>Feature status:</strong> Experimental</li>
<li><strong>Feature gate:</strong> BootstrapFormatIgnition=true</li>
</ul>
<p>The default configuration engine for bootstrapping workload cluster machines is <a href="https://cloudinit.readthedocs.io/">cloud-init</a>.
<strong>Ignition</strong> is an alternative engine used by Linux distributions such as <a href="https://www.flatcar.org/docs/latest/provisioning/ignition/">Flatcar Container Linux</a>
and <a href="https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/">Fedora CoreOS</a> and therefore should be used when choosing an Ignition-based distribution as
the underlying OS for workload clusters.</p>
<aside class="note warning">
<h1><a class="header" href="#note-2" id="note-2">Note</a></h1>
<p>This initial implementation used Ignition <strong>v2</strong> and was tested with <strong>Flatcar Container Linux</strong> only.
Further releases added Ignition <strong>v3</strong> support.</p>
</aside>
<p>This document explains how Ignition support works.</p>
<p>For more generic information, see <a href="https://cluster-api.sigs.k8s.io/tasks/experimental-features/ignition.html">Cluster API documentation on Ignition Bootstrap configuration</a>.</p>
<h2><a class="header" href="#overview-7" id="overview-7">Overview</a></h2>
<p>When using CloudInit for bootstrapping, by default the awsmachine controller stores EC2 instance user data using SSM to store it encrypted, which underneath uses multi part mime types.
Unfortunately multi part mime types are <a href="https://github.com/coreos/ignition/issues/1072">not supported</a> by Ignition. Moreover EC2 instance user data storage is also limited to 64 KB, which might not always be enough to provision Kubernetes controlplane because of the size of required certificates and configuration files.</p>
<p>To address these limitations, when using Ignition for bootstrapping, by default the awsmachine controller uses a Cluster Object Store (e.g. S3 Bucket), configured in the AWSCluster, to store user data,
which will be then pulled by the instances during provisioning.</p>
<p>Optionally, when using Ignition for bootstrapping, users can optionally choose an alternative storageType for user data.
For now the single available alternative is to store user data unencrypted directly in the EC2 instance user data.
This storageType option is although discouraged unless strictly necessary, as it is not considered as safe as storing it in the S3 Bucket.</p>
<h2><a class="header" href="#prerequirements-for-enabling-ignition-bootstrapping" id="prerequirements-for-enabling-ignition-bootstrapping">Prerequirements for enabling Ignition bootstrapping</a></h2>
<h3><a class="header" href="#enabling-exp_bootstrap_format_ignition-feature-gate" id="enabling-exp_bootstrap_format_ignition-feature-gate">Enabling EXP_BOOTSTRAP_FORMAT_IGNITION feature gate</a></h3>
<p>In order to activate Ignition bootstrap you first need to enable its feature gate.</p>
<p>When deploying CAPA using <code>clusterctl</code>, make sure you set <code>BOOTSTRAP_FORMAT_IGNITION=true</code> and
<code>EXP_KUBEADM_BOOTSTRAP_FORMAT_IGNITION=true </code>environment variables to enable experimental Ignition bootstrap
support.</p>
<pre><code class="language-sh"># Enable the feature gates controlling Ignition bootstrap.
export EXP_KUBEADM_BOOTSTRAP_FORMAT_IGNITION=true # Used by the kubeadm bootstrap provider.
export EXP_BOOTSTRAP_FORMAT_IGNITION=true # Used by the AWS provider.

# Initialize the management cluster.
clusterctl init --infrastructure aws
</code></pre>
<h2><a class="header" href="#choosing-a-storage-type-for-ignition-user-data" id="choosing-a-storage-type-for-ignition-user-data">Choosing a storage type for Ignition user data</a></h2>
<p>S3 is the default storage type when Ignition is enabled for managing machine’s bootstrapping.
But other methods can be choosen for storing Ignition user data.</p>
<h3><a class="header" href="#store-ignition-config-in-a-cluster-object-store-eg-s3-bucket" id="store-ignition-config-in-a-cluster-object-store-eg-s3-bucket">Store Ignition config in a Cluster Object Store (e.g. S3 bucket)</a></h3>
<p>To explicitly set ClusterObjectStore as the storage type, provide the following config in the <code>AWSMachineTemplate</code>:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: &quot;test&quot;
spec:
  template:
    spec:
      ignition:
        storageType: ClusterObjectStore
</code></pre>
<h4><a class="header" href="#cluster-object-store-and-object-management" id="cluster-object-store-and-object-management">Cluster Object Store and object management</a></h4>
<p>When you want to use Ignition user data format for you machines, you need to configure your cluster to
specify which Cluster Object Store to use. Controller will then check that the bucket already exists and that required policies
are in place.</p>
<p>See the configuration snippet below to learn how to configure <code>AWSCluster</code> to manage S3 bucket.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
spec:
  s3Bucket:
    controlPlaneIAMInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io
    name: cluster-api-provider-aws-unique-suffix
    nodesIAMInstanceProfiles:
    - nodes.cluster-api-provider-aws.sigs.k8s.io
</code></pre>
<p>Buckets are safe to be reused between clusters.</p>
<p>After successful machine provisioning, the bootstrap data is removed from the object store.</p>
<p>During cluster removal, if the Cluster Object Store is empty, it will be deleted as well.</p>
<h4><a class="header" href="#s3-iam-permissions" id="s3-iam-permissions">S3 IAM Permissions</a></h4>
<p>If you choose to use an S3 bucket as the Cluster Object Store, CAPA controllers require additional IAM permissions.</p>
<p>If you use <code>clusterawsadm</code> for managing the IAM roles, you can use the configuration below to create S3-related
IAM permissions.</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  s3Buckets:
    enable: true
</code></pre>
<p>See <a href="topics/./using-clusterawsadm-to-fulfill-prerequisites.html">Using clusterawsadm to fulfill prerequisites</a> for more
details.</p>
<h4><a class="header" href="#cluster-object-store-naming" id="cluster-object-store-naming">Cluster Object Store naming</a></h4>
<p>Cluster Object Store and bucket naming must follow <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html">S3 Bucket naming rules</a>.</p>
<p>In addition, by default <code>clusterawsadm</code> creates IAM roles to only allow interacting with buckets with
<code>cluster-api-provider-aws-</code> prefix to reduce the permissions of CAPA controller, so all bucket names should
use this prefix.</p>
<p>To change it, use <code>spec.s3Buckets.namePrefix</code> field in <code>AWSIAMConfiguration</code>.</p>
<pre><code class="language-yaml">apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  s3Buckets:
    namePrefix: my-custom-secure-bucket-prefix-
</code></pre>
<h3><a class="header" href="#store-ignition-config-as-unencrypteduserdata" id="store-ignition-config-as-unencrypteduserdata">Store Ignition config as UnencryptedUserData</a></h3>
<aside class="note warning">
<h1><a class="header" href="#warning-7" id="warning-7">WARNING</a></h1>
**This is discouraged as is not considered as secure as other storage types.**
</aside>
<p>To instruct the controllers to store the user data directly in the EC2 instance user data unencrypted,
provide the following config in the <code>AWSMachineTemplate</code>:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: &quot;test&quot;
spec:
  template:
    spec:
      ignition:
        storageType: UnencryptedUserData
</code></pre>
<p>No further requirements are necessary.</p>
<h2><a class="header" href="#supported-bootstrap-providers" id="supported-bootstrap-providers">Supported bootstrap providers</a></h2>
<p>At the moment only <a href="https://cluster-api.sigs.k8s.io/tasks/experimental-features/ignition.html">CABPK</a> is known to support producing bootstrap data in Ignition format.</p>
<h2><a class="header" href="#trying-it-out" id="trying-it-out">Trying it out</a></h2>
<p>If you want to test Ignition support, use <code>flatcar</code> cluster flavor.</p>
<h2><a class="header" href="#other-bootstrap-providers" id="other-bootstrap-providers">Other bootstrap providers</a></h2>
<p>If you want to use Ignition support with custom bootstrap provider which supports producing Ignition bootstrap
data, ensure that bootstrap provider sets the <code>format</code> field in machine bootstrap secret to <code>ignition</code>. This
information is used by the machine controller to determine which user data format to use for the instances.</p>
<h1><a class="header" href="#external-resource-garbage-collection" id="external-resource-garbage-collection">External Resource Garbage Collection</a></h1>
<ul>
<li><strong>Feature status:</strong> Stable</li>
<li><strong>Feature gate (required):</strong> ExternalResourceGC=true</li>
</ul>
<h2><a class="header" href="#overview-8" id="overview-8">Overview</a></h2>
<p>Workload clusters that CAPA has created may have additional resources in AWS that need to be deleted when the cluster is deleted.</p>
<p>For example, if the workload cluster has <code>Services</code> of type <code>LoadBalancer</code> then AWS ELB/NLB are provisioned. If you try to delete the workload cluster in this example, it will fail as these load balancers are still using the VPC.</p>
<p>This feature enables deletion of these external resources as part of cluster deletion. During the deletion of a workload cluster the external AWS resources that where created by the Cloud Controller Manager (CCM) in the workload cluster will be identified and deleted.</p>
<blockquote>
<p>NOTE: This is not related to <a href="https://cluster-api-aws.sigs.k8s.io/topics/bring-your-own-aws-infrastructure.html">externally managed infrastructure</a>.</p>
</blockquote>
<p>Currently, we support cleaning up the following:</p>
<ul>
<li>AWS ELB/NLB - by deleting <code>Services</code> of type <code>LoadBalancer</code> from the workload cluster</li>
</ul>
<p>We will look to support deleting EBS volumes in the future potentially.</p>
<blockquote>
<p>Note: this feature will likely be superseded by an upstream CAPI feature in the future when <a href="https://github.com/kubernetes-sigs/cluster-api/issues/3075">this issue</a> is resolved.</p>
</blockquote>
<h2><a class="header" href="#disabling" id="disabling">Disabling</a></h2>
<p>The garbage collection feature is enabled by default. If you want to disable the feature then you must set the <code>ExternalResourceGC</code> feature gate to <code>false</code> on the controller manager. The easiest way to do this is via an environment variable:</p>
<pre><code class="language-bash">export EXTERNAL_RESOURCE_GC=false
clusterctl init --infrastructure aws
</code></pre>
<blockquote>
<p>Note: if you disable this feature <strong>ALL</strong> clusters will be marked as not requiring garbage collection.</p>
</blockquote>
<h2><a class="header" href="#operations" id="operations">Operations</a></h2>
<h3><a class="header" href="#manually-disabling-garbage-collection-for-a-cluster" id="manually-disabling-garbage-collection-for-a-cluster">Manually Disabling Garbage Collection for a Cluster</a></h3>
<p>There are 2 ways to manually disable garbage collection for an individual cluster:</p>
<h4><a class="header" href="#using-clusterawsadm" id="using-clusterawsadm">Using <code>clusterawsadm</code></a></h4>
<p>By running the following command:</p>
<pre><code class="language-bash">clusterawsadm gc disable --cluster-name mycluster
</code></pre>
<p>See the command help for more examples.</p>
<h4><a class="header" href="#editing-awsclusterawsmanagedcontrolplane" id="editing-awsclusterawsmanagedcontrolplane">Editing <code>AWSCluster\AWSManagedControlPlane</code></a></h4>
<p>Or, by editing your <code>AWSCluster</code> or <code>AWSManagedControlPlane</code> so that the annotation <code>aws.cluster.x-k8s.io/external-resource-gc</code> is set to <strong>false</strong>.</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: AWSManagedControlPlane
metadata:
  annotations:
    aws.cluster.x-k8s.io/external-resource-gc: &quot;false&quot;
</code></pre>
<h3><a class="header" href="#manually-enabling-garbage-collection-for-a-cluster" id="manually-enabling-garbage-collection-for-a-cluster">Manually Enabling Garbage Collection for a Cluster</a></h3>
<p>There are 2 ways to manually enable garbage collection for an individual cluster:</p>
<h4><a class="header" href="#using-clusterawsadm-1" id="using-clusterawsadm-1">Using <code>clusterawsadm</code></a></h4>
<p>By running the following command:</p>
<pre><code class="language-bash">clusterawsadm gc enable --cluster-name mycluster
</code></pre>
<p>See the command help for more examples.</p>
<h4><a class="header" href="#editing-awsclusterawsmanagedcontrolplane-1" id="editing-awsclusterawsmanagedcontrolplane-1">Editing <code>AWSCluster\AWSManagedControlPlane</code></a></h4>
<p>Or, by editing your <code>AWSCluster</code> or <code>AWSManagedControlPlane</code> o that the annotation <code>aws.cluster.x-k8s.io/external-resource-gc</code> is either removed or set to <strong>true</strong>.</p>
<pre><code class="language-yaml">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: AWSManagedControlPlane
metadata:
  annotations:
    aws.cluster.x-k8s.io/external-resource-gc: &quot;true&quot;
</code></pre>
<h1><a class="header" href="#instance-metadata-service" id="instance-metadata-service">Instance Metadata Service</a></h1>
<p>Instance metadata is data about your instance that you can use to configure or manage the running instance which you can access from a running instance using one of the following methods:</p>
<ul>
<li>Instance Metadata Service Version 1 (IMDSv1) – a request/response method</li>
<li>Instance Metadata Service Version 2 (IMDSv2) – a session-oriented method</li>
</ul>
<p>CAPA defaults to use IMDSv2 as optional property when creating instances.</p>
<p>CAPA expose options to configure IMDSv2 as required when creating instances, as it provides a <a href="https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/">better level of security</a>.</p>
<p>It is possible to configure the instance metadata options using the field called <code>instanceMetadataOptions</code> in the <code>AWSMachineTemplate</code>.</p>
<p>Example:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: &quot;test&quot;
spec:
  template:
    spec:
      instanceMetadataOptions:
        httpEndpoint: enabled
        httpPutResponseHopLimit: 1
        httpTokens: optional
        instanceMetadataTags: disabled
</code></pre>
<p>To use IMDSv2, simply set <code>httpTokens</code> value to <code>required</code> (in other words, set the use of IMDSv2 to required).
To use IMDSv2, please also set <code>httpPutResponseHopLimit</code> value to <code>2</code>, as it is recommended in container environment according to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html#imds-considerations">AWS document</a>.</p>
<p>Similarly, this can be done with <code>AWSManagedMachinePool</code> for use with EKS Managed Nodegroups. One slight difference here is that you <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-metadata-transition-to-version-2.html">must use Launch Templates to configure IMDSv2 with Autoscaling Groups</a>. In order to configure the LaunchTemplate, you must use a custom AMI type according to the AWS API. This can be done by setting <code>AWSManagedMachinePool.spec.amiType</code> to <code>CUSTOM</code>. This change means that you must also specify a bootstrapping script to the worker node, which allows it to be joined to the EKS cluster. The default AWS Managed Node Group bootstrap script can be found <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh">here on Github</a>.</p>
<p>The following example will use the default Amazon EKS Worker Node AMI which includes the default EKS Bootstrapping script. This must be installed on the management cluster as a Secret, under the key <code>value</code>. The secret’s name must then be included in your <code>MachinePool</code> manifest at <code>MachinePool.spec.template.spec.bootstrap.dataSecretName</code>. Some assumptions are made for this example:</p>
<ul>
<li>Your cluster name is <code>capi-imds</code>, which CAPA renames to <code>default_capi-imds-control-plane</code> automatically</li>
<li>Your cluster is Kubernetes Version <code>v1.25.9</code></li>
<li>Your <code>AWSManagedCluster</code> is deployed in the <code>default</code> namespace along with the bootstrap secret <code>eks-bootstrap</code></li>
</ul>
<pre><code class="language-yaml">kind: Secret
apiVersion: v1
type: Opaque
data:
  value: IyEvYmluL2Jhc2ggLXhlCi9ldGMvZWtzL2Jvb3RzdHJhcC5zaCBkZWZhdWx0X2NhcGktaW1kcy1jb250cm9sLXBsYW5l
metadata:
  name: eks-bootstrap
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedMachinePool
metadata:
  name: &quot;capi-imds-pool-launchtemplate&quot;
spec:
  amiType: CUSTOM
  awsLaunchTemplate:
    name: my-aws-launch-template
    instanceType: t3.nano
    metadataOptions:
      httpTokens: required
      httpPutResponseHopLimit: 2
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: &quot;capi-imds-pool-1&quot;
spec:
  clusterName: &quot;capi-imds&quot;
  replicas: 1
  template:
    spec:
      version: v1.25.9
      clusterName: &quot;capi-imds&quot;
      bootstrap:
        dataSecretName: &quot;eks-bootstrap&quot;
      infrastructureRef:
        name: &quot;capi-imds-pool-launchtemplate&quot;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSManagedMachinePool
</code></pre>
<p><code>IyEvYmluL2Jhc2ggLXhlCi9ldGMvZWtzL2Jvb3RzdHJhcC5zaCBkZWZhdWx0X2NhcGktaW1kcy1jb250cm9sLXBsYW5l</code> in the above secret is a Base64 encoded version of the following script:</p>
<pre><code class="language-bash">#!/bin/bash -xe
/etc/eks/bootstrap.sh default_capi-imds-control-plane
</code></pre>
<p>If your cluster is not named <code>default_capi-imds-control-plane</code> in the AWS EKS console, you must update the name and store it as a Secret again.</p>
<p>See <a href="https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/modify-instance-metadata-options.html">the CLI command reference</a> for more information.</p>
<p>Before you decide to use IMDSv2 for the cluster instances, please make sure all your applications are compatible with IMDSv2.</p>
<p>See the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-metadata-transition-to-version-2.html#recommended-path-for-requiring-imdsv2">transition guide</a> for more information.</p>
<h1><a class="header" href="#setting-up-a-network-load-balancer" id="setting-up-a-network-load-balancer">Setting up a Network Load Balancer</a></h1>
<h2><a class="header" href="#overview-9" id="overview-9">Overview</a></h2>
<p>It’s possible to set up and use a <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a> with <code>AWSCluster</code> instead of the
Classic Load Balancer that is created by default.</p>
<h2><a class="header" href="#awscluster-setting" id="awscluster-setting"><code>AWSCluster</code> setting</a></h2>
<p>To make CAPA create a network load balancer simply set the load balancer type to <code>network</code> like this:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: &quot;test-aws-cluster&quot;
spec:
  region: &quot;eu-central-1&quot;
  controlPlaneLoadBalancer:
    loadBalancerType: nlb
</code></pre>
<p>This will create the following objects:</p>
<ul>
<li>A network load balancer</li>
<li>Listeners</li>
<li>A target group</li>
</ul>
<p>It will also take into consideration IPv6 enabled clusters and create an IPv6 aware load balancer.</p>
<h2><a class="header" href="#preserve-client-ips" id="preserve-client-ips">Preserve Client IPs</a></h2>
<p>By default, client ip preservation is disabled. This is to avoid <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html#loopback-timeout">hairpinning</a> issues between kubelet and the node
registration process. To enable client IP preservation, you can set it to enable with the following flag:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: &quot;test-aws-cluster&quot;
spec:
  region: &quot;eu-central-1&quot;
  sshKeyName: &quot;capa-key&quot;
  controlPlaneLoadBalancer:
    loadBalancerType: nlb
    preserveClientIP: true
</code></pre>
<h2><a class="header" href="#security" id="security">Security</a></h2>
<p>NLBs can use security groups, but only if one is associated at the time of creation.
CAPA will associate the default control plane security groups with a new NLB by default.</p>
<p>For more information, see AWS’s <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html">Network Load Balancer and Security Groups</a> documentation.</p>
<h2><a class="header" href="#extension-of-the-code" id="extension-of-the-code">Extension of the code</a></h2>
<p>Right now, only NLBs and a Classic Load Balancer is supported. However, the code has been written in a way that it
should be easy to extend with an ALB or a GLB.</p>
<h1><a class="header" href="#enabling-a-secondary-control-plane-load-balancer" id="enabling-a-secondary-control-plane-load-balancer">Enabling a Secondary Control Plane Load Balancer</a></h1>
<h2><a class="header" href="#overview-10" id="overview-10">Overview</a></h2>
<p>It is possible to use a second control plane load balancer within a CAPA cluster.
This secondary control plane load balancer is primarily meant to be used for internal cluster traffic, for use cases where traffic between nodes and pods should be kept internal to the VPC network.
This adds a layer of privacy to traffic, as well as potentially saving on egress costs for traffic to the Kubernetes API server.</p>
<p>A dual load balancer topology is not used as a default in order to maintain backward compatibility with existing CAPA clusters.</p>
<h2><a class="header" href="#requirements-and-defaults" id="requirements-and-defaults">Requirements and defaults</a></h2>
<ul>
<li>A secondary control plane load balancer is <em>not</em> created by default.</li>
<li>The secondary control plane load balancer <em>must</em> be a <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a>, and will default to this type.</li>
<li>The secondary control plane load balancer must also be provided a name.</li>
<li>The secondary control plane’s <code>Scheme</code> defaults to <code>internal</code>, and <em>must</em> be different from the <code>spec.controlPlaneLoadBalancer</code>‘s <code>Scheme</code>.</li>
</ul>
<p>The secondary load balancer will use the same Security Group information as the primary control plane load balancer.</p>
<h2><a class="header" href="#creating-a-secondary-load-balancer" id="creating-a-secondary-load-balancer">Creating a secondary load balancer</a></h2>
<p>To create a secondary load balancer, add the <code>secondaryControlPlaneLoadBalancer</code> stanza to your <code>AWSCluster</code>.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: test-aws-cluster
spec:
  region: us-east-2
  sshKeyName: nrb-default
  secondaryControlPlaneLoadBalancer:
    name: internal-apiserver
    scheme: internal     # optional
</code></pre>
<h1><a class="header" href="#manage-local-zone-subnets" id="manage-local-zone-subnets">Manage Local Zone subnets</a></h1>
<h2><a class="header" href="#overview-11" id="overview-11">Overview</a></h2>
<p>CAPA provides the option to manage network resources required to provision compute nodes
to Local Zone and Wavelength Zone locations.</p>
<p><a href="https://aws.amazon.com/about-aws/global-infrastructure/localzones/">AWS Local Zones</a>
extends the cloud infrastructure to metropolitan regions,
allowing to deliver applications closer to the end-users, decreasing the
network latency.</p>
<p><a href="https://aws.amazon.com/wavelength/">AWS Wavelength Zones</a>
extends the AWS infrastructure deployments infrastructure to carrier infrastructure,
allowing to deploy within communications service providers’ (CSP) 5G networks.</p>
<p>When “edge zones” is mentioned in this document, it is referencing to AWS Local Zones and AWS Wavelength Zones.</p>
<h2><a class="header" href="#requirements-and-defaults-1" id="requirements-and-defaults-1">Requirements and defaults</a></h2>
<p>For both Local Zones and Wavelength Zones (’edge zones’):</p>
<ul>
<li>Subnets in edge zones are <em>not</em> created by default.</li>
<li>When you choose to CAPA manage edge zone’s subnets, you also must specify the
regular zones (Availability Zones) you will create the cluster.</li>
<li>IPv6 is not globally supported by AWS across Local Zones,
and is not supported in Wavelength zones, CAPA support is limited to IPv4
subnets in edge zones.</li>
<li>The subnets in edge zones will not be used by CAPA to create NAT Gateways,
Network Load Balancers, or provision Control Plane or Compute nodes by default.</li>
<li>NAT Gateways are not globally available to edge zone’s locations, the CAPA uses
the Parent Zone for the edge zone to create the NAT Gateway to allow the instances on
private subnets to egress traffic to the internet.</li>
<li>The CAPA subnet controllers discovers the zone attributes <code>ZoneType</code> and
<code>ParentZoneName</code> for each subnet on creation, those fields are used to ensure subnets for
it’s role. For example: only subnets with <code>ZoneType</code> with value <code>availability-zone</code>
can be used to create a load balancer for API.</li>
<li>It is required to manually opt-in to each zone group for edge zones you are planning to create subnets.</li>
</ul>
<p>The following steps are example to describe the zones and opt-into an zone group for an Local Zone:</p>
<pre><code>- To check the zone group name for a Local Zone, you can use the [EC2 API `DescribeAvailabilityZones`][describe-availability-zones]. For example:
</code></pre>
<pre><code class="language-sh">aws --region &quot;&lt;value_of_AWS_Region&gt;&quot; ec2 describe-availability-zones \
    --query 'AvailabilityZones[].[{ZoneName: ZoneName, GroupName: GroupName, Status: OptInStatus}]' \
    --filters Name=zone-type,Values=local-zone \
    --all-availability-zones
</code></pre>
<pre><code>- To opt-int the zone group, you can use the [EC2 API `ModifyZoneAttributes`](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyAvailabilityZoneGroup.html):
</code></pre>
<pre><code class="language-sh">aws ec2 modify-availability-zone-group \
    --group-name &quot;&lt;value_of_GroupName&gt;&quot; \
    --opt-in-status opted-in
</code></pre>
<h2><a class="header" href="#installing-managed-clusters-extending-subnets-to-local-zones" id="installing-managed-clusters-extending-subnets-to-local-zones">Installing managed clusters extending subnets to Local Zones</a></h2>
<p>To create a cluster with support of subnets on AWS Local Zones, add the <code>Subnets</code> stanza to your <code>AWSCluster.NetworkSpec</code>. Example:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: aws-cluster-localzone
spec:
  region: us-east-1
  networkSpec:
    vpc:
      cidrBlock: &quot;10.0.0.0/20&quot;
    subnets:
    # regular zones (availability zones)
    - availabilityZone: us-east-1a
      cidrBlock: &quot;10.0.0.0/24&quot;
      id: &quot;cluster-subnet-private-us-east-1a&quot;
      isPublic: false
    - availabilityZone: us-east-1a
      cidrBlock: &quot;10.0.1.0/24&quot;
      id: &quot;cluster-subnet-public-us-east-1a&quot;
      isPublic: true
    - availabilityZone: us-east-1b
      cidrBlock: &quot;10.0.3.0/24&quot;
      id: &quot;cluster-subnet-private-us-east-1b&quot;
      isPublic: false
    - availabilityZone: us-east-1b
      cidrBlock: &quot;10.0.4.0/24&quot;
      id: &quot;cluster-subnet-public-us-east-1b&quot;
      isPublic: true
    - availabilityZone: us-east-1c
      cidrBlock: &quot;10.0.5.0/24&quot;
      id: &quot;cluster-subnet-private-us-east-1c&quot;
      isPublic: false
    - availabilityZone: us-east-1c
      cidrBlock: &quot;10.0.6.0/24&quot;
      id: &quot;cluster-subnet-public-us-east-1c&quot;
      isPublic: true
    # Subnets in Local Zones of New York location (public and private)
    - availabilityZone: us-east-1-nyc-1a
      cidrBlock: &quot;10.0.128.0/25&quot;
      id: &quot;cluster-subnet-private-us-east-1-nyc-1a&quot;
      isPublic: false
    - availabilityZone: us-east-1-nyc-1a
      cidrBlock: &quot;10.0.128.128/25&quot;
      id: &quot;cluster-subnet-public-us-east-1-nyc-1a&quot;
      isPublic: true
</code></pre>
<h2><a class="header" href="#installing-managed-clusters-extending-subnets-to-wavelength-zones" id="installing-managed-clusters-extending-subnets-to-wavelength-zones">Installing managed clusters extending subnets to Wavelength Zones</a></h2>
<p>To create a cluster with support of subnets on AWS Wavelength Zones, add the <code>Subnets</code> stanza to your <code>AWSCluster.NetworkSpec</code>. Example:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: aws-cluster-wavelengthzone
spec:
  region: us-east-1
  networkSpec:
    vpc:
      cidrBlock: &quot;10.0.0.0/20&quot;
    subnets:
    # &lt;placeholder for regular zones (availability zones)&gt;
    - availabilityZone: us-east-1-wl1-was-wlz-1
      cidrBlock: &quot;10.0.128.0/25&quot;
      id: &quot;cluster-subnet-private-us-east-1-wl1-was-wlz-1&quot;
      isPublic: false
    - availabilityZone: us-east-1-wl1-was-wlz-1
      cidrBlock: &quot;10.0.128.128/25&quot;
      id: &quot;cluster-subnet-public-us-east-1-wl1-was-wlz-1&quot;
      isPublic: true
</code></pre>
<h2><a class="header" href="#installing-managed-clusters-extending-subnets-to-local-and-wavelength-zones" id="installing-managed-clusters-extending-subnets-to-local-and-wavelength-zones">Installing managed clusters extending subnets to Local and Wavelength Zones</a></h2>
<p>It is also possible to mix the creation across both Local and Wavelength zones.</p>
<p>To create a cluster with support of edge zones, add the <code>Subnets</code> stanza to your <code>AWSCluster.NetworkSpec</code>. Example:</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: aws-cluster-edge
spec:
  region: us-east-1
  networkSpec:
    vpc:
      cidrBlock: &quot;10.0.0.0/20&quot;
    subnets:
    # &lt;placeholder for regular zones (availability zones)&gt;
    - availabilityZone: us-east-1-nyc-1a
      cidrBlock: &quot;10.0.128.0/25&quot;
      id: &quot;cluster-subnet-private-us-east-1-nyc-1a&quot;
      isPublic: false
    - availabilityZone: us-east-1-nyc-1a
      cidrBlock: &quot;10.0.128.128/25&quot;
      id: &quot;cluster-subnet-public-us-east-1-nyc-1a&quot;
      isPublic: true
    - availabilityZone: us-east-1-wl1-was-wlz-1
      cidrBlock: &quot;10.0.129.0/25&quot;
      id: &quot;cluster-subnet-private-us-east-1-wl1-was-wlz-1&quot;
      isPublic: false
    - availabilityZone: us-east-1-wl1-was-wlz-1
      cidrBlock: &quot;10.0.129.128/25&quot;
      id: &quot;cluster-subnet-public-us-east-1-wl1-was-wlz-1&quot;
      isPublic: true
</code></pre>
<h2><a class="header" href="#clusterawsadm-1" id="clusterawsadm-1">clusterawsadm</a></h2>
<p>Kubernetes Cluster API Provider AWS Management Utility</p>
<h3><a class="header" href="#synopsis" id="synopsis">Synopsis</a></h3>
<p>clusterawsadm provides helpers for bootstrapping Kubernetes Cluster API Provider AWS. Use clusterawsadm to view required AWS Identity and Access Management (IAM) policies as JSON docs, or create IAM roles and instance profiles automatically using AWS CloudFormation.</p>
<p>clusterawsadm additionally helps provide credentials for use with clusterctl.</p>
<pre><code>clusterawsadm [flags]
</code></pre>
<h3><a class="header" href="#examples-3" id="examples-3">Examples</a></h3>
<pre><code>  # Create AWS Identity and Access Management (IAM) roles for use with
  # Kubernetes Cluster API Provider AWS.
  clusterawsadm bootstrap iam create-cloudformation-stack
  
  # Encode credentials for use with clusterctl init
  export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
  clusterctl init --infrastructure aws
</code></pre>
<h3><a class="header" href="#options" id="options">Options</a></h3>
<pre><code>  -h, --help    help for clusterawsadm
  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-2" id="see-also-2">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_ami.html">clusterawsadm ami</a>	 - AMI commands</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap.html">clusterawsadm bootstrap</a>	 - bootstrap commands</li>
<li><a href="clusterawsadm/clusterawsadm_controller.html">clusterawsadm controller</a>	 - controller commands</li>
<li><a href="clusterawsadm/clusterawsadm_eks.html">clusterawsadm eks</a>	 - Commands related to EKS</li>
<li><a href="clusterawsadm/clusterawsadm_gc.html">clusterawsadm gc</a>	 - Commands related to garbage collecting external resources of clusters</li>
<li><a href="clusterawsadm/clusterawsadm_resource.html">clusterawsadm resource</a>	 - Commands related to AWS resources</li>
<li><a href="clusterawsadm/clusterawsadm_version.html">clusterawsadm version</a>	 - Print version of clusterawsadm</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026" id="auto-generated-by-spf13cobra-on-14-jan-2026">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap" id="clusterawsadm-bootstrap">clusterawsadm bootstrap</a></h2>
<p>bootstrap commands</p>
<h3><a class="header" href="#synopsis-1" id="synopsis-1">Synopsis</a></h3>
<p>In order to use Kubernetes Cluster API Provider AWS, an AWS account needs to be prepared with AWS Identity and Access Management (IAM) roles to be used by clusters as well as provide Kubernetes Cluster API Provider AWS with credentials to use to provision infrastructure.</p>
<pre><code>clusterawsadm bootstrap [command] [flags]
</code></pre>
<h3><a class="header" href="#options-1" id="options-1">Options</a></h3>
<pre><code>  -h, --help   help for bootstrap
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands" id="options-inherited-from-parent-commands">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-3" id="see-also-3">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_credentials.html">clusterawsadm bootstrap credentials</a>	 - Encode credentials to use with Kubernetes Cluster API Provider AWS</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-1" id="auto-generated-by-spf13cobra-on-14-jan-2026-1">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-credentials" id="clusterawsadm-bootstrap-credentials">clusterawsadm bootstrap credentials</a></h2>
<p>Encode credentials to use with Kubernetes Cluster API Provider AWS</p>
<h3><a class="header" href="#synopsis-2" id="synopsis-2">Synopsis</a></h3>
<p>Encode credentials to use with Kubernetes Cluster API Provider AWS.</p>
<p>The utility will attempt to find credentials in the following order:</p>
<ol>
<li>Check for the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.</li>
<li>Read the default credentials from the shared configuration files ~/.aws/credentials or the default profile in ~/.aws/config.</li>
<li>Check for the presence of an EC2 IAM instance profile if it’s running on AWS.</li>
<li>Check for ECS credentials.</li>
</ol>
<p>IAM role assumption can be performed by using any valid configuration for the AWS CLI at: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. For role assumption to be used, a region is required for the utility to use the AWS Security Token Service (STS). The utility resolves the region in the following order:</p>
<ol>
<li>Check for the --region flag.</li>
<li>Check for the AWS_REGION environment variable.</li>
<li>Check for the DEFAULT_AWS_REGION environment variable.</li>
<li>Check that a region is specified in the shared configuration file.</li>
</ol>
<p>The utility will then generate an ini-file with a default profile corresponding to the resolved credentials.</p>
<p>If a region cannot be found, for the purposes of using AWS Security Token Service, this utility will fall back to us-east-1. This does not affect the region in which clusters will be created.</p>
<p>In the case of an instance profile or role assumption, note that encoded credentials are time-limited.</p>
<pre><code>clusterawsadm bootstrap credentials [flags]
</code></pre>
<h3><a class="header" href="#examples-4" id="examples-4">Examples</a></h3>
<pre><code>  # Encode credentials from the environment for use with clusterctl
  export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
  clusterctl init --infrastructure aws
</code></pre>
<h3><a class="header" href="#options-2" id="options-2">Options</a></h3>
<pre><code>  -h, --help   help for credentials
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-1" id="options-inherited-from-parent-commands-1">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-4" id="see-also-4">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap.html">clusterawsadm bootstrap</a>	 - bootstrap commands</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_credentials_encode-as-profile.html">clusterawsadm bootstrap credentials encode-as-profile</a>	 - Generate an AWS profile from the current environment</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-2" id="auto-generated-by-spf13cobra-on-14-jan-2026-2">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-credentials-encode-as-profile" id="clusterawsadm-bootstrap-credentials-encode-as-profile">clusterawsadm bootstrap credentials encode-as-profile</a></h2>
<p>Generate an AWS profile from the current environment</p>
<h3><a class="header" href="#synopsis-3" id="synopsis-3">Synopsis</a></h3>
<p>Generate an AWS profile from the current environment for the ephemeral bootstrap cluster.</p>
<p>The utility will attempt to find credentials in the following order:</p>
<ol>
<li>Check for the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.</li>
<li>Read the default credentials from the shared configuration files ~/.aws/credentials or the default profile in ~/.aws/config.</li>
<li>Check for the presence of an EC2 IAM instance profile if it’s running on AWS.</li>
<li>Check for ECS credentials.</li>
</ol>
<p>IAM role assumption can be performed by using any valid configuration for the AWS CLI at: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. For role assumption to be used, a region is required for the utility to use the AWS Security Token Service (STS). The utility resolves the region in the following order:</p>
<ol>
<li>Check for the --region flag.</li>
<li>Check for the AWS_REGION environment variable.</li>
<li>Check for the DEFAULT_AWS_REGION environment variable.</li>
<li>Check that a region is specified in the shared configuration file.</li>
</ol>
<p>The utility will then generate an ini-file with a default profile corresponding to the resolved credentials.</p>
<p>If a region cannot be found, for the purposes of using AWS Security Token Service, this utility will fall back to us-east-1. This does not affect the region in which clusters will be created.</p>
<p>In the case of an instance profile or role assumption, note that encoded credentials are time-limited.</p>
<pre><code>clusterawsadm bootstrap credentials encode-as-profile [flags]
</code></pre>
<h3><a class="header" href="#examples-5" id="examples-5">Examples</a></h3>
<pre><code>  # Encode credentials from the environment for use with clusterctl
  export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
  clusterctl init --infrastructure aws
</code></pre>
<h3><a class="header" href="#options-3" id="options-3">Options</a></h3>
<pre><code>  -h, --help             help for encode-as-profile
      --output string    Output for credential configuration (rawSharedConfig, base64SharedConfig) (default &quot;base64SharedConfig&quot;)
      --profile string   The AWS profile to use for authentication
      --region string    The AWS region in which to provision
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-2" id="options-inherited-from-parent-commands-2">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-5" id="see-also-5">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_credentials.html">clusterawsadm bootstrap credentials</a>	 - Encode credentials to use with Kubernetes Cluster API Provider AWS</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-3" id="auto-generated-by-spf13cobra-on-14-jan-2026-3">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam" id="clusterawsadm-bootstrap-iam">clusterawsadm bootstrap iam</a></h2>
<p>View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</p>
<h3><a class="header" href="#synopsis-4" id="synopsis-4">Synopsis</a></h3>
<p>View/output AWS Identity and Access Management (IAM) policy documents required for configuring Kubernetes Cluster API Provider AWS as well as create/update AWS IAM resources using AWS CloudFormation.</p>
<pre><code>clusterawsadm bootstrap iam [command] [flags]
</code></pre>
<h3><a class="header" href="#options-4" id="options-4">Options</a></h3>
<pre><code>  -h, --help   help for iam
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-3" id="options-inherited-from-parent-commands-3">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-6" id="see-also-6">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap.html">clusterawsadm bootstrap</a>	 - bootstrap commands</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam_create-cloudformation-stack.html">clusterawsadm bootstrap iam create-cloudformation-stack</a>	 - Create or update an AWS CloudFormation stack</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam_delete-cloudformation-stack.html">clusterawsadm bootstrap iam delete-cloudformation-stack</a>	 - Delete an AWS CloudFormation stack</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-cloudformation-template.html">clusterawsadm bootstrap iam print-cloudformation-template</a>	 - Print cloudformation template</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-config.html">clusterawsadm bootstrap iam print-config</a>	 - Print configuration</li>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam_print-policy.html">clusterawsadm bootstrap iam print-policy</a>	 - Generate and show an IAM policy</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-4" id="auto-generated-by-spf13cobra-on-14-jan-2026-4">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam-print-config" id="clusterawsadm-bootstrap-iam-print-config">clusterawsadm bootstrap iam print-config</a></h2>
<p>Print configuration</p>
<h3><a class="header" href="#synopsis-5" id="synopsis-5">Synopsis</a></h3>
<p>Print configuration</p>
<pre><code>clusterawsadm bootstrap iam print-config [flags]
</code></pre>
<h3><a class="header" href="#examples-6" id="examples-6">Examples</a></h3>
<pre><code>  # Print the default configuration.
  clusterawsadm bootstrap iam print-config
  
  # Apply defaults to a configuration file and print the result
  clusterawsadm bootstrap iam print-config --config bootstrap_config.yaml
</code></pre>
<h3><a class="header" href="#options-5" id="options-5">Options</a></h3>
<pre><code>      --config string   clusterawsadm will load a bootstrap configuration from this file. The path may be absolute or relative; relative paths start at the current working directory.
                        
                         The configuration file is a Kubernetes YAML using the bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1/AWSIAMConfiguration kind.
                        
                         Documentation for this kind can be found at: https://pkg.go.dev/sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/bootstrap/v1beta1
                        
                         To see the default configuration, run 'clusterawsadm bootstrap iam print-config'.
  -h, --help            help for print-config
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-4" id="options-inherited-from-parent-commands-4">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-7" id="see-also-7">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-5" id="auto-generated-by-spf13cobra-on-14-jan-2026-5">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam-print-policy" id="clusterawsadm-bootstrap-iam-print-policy">clusterawsadm bootstrap iam print-policy</a></h2>
<p>Generate and show an IAM policy</p>
<h3><a class="header" href="#synopsis-6" id="synopsis-6">Synopsis</a></h3>
<p>Generate and show an AWS Identity and Access Management (IAM) policy for Kubernetes Cluster API Provider AWS.</p>
<pre><code>clusterawsadm bootstrap iam print-policy [flags]
</code></pre>
<h3><a class="header" href="#examples-7" id="examples-7">Examples</a></h3>
<pre><code>  # Print out all the IAM policies for the Kubernetes CLuster API Provider AWS.
  clusterawsadm bootstrap iam print-policy
  
  # Print out the IAM policy for the Kubernetes Cluster API Provider AWS Controller.
  clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyControllers
  
  # Print out the IAM policy for the Kubernetes Cluster API Provider AWS Controller using a given configuration file.
  clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyControllers --config bootstrap_config.yaml
  
  # Print out the IAM policy for the Kubernetes AWS Cloud Provider for the control plane.
  clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyCloudProviderControlPlane
  
  # Print out the IAM policy for the Kubernetes AWS Cloud Provider for all nodes.
  clusterawsadm bootstrap iam print-policy --document AWSIAMManagedPolicyCloudProviderNodes
  
  # Print out the IAM policy for the Kubernetes AWS EBS CSI Driver Controller.
  # Note that this is available only when 'spec.controlPlane.enableCSIPolicy' is set to 'true' in the configuration file.
  clusterawsadm bootstrap iam print-policy --document AWSEBSCSIPolicyControllerc
</code></pre>
<h3><a class="header" href="#options-6" id="options-6">Options</a></h3>
<pre><code>      --config string     clusterawsadm will load a bootstrap configuration from this file. The path may be absolute or relative; relative paths start at the current working directory.
                          
                           The configuration file is a Kubernetes YAML using the bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1/AWSIAMConfiguration kind.
                          
                           Documentation for this kind can be found at: https://pkg.go.dev/sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/bootstrap/v1beta1
                          
                           To see the default configuration, run 'clusterawsadm bootstrap iam print-config'.
      --document string   which document to show: [AWSIAMManagedPolicyControllers AWSIAMManagedPolicyControllersEKS AWSIAMManagedPolicyCloudProviderControlPlane AWSIAMManagedPolicyCloudProviderNodes AWSEBSCSIPolicyController]
  -h, --help              help for print-policy
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-5" id="options-inherited-from-parent-commands-5">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-8" id="see-also-8">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-6" id="auto-generated-by-spf13cobra-on-14-jan-2026-6">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam-create-cloudformation-stack" id="clusterawsadm-bootstrap-iam-create-cloudformation-stack">clusterawsadm bootstrap iam create-cloudformation-stack</a></h2>
<p>Create or update an AWS CloudFormation stack</p>
<h3><a class="header" href="#synopsis-7" id="synopsis-7">Synopsis</a></h3>
<p>Create or update an AWS CloudFormation stack for bootstrapping Kubernetes Cluster API and Kubernetes AWS Identity and Access Management (IAM) permissions. To use this command, there must be AWS credentials loaded in this environment.</p>
<p>The utility will attempt to find credentials in the following order:</p>
<ol>
<li>Check for the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.</li>
<li>Read the default credentials from the shared configuration files ~/.aws/credentials or the default profile in ~/.aws/config.</li>
<li>Check for the presence of an EC2 IAM instance profile if it’s running on AWS.</li>
<li>Check for ECS credentials.</li>
</ol>
<p>IAM role assumption can be performed by using any valid configuration for the AWS CLI at: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. For role assumption to be used, a region is required for the utility to use the AWS Security Token Service (STS). The utility resolves the region in the following order:</p>
<ol>
<li>Check for the --region flag.</li>
<li>Check for the AWS_REGION environment variable.</li>
<li>Check for the DEFAULT_AWS_REGION environment variable.</li>
<li>Check that a region is specified in the shared configuration file.</li>
</ol>
<pre><code>clusterawsadm bootstrap iam create-cloudformation-stack [flags]
</code></pre>
<h3><a class="header" href="#examples-8" id="examples-8">Examples</a></h3>
<pre><code>  # Create or update IAM roles and policies for Kubernetes using a AWS CloudFormation stack.
  clusterawsadm bootstrap iam create-cloudformation-stack
  
  # Create or update IAM roles and policies for Kubernetes using a AWS CloudFormation stack with a custom configuration.
  clusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap_config.yaml
</code></pre>
<h3><a class="header" href="#options-7" id="options-7">Options</a></h3>
<pre><code>      --config string   clusterawsadm will load a bootstrap configuration from this file. The path may be absolute or relative; relative paths start at the current working directory.
                        
                         The configuration file is a Kubernetes YAML using the bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1/AWSIAMConfiguration kind.
                        
                         Documentation for this kind can be found at: https://pkg.go.dev/sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/bootstrap/v1beta1
                        
                         To see the default configuration, run 'clusterawsadm bootstrap iam print-config'.
  -h, --help            help for create-cloudformation-stack
      --region string   The AWS region in which to provision
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-6" id="options-inherited-from-parent-commands-6">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-9" id="see-also-9">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-7" id="auto-generated-by-spf13cobra-on-14-jan-2026-7">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam-delete-cloudformation-stack" id="clusterawsadm-bootstrap-iam-delete-cloudformation-stack">clusterawsadm bootstrap iam delete-cloudformation-stack</a></h2>
<p>Delete an AWS CloudFormation stack</p>
<h3><a class="header" href="#synopsis-8" id="synopsis-8">Synopsis</a></h3>
<p>Delete the AWS CloudFormation stack that created AWS Identity and Access Management (IAM) resources for use with Kubernetes Cluster API Provider AWS.</p>
<pre><code>clusterawsadm bootstrap iam delete-cloudformation-stack [flags]
</code></pre>
<h3><a class="header" href="#options-8" id="options-8">Options</a></h3>
<pre><code>      --config string   clusterawsadm will load a bootstrap configuration from this file. The path may be absolute or relative; relative paths start at the current working directory.
                        
                         The configuration file is a Kubernetes YAML using the bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1/AWSIAMConfiguration kind.
                        
                         Documentation for this kind can be found at: https://pkg.go.dev/sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/bootstrap/v1beta1
                        
                         To see the default configuration, run 'clusterawsadm bootstrap iam print-config'.
  -h, --help            help for delete-cloudformation-stack
      --region string   The AWS region in which to provision
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-7" id="options-inherited-from-parent-commands-7">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-10" id="see-also-10">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-8" id="auto-generated-by-spf13cobra-on-14-jan-2026-8">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-bootstrap-iam-print-cloudformation-template" id="clusterawsadm-bootstrap-iam-print-cloudformation-template">clusterawsadm bootstrap iam print-cloudformation-template</a></h2>
<p>Print cloudformation template</p>
<h3><a class="header" href="#synopsis-9" id="synopsis-9">Synopsis</a></h3>
<p>Generate and print out a CloudFormation template that can be used to provision AWS Identity and Access Management (IAM) policies and roles for use with Kubernetes Cluster API Provider AWS.</p>
<pre><code>clusterawsadm bootstrap iam print-cloudformation-template [flags]
</code></pre>
<h3><a class="header" href="#examples-9" id="examples-9">Examples</a></h3>
<pre><code>  # Print out the default CloudFormation template.
  clusterawsadm bootstrap iam print-cloudformation-template
  
  # Print out a CloudFormation template using a custom configuration.
  clusterawsadm bootstrap iam print-cloudformation-template --config bootstrap_config.yaml
</code></pre>
<h3><a class="header" href="#options-9" id="options-9">Options</a></h3>
<pre><code>      --config string   clusterawsadm will load a bootstrap configuration from this file. The path may be absolute or relative; relative paths start at the current working directory.
                        
                         The configuration file is a Kubernetes YAML using the bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1/AWSIAMConfiguration kind.
                        
                         Documentation for this kind can be found at: https://pkg.go.dev/sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/bootstrap/v1beta1
                        
                         To see the default configuration, run 'clusterawsadm bootstrap iam print-config'.
  -h, --help            help for print-cloudformation-template
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-8" id="options-inherited-from-parent-commands-8">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-11" id="see-also-11">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_bootstrap_iam.html">clusterawsadm bootstrap iam</a>	 - View required AWS IAM policies and create/update IAM roles using AWS CloudFormation</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-9" id="auto-generated-by-spf13cobra-on-14-jan-2026-9">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-controller" id="clusterawsadm-controller">clusterawsadm controller</a></h2>
<p>controller commands</p>
<h3><a class="header" href="#synopsis-10" id="synopsis-10">Synopsis</a></h3>
<p>All controller related actions such as:
Zero controller credentials and rollout controllers</p>
<pre><code>clusterawsadm controller [command] [flags]
</code></pre>
<h3><a class="header" href="#options-10" id="options-10">Options</a></h3>
<pre><code>  -h, --help   help for controller
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-9" id="options-inherited-from-parent-commands-9">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-12" id="see-also-12">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_controller_print-credentials.html">clusterawsadm controller print-credentials</a>	 - print credentials the controller is using</li>
<li><a href="clusterawsadm/clusterawsadm_controller_rollout-controller.html">clusterawsadm controller rollout-controller</a>	 - initiates rollout and restart on capa-controller-manager deployment</li>
<li><a href="clusterawsadm/clusterawsadm_controller_update-credentials.html">clusterawsadm controller update-credentials</a>	 - update credentials the controller is using (i.e., update controller bootstrap secret)</li>
<li><a href="clusterawsadm/clusterawsadm_controller_zero-credentials.html">clusterawsadm controller zero-credentials</a>	 - zero credentials the controller is started with</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-10" id="auto-generated-by-spf13cobra-on-14-jan-2026-10">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-controller-print-credentials" id="clusterawsadm-controller-print-credentials">clusterawsadm controller print-credentials</a></h2>
<p>print credentials the controller is using</p>
<h3><a class="header" href="#synopsis-11" id="synopsis-11">Synopsis</a></h3>
<p>print credentials the controller is using</p>
<pre><code>clusterawsadm controller print-credentials [flags]
</code></pre>
<h3><a class="header" href="#examples-10" id="examples-10">Examples</a></h3>
<pre><code>  # print credentials
  clusterawsadm controller print-credentials --kubeconfig=kubeconfig --namespace=capa-system
</code></pre>
<h3><a class="header" href="#options-11" id="options-11">Options</a></h3>
<pre><code>  -h, --help                        help for print-credentials
      --kubeconfig string           Path to the kubeconfig file to use for the management cluster. If empty, default discovery rules apply.
      --kubeconfig-context string   Context to be used within the kubeconfig file. If empty, current context will be used.
      --namespace string            Namespace the controllers are in. If empty, default value (capa-system) is used (default &quot;capa-system&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-10" id="options-inherited-from-parent-commands-10">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-13" id="see-also-13">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_controller.html">clusterawsadm controller</a>	 - controller commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-11" id="auto-generated-by-spf13cobra-on-14-jan-2026-11">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-controller-rollout-controller" id="clusterawsadm-controller-rollout-controller">clusterawsadm controller rollout-controller</a></h2>
<p>initiates rollout and restart on capa-controller-manager deployment</p>
<h3><a class="header" href="#synopsis-12" id="synopsis-12">Synopsis</a></h3>
<p>initiates rollout and restart on capa-controller-manager deployment</p>
<pre><code>clusterawsadm controller rollout-controller [flags]
</code></pre>
<h3><a class="header" href="#examples-11" id="examples-11">Examples</a></h3>
<pre><code>  # rollout controller deployment
  clusterawsadm controller rollout-controller --kubeconfig=kubeconfig --namespace=capa-system
</code></pre>
<h3><a class="header" href="#options-12" id="options-12">Options</a></h3>
<pre><code>  -h, --help                        help for rollout-controller
      --kubeconfig string           Path to the kubeconfig file to use for the management cluster. If empty, default discovery rules apply.
      --kubeconfig-context string   Context to be used within the kubeconfig file. If empty, current context will be used.
      --namespace string            Namespace the controllers are in. If empty, default value (capa-system) is used (default &quot;capa-system&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-11" id="options-inherited-from-parent-commands-11">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-14" id="see-also-14">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_controller.html">clusterawsadm controller</a>	 - controller commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-12" id="auto-generated-by-spf13cobra-on-14-jan-2026-12">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-controller-update-credentials" id="clusterawsadm-controller-update-credentials">clusterawsadm controller update-credentials</a></h2>
<p>update credentials the controller is using (i.e., update controller bootstrap secret)</p>
<h3><a class="header" href="#synopsis-13" id="synopsis-13">Synopsis</a></h3>
<p>Update credentials the controller is started with</p>
<pre><code>clusterawsadm controller update-credentials [flags]
</code></pre>
<h3><a class="header" href="#examples-12" id="examples-12">Examples</a></h3>
<pre><code>  # update credentials: AWS_B64ENCODED_CREDENTIALS environment variable must be set and be used to update the bootstrap secret
  # Kubeconfig file will be searched in default locations
  clusterawsadm controller update-credentials --namespace=capa-system
  # Provided kubeconfig file will be used
  clusterawsadm controller update-credentials --kubeconfig=kubeconfig  --namespace=capa-system
  # Kubeconfig in the default location will be retrieved and the provided context will be used
  clusterawsadm controller update-credentials --kubeconfig-context=mgmt-cluster  --namespace=capa-system
</code></pre>
<h3><a class="header" href="#options-13" id="options-13">Options</a></h3>
<pre><code>  -h, --help                        help for update-credentials
      --kubeconfig string           Path to the kubeconfig file to use for the management cluster. If empty, default discovery rules apply.
      --kubeconfig-context string   Context to be used within the kubeconfig file. If empty, current context will be used.
      --namespace string            Namespace the controllers are in. If empty, default value (capa-system) is used (default &quot;capa-system&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-12" id="options-inherited-from-parent-commands-12">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-15" id="see-also-15">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_controller.html">clusterawsadm controller</a>	 - controller commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-13" id="auto-generated-by-spf13cobra-on-14-jan-2026-13">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-controller-zero-credentials" id="clusterawsadm-controller-zero-credentials">clusterawsadm controller zero-credentials</a></h2>
<p>zero credentials the controller is started with</p>
<h3><a class="header" href="#synopsis-14" id="synopsis-14">Synopsis</a></h3>
<p>Zero credentials the controller is started with</p>
<pre><code>clusterawsadm controller zero-credentials [flags]
</code></pre>
<h3><a class="header" href="#examples-13" id="examples-13">Examples</a></h3>
<pre><code>  # zero credentials
  # Kubeconfig file will be searched in default locations
  clusterawsadm controller zero-credentials --namespace=capa-system
  # Provided kubeconfig file will be used
  clusterawsadm controller zero-credentials --kubeconfig=kubeconfig  --namespace=capa-system
  # Kubeconfig in the default location will be retrieved and the provided context will be used
  clusterawsadm controller zero-credentials --kubeconfig-context=mgmt-cluster  --namespace=capa-system
</code></pre>
<h3><a class="header" href="#options-14" id="options-14">Options</a></h3>
<pre><code>  -h, --help                        help for zero-credentials
      --kubeconfig string           Path to the kubeconfig file to use for the management cluster. If empty, default discovery rules apply.
      --kubeconfig-context string   Context to be used within the kubeconfig file. If empty, current context will be used.
      --namespace string            Namespace the controllers are in. If empty, default value (capa-system) is used (default &quot;capa-system&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-13" id="options-inherited-from-parent-commands-13">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-16" id="see-also-16">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_controller.html">clusterawsadm controller</a>	 - controller commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-14" id="auto-generated-by-spf13cobra-on-14-jan-2026-14">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-eks" id="clusterawsadm-eks">clusterawsadm eks</a></h2>
<p>Commands related to EKS</p>
<pre><code>clusterawsadm eks [flags]
</code></pre>
<h3><a class="header" href="#options-15" id="options-15">Options</a></h3>
<pre><code>  -h, --help   help for eks
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-14" id="options-inherited-from-parent-commands-14">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-17" id="see-also-17">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_eks_addons.html">clusterawsadm eks addons</a>	 - Commands related to EKS addons</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-15" id="auto-generated-by-spf13cobra-on-14-jan-2026-15">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-eks-addons" id="clusterawsadm-eks-addons">clusterawsadm eks addons</a></h2>
<p>Commands related to EKS addons</p>
<pre><code>clusterawsadm eks addons [flags]
</code></pre>
<h3><a class="header" href="#options-16" id="options-16">Options</a></h3>
<pre><code>  -h, --help   help for addons
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-15" id="options-inherited-from-parent-commands-15">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-18" id="see-also-18">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_eks.html">clusterawsadm eks</a>	 - Commands related to EKS</li>
<li><a href="clusterawsadm/clusterawsadm_eks_addons_list-available.html">clusterawsadm eks addons list-available</a>	 - List available EKS addons</li>
<li><a href="clusterawsadm/clusterawsadm_eks_addons_list-installed.html">clusterawsadm eks addons list-installed</a>	 - List installed EKS addons</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-16" id="auto-generated-by-spf13cobra-on-14-jan-2026-16">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-eks-addons-list-available" id="clusterawsadm-eks-addons-list-available">clusterawsadm eks addons list-available</a></h2>
<p>List available EKS addons</p>
<h3><a class="header" href="#synopsis-15" id="synopsis-15">Synopsis</a></h3>
<p>Lists the addons that are available for use with an EKS cluster</p>
<pre><code>clusterawsadm eks addons list-available [flags]
</code></pre>
<h3><a class="header" href="#options-17" id="options-17">Options</a></h3>
<pre><code>  -n, --cluster-name string   The name of the cluster to get the list of available addons for
  -h, --help                  help for list-available
  -o, --output string         The output format of the results. Possible values: table,json,yaml (default &quot;table&quot;)
  -r, --region string         The AWS region containing the EKS cluster
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-16" id="options-inherited-from-parent-commands-16">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-19" id="see-also-19">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_eks_addons.html">clusterawsadm eks addons</a>	 - Commands related to EKS addons</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-17" id="auto-generated-by-spf13cobra-on-14-jan-2026-17">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-eks-addons-list-installed" id="clusterawsadm-eks-addons-list-installed">clusterawsadm eks addons list-installed</a></h2>
<p>List installed EKS addons</p>
<h3><a class="header" href="#synopsis-16" id="synopsis-16">Synopsis</a></h3>
<p>Lists the addons that are installed for an EKS cluster</p>
<pre><code>clusterawsadm eks addons list-installed [flags]
</code></pre>
<h3><a class="header" href="#options-18" id="options-18">Options</a></h3>
<pre><code>  -n, --cluster-name string   The name of the cluster to get the list of installed addons for
  -h, --help                  help for list-installed
  -o, --output string         The output format of the results. Possible values: table,json,yaml (default &quot;table&quot;)
  -r, --region string         The AWS region containing the EKS cluster
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-17" id="options-inherited-from-parent-commands-17">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-20" id="see-also-20">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_eks_addons.html">clusterawsadm eks addons</a>	 - Commands related to EKS addons</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-18" id="auto-generated-by-spf13cobra-on-14-jan-2026-18">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-gc" id="clusterawsadm-gc">clusterawsadm gc</a></h2>
<p>Commands related to garbage collecting external resources of clusters</p>
<pre><code>clusterawsadm gc [command] [flags]
</code></pre>
<h3><a class="header" href="#options-19" id="options-19">Options</a></h3>
<pre><code>  -h, --help   help for gc
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-18" id="options-inherited-from-parent-commands-18">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-21" id="see-also-21">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_gc_configure.html">clusterawsadm gc configure</a>	 - Specify what cleanup tasks will be executed on a given cluster</li>
<li><a href="clusterawsadm/clusterawsadm_gc_disable.html">clusterawsadm gc disable</a>	 - Mark a cluster as NOT requiring external resource garbage collection</li>
<li><a href="clusterawsadm/clusterawsadm_gc_enable.html">clusterawsadm gc enable</a>	 - Mark a cluster as requiring external resource garbage collection</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-19" id="auto-generated-by-spf13cobra-on-14-jan-2026-19">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-gc-configure" id="clusterawsadm-gc-configure">clusterawsadm gc configure</a></h2>
<p>Specify what cleanup tasks will be executed on a given cluster</p>
<h3><a class="header" href="#synopsis-17" id="synopsis-17">Synopsis</a></h3>
<p>This command will set what cleanup tasks to execute on the given cluster during garbage collection (i.e. deleting) when the cluster is requested to be deleted. Supported values: load-balancer, security-group, target-group.</p>
<pre><code>clusterawsadm gc configure [flags]
</code></pre>
<h3><a class="header" href="#examples-14" id="examples-14">Examples</a></h3>
<pre><code>  # Configure GC for a cluster to delete only load balancers and security groups using existing k8s context
  clusterawsadm gc configure --cluster-name=test-cluster --gc-task load-balancer --gc-task security-group
  
  # Reset GC configuration for a cluster using kubeconfig
  clusterawsadm gc configure --cluster-name=test-cluster --kubeconfig=test.kubeconfig
</code></pre>
<h3><a class="header" href="#options-20" id="options-20">Options</a></h3>
<pre><code>      --cluster-name string   The name of the CAPA cluster
      --gc-task strings       Garbage collection tasks to execute during cluster deletion
  -h, --help                  help for configure
      --kubeconfig string     Path to the kubeconfig file to use (default &quot;/home/vitor/.kube/config&quot;)
  -n, --namespace string      The namespace for the cluster definition (default &quot;default&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-19" id="options-inherited-from-parent-commands-19">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-22" id="see-also-22">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_gc.html">clusterawsadm gc</a>	 - Commands related to garbage collecting external resources of clusters</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-20" id="auto-generated-by-spf13cobra-on-14-jan-2026-20">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-gc-disable" id="clusterawsadm-gc-disable">clusterawsadm gc disable</a></h2>
<p>Mark a cluster as NOT requiring external resource garbage collection</p>
<h3><a class="header" href="#synopsis-18" id="synopsis-18">Synopsis</a></h3>
<p>This command will mark the given cluster as not requiring external resource garbage collection (i.e. deleting) when the cluster is requested to be deleted.</p>
<pre><code>clusterawsadm gc disable [flags]
</code></pre>
<h3><a class="header" href="#examples-15" id="examples-15">Examples</a></h3>
<pre><code>  # Disable GC for a cluster using existing k8s context
  clusterawsadm gc disable --cluster-name=test-cluster
  
  # Disable GC for a cluster using kubeconfig
  clusterawsadm gc disable --cluster-name=test-cluster --kubeconfig=test.kubeconfig
</code></pre>
<h3><a class="header" href="#options-21" id="options-21">Options</a></h3>
<pre><code>      --cluster-name string   The name of the CAPA cluster
  -h, --help                  help for disable
      --kubeconfig string     Path to the kubeconfig file to use (default &quot;/home/vitor/.kube/config&quot;)
  -n, --namespace string      The namespace for the cluster definition (default &quot;default&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-20" id="options-inherited-from-parent-commands-20">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-23" id="see-also-23">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_gc.html">clusterawsadm gc</a>	 - Commands related to garbage collecting external resources of clusters</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-21" id="auto-generated-by-spf13cobra-on-14-jan-2026-21">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-gc-enable" id="clusterawsadm-gc-enable">clusterawsadm gc enable</a></h2>
<p>Mark a cluster as requiring external resource garbage collection</p>
<h3><a class="header" href="#synopsis-19" id="synopsis-19">Synopsis</a></h3>
<p>This command will mark the given cluster as requiring external resource garbage collection (i.e. deleting) when the cluster is requested to be deleted. This works by adding an annotation to the infra cluster.</p>
<pre><code>clusterawsadm gc enable [flags]
</code></pre>
<h3><a class="header" href="#examples-16" id="examples-16">Examples</a></h3>
<pre><code>  # Enable GC for a cluster using existing k8s context
  clusterawsadm gc enable --cluster-name=test-cluster
  
  # Enable GC for a cluster using kubeconfig
  clusterawsadm gc enable --cluster-name=test-cluster --kubeconfig=test.kubeconfig
</code></pre>
<h3><a class="header" href="#options-22" id="options-22">Options</a></h3>
<pre><code>      --cluster-name string   The name of the CAPA cluster
  -h, --help                  help for enable
      --kubeconfig string     Path to the kubeconfig file to use (default &quot;/home/vitor/.kube/config&quot;)
  -n, --namespace string      The namespace for the cluster definition (default &quot;default&quot;)
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-21" id="options-inherited-from-parent-commands-21">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-24" id="see-also-24">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_gc.html">clusterawsadm gc</a>	 - Commands related to garbage collecting external resources of clusters</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-22" id="auto-generated-by-spf13cobra-on-14-jan-2026-22">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-resource" id="clusterawsadm-resource">clusterawsadm resource</a></h2>
<p>Commands related to AWS resources</p>
<h3><a class="header" href="#synopsis-20" id="synopsis-20">Synopsis</a></h3>
<p>All AWS resources related actions such as:
List of AWS resources created by CAPA</p>
<pre><code>clusterawsadm resource [command] [flags]
</code></pre>
<h3><a class="header" href="#options-23" id="options-23">Options</a></h3>
<pre><code>  -h, --help   help for resource
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-22" id="options-inherited-from-parent-commands-22">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-25" id="see-also-25">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_resource_list.html">clusterawsadm resource list</a>	 - List all AWS resources created by CAPA</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-23" id="auto-generated-by-spf13cobra-on-14-jan-2026-23">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-resource-list" id="clusterawsadm-resource-list">clusterawsadm resource list</a></h2>
<p>List all AWS resources created by CAPA</p>
<h3><a class="header" href="#synopsis-21" id="synopsis-21">Synopsis</a></h3>
<p>List AWS resources directly created by CAPA based on region and cluster-name. There are some indirect resources like Cloudwatch alarms, rules, etc which are not directly created by CAPA, so those resources are not listed here. If region and cluster-name are not set, then it will throw an error.</p>
<pre><code>clusterawsadm resource list [flags]
</code></pre>
<h3><a class="header" href="#examples-17" id="examples-17">Examples</a></h3>
<pre><code>  # List AWS resources directly created by CAPA in given region and clustername
  clusterawsadm resource list --region=us-east-1 --cluster-name=test-cluster
</code></pre>
<h3><a class="header" href="#options-24" id="options-24">Options</a></h3>
<pre><code>  -n, --cluster-name string   The name of the cluster where AWS resources created by CAPA
  -h, --help                  help for list
  -o, --output string         The output format of the results. Possible values: table, json, yaml (default &quot;table&quot;)
  -r, --region string         The AWS region where resources are created by CAPA
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-23" id="options-inherited-from-parent-commands-23">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-26" id="see-also-26">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_resource.html">clusterawsadm resource</a>	 - Commands related to AWS resources</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-24" id="auto-generated-by-spf13cobra-on-14-jan-2026-24">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-version" id="clusterawsadm-version">clusterawsadm version</a></h2>
<p>Print version of clusterawsadm</p>
<pre><code>clusterawsadm version [flags]
</code></pre>
<h3><a class="header" href="#options-25" id="options-25">Options</a></h3>
<pre><code>  -h, --help            help for version
  -o, --output string   Output format; available options are 'yaml', 'json' and 'short'
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-24" id="options-inherited-from-parent-commands-24">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-27" id="see-also-27">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-25" id="auto-generated-by-spf13cobra-on-14-jan-2026-25">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-ami" id="clusterawsadm-ami">clusterawsadm ami</a></h2>
<p>AMI commands</p>
<h3><a class="header" href="#synopsis-22" id="synopsis-22">Synopsis</a></h3>
<p>All AMI related actions such as:
Copy AMIs based on Kubernetes version, OS etc from an AWS account where AMIs are stored
to the current AWS account (use case: air-gapped deployments)
(to be implemented) List available AMIs</p>
<pre><code>clusterawsadm ami [command] [flags]
</code></pre>
<h3><a class="header" href="#options-26" id="options-26">Options</a></h3>
<pre><code>  -h, --help   help for ami
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-25" id="options-inherited-from-parent-commands-25">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-28" id="see-also-28">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm.html">clusterawsadm</a>	 - Kubernetes Cluster API Provider AWS Management Utility</li>
<li><a href="clusterawsadm/clusterawsadm_ami_copy.html">clusterawsadm ami copy</a>	 - Copy AMIs from an AWS account to the AWS account which credentials are provided</li>
<li><a href="clusterawsadm/clusterawsadm_ami_encrypted-copy.html">clusterawsadm ami encrypted-copy</a>	 - Encrypt and copy AMI snapshot, then create an AMI with that snapshot</li>
<li><a href="clusterawsadm/clusterawsadm_ami_list.html">clusterawsadm ami list</a>	 - List AMIs from the default AWS account where AMIs are stored</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-26" id="auto-generated-by-spf13cobra-on-14-jan-2026-26">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-ami-copy" id="clusterawsadm-ami-copy">clusterawsadm ami copy</a></h2>
<p>Copy AMIs from an AWS account to the AWS account which credentials are provided</p>
<h3><a class="header" href="#synopsis-23" id="synopsis-23">Synopsis</a></h3>
<p>Copy AMIs based on Kubernetes version, OS, region from an AWS account where AMIs are stored to the current AWS account (use case: air-gapped deployments)</p>
<pre><code>clusterawsadm ami copy [flags]
</code></pre>
<h3><a class="header" href="#examples-18" id="examples-18">Examples</a></h3>
<pre><code>  # Copy AMI from the default AWS account where AMIs are stored.
  # Available os options: centos-7, ubuntu-24.04, ubuntu-22.04, amazon-2, flatcar-stable
  clusterawsadm ami copy --kubernetes-version=v1.30.1 --os=ubuntu-22.04  --region=us-west-2
  
  # owner-id and dry-run flags are optional. region can be set via flag or env
  clusterawsadm ami copy --os centos-7 --kubernetes-version=v1.19.4 --owner-id=111111111111 --dry-run
  
  # copy from us-east-1 to us-east-2
  clusterawsadm ami copy --os centos-7 --kubernetes-version=v1.19.4 --region us-east-2 --source-region us-east-1
</code></pre>
<h3><a class="header" href="#options-27" id="options-27">Options</a></h3>
<pre><code>      --dry-run                     Check if AMI exists and can be copied
  -h, --help                        help for copy
      --kubernetes-version string   Kubernetes version of the AMI to be copied
      --os string                   Operating system of the AMI to be copied
      --owner-id string             The source AWS owner ID, where the AMI will be copied from (default &quot;819546954734&quot;)
      --region string               The AWS region in which to provision
      --source-region string        Set if wanting to copy an AMI from a different region
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-26" id="options-inherited-from-parent-commands-26">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-29" id="see-also-29">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_ami.html">clusterawsadm ami</a>	 - AMI commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-27" id="auto-generated-by-spf13cobra-on-14-jan-2026-27">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-ami-encrypted-copy" id="clusterawsadm-ami-encrypted-copy">clusterawsadm ami encrypted-copy</a></h2>
<p>Encrypt and copy AMI snapshot, then create an AMI with that snapshot</p>
<h3><a class="header" href="#synopsis-24" id="synopsis-24">Synopsis</a></h3>
<p>Find the AMI based on Kubernetes version, OS, region in the AWS account where AMIs are stored. Encrypt and copy the snapshot of the AMI to the current AWS account. Create an AMI with that snapshot.</p>
<pre><code>clusterawsadm ami encrypted-copy [flags]
</code></pre>
<h3><a class="header" href="#examples-19" id="examples-19">Examples</a></h3>
<pre><code>  # Create an encrypted AMI:
  # Available os options: centos-7, ubuntu-24.04, ubuntu-22.04, amazon-2, flatcar-stable
  clusterawsadm ami encrypted-copy --kubernetes-version=v1.18.12 --os=ubuntu-20.04  --region=us-west-2
  
  # owner-id and dry-run flags are optional. region can be set via flag or env
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --owner-id=111111111111 --dry-run
  
  # copy from us-east-1 to us-east-2
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --owner-id=111111111111 --region us-east-2 --source-region us-east-1
  
  # Encrypt using a non-default KmsKeyId specified using Key ID:
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --kms-key-id=key/1234abcd-12ab-34cd-56ef-1234567890ab
  
  # Encrypt using a non-default KmsKeyId specified using Key alias:
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --kms-key-id=alias/ExampleAlias
  
  # Encrypt using a non-default KmsKeyId specified using Key ARN:
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --kms-key-id=arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef
  
  # Encrypt using a non-default KmsKeyId specified using Alias ARN:
  clusterawsadm ami encrypted-copy --os centos-7 --kubernetes-version=v1.19.4 --kms-key-id=arn:aws:kms:us-east-1:012345678910:alias/ExampleAlias
</code></pre>
<h3><a class="header" href="#options-28" id="options-28">Options</a></h3>
<pre><code>      --dry-run                     Check if AMI exists and can be copied
  -h, --help                        help for encrypted-copy
      --kms-key-id string           The ID of the KMS key for Amazon EBS encryption
      --kubernetes-version string   Kubernetes version of the AMI to be copied
      --os string                   Operating system of the AMI to be copied
      --owner-id string             The source AWS owner ID, where the AMI will be copied from (default &quot;819546954734&quot;)
      --region string               The AWS region in which to provision
      --source-region string        Set if wanting to copy an AMI from a different region
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-27" id="options-inherited-from-parent-commands-27">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-30" id="see-also-30">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_ami.html">clusterawsadm ami</a>	 - AMI commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-28" id="auto-generated-by-spf13cobra-on-14-jan-2026-28">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h2><a class="header" href="#clusterawsadm-ami-list" id="clusterawsadm-ami-list">clusterawsadm ami list</a></h2>
<p>List AMIs from the default AWS account where AMIs are stored</p>
<h3><a class="header" href="#synopsis-25" id="synopsis-25">Synopsis</a></h3>
<p>List AMIs based on Kubernetes version, OS, region. If no arguments are provided, it will print all AMIs in all regions, OS types for the supported Kubernetes versions. Supported Kubernetes versions start from the latest stable version and goes 2 release back: if the latest stable release is v1.20.4- v1.19.x and v1.18.x are supported. Note: First release of each version will be skipped, e.g., v1.21.0 To list AMIs of unsupported Kubernetes versions, --kubernetes-version flag needs to be provided.</p>
<pre><code>clusterawsadm ami list [flags]
</code></pre>
<h3><a class="header" href="#examples-20" id="examples-20">Examples</a></h3>
<pre><code>  # List AMIs from the default AWS account where AMIs are stored.
  # Available os options: centos-7, ubuntu-24.04, ubuntu-22.04, amazon-2, flatcar-stable
  clusterawsadm ami list --kubernetes-version=v1.18.12 --os=ubuntu-20.04  --region=us-west-2
  # To list all supported AMIs in all supported Kubernetes versions, regions, and linux distributions:
  clusterawsadm ami list
</code></pre>
<h3><a class="header" href="#options-29" id="options-29">Options</a></h3>
<pre><code>  -h, --help                        help for list
      --kubernetes-version string   Kubernetes version of the AMI to be copied
      --os string                   Operating system of the AMI to be listed
  -o, --output string               The output format of the results. Possible values: table,json,yaml (default &quot;table&quot;)
      --owner-id string             The owner ID of the AWS account to be used for listing AMIs
      --region string               The AWS region in which to provision
</code></pre>
<h3><a class="header" href="#options-inherited-from-parent-commands-28" id="options-inherited-from-parent-commands-28">Options inherited from parent commands</a></h3>
<pre><code>  -v, --v int   Set the log level verbosity. (default 2)
</code></pre>
<h3><a class="header" href="#see-also-31" id="see-also-31">SEE ALSO</a></h3>
<ul>
<li><a href="clusterawsadm/clusterawsadm_ami.html">clusterawsadm ami</a>	 - AMI commands</li>
</ul>
<h6><a class="header" href="#auto-generated-by-spf13cobra-on-14-jan-2026-29" id="auto-generated-by-spf13cobra-on-14-jan-2026-29">Auto generated by spf13/cobra on 14-Jan-2026</a></h6>
<h1><a class="header" href="#developer-guide" id="developer-guide">Developer Guide</a></h1>
<h2><a class="header" href="#initial-setup-for-development-environment" id="initial-setup-for-development-environment">Initial setup for development environment</a></h2>
<h3><a class="header" href="#install-prerequisites" id="install-prerequisites">Install prerequisites</a></h3>
<ol>
<li>Install <a href="https://golang.org/doc/install">go</a>
<ul>
<li>Get the latest patch version for go v1.22.</li>
</ul>
</li>
<li>Install <a href="https://stedolan.github.io/jq/download/">jq</a>
<ul>
<li><code>brew install jq</code> on macOS.</li>
<li><code>chocolatey install jq</code> on Windows.</li>
<li><code>sudo apt install jq</code> on Ubuntu Linux.</li>
</ul>
</li>
<li>Install <a href="https://sigs.k8s.io/kind">KIND</a>
<ul>
<li><code>GO111MODULE=&quot;on&quot; go get sigs.k8s.io/kind@v0.12.0</code>.</li>
</ul>
</li>
<li>Install <a href="https://github.com/kubernetes-sigs/kustomize">Kustomize</a>
<ul>
<li><a href="https://kubectl.docs.kubernetes.io/installation/kustomize/">install instructions</a></li>
</ul>
</li>
<li>Install <a href="https://github.com/a8m/envsubst">envsubst</a></li>
<li>Install make.</li>
<li>Install direnv
<ul>
<li><code>brew install direnv</code> on macOS.</li>
</ul>
</li>
<li>Set AWS Environment variable for an IAM Admin user
<ul>
<li>
<pre><code class="language-bash">export AWS_ACCESS_KEY_ID=ADMID
export AWS_SECRET_ACCESS_KEY=ADMKEY
export AWS_REGION=eu-west-1
</code></pre>
</li>
</ul>
</li>
</ol>
<h3><a class="header" href="#get-the-source" id="get-the-source">Get the source</a></h3>
<p>Fork the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws">cluster-api-provider-aws repo</a>:</p>
<pre><code class="language-bash">cd &quot;$(go env GOPATH)&quot;/src
mkdir sigs.k8s.io
cd sigs.k8s.io/
git clone git@github.com:&lt;GITHUB USERNAME&gt;/cluster-api-provider-aws.git
cd cluster-api-provider-aws
git remote add upstream git@github.com:kubernetes-sigs/cluster-api-provider-aws.git
git fetch upstream
</code></pre>
<h3><a class="header" href="#build-clusterawsadm" id="build-clusterawsadm">Build clusterawsadm</a></h3>
<p>Build <code>clusterawsadm</code> in <code>cluster-api-provider-aws</code>:</p>
<pre><code class="language-bash">cd &quot;$(go env GOPATH)&quot;/src/sigs.k8s.io/cluster-api-provider-aws/
make clusterawsadm
sudo mv ./bin/clusterawsadm /usr/local/bin/clusterawsadm
</code></pre>
<h3><a class="header" href="#setup-aws-environment" id="setup-aws-environment">Setup AWS Environment</a></h3>
<p>Create bootstrap file and bootstrap IAM roles and policies using <code>clusterawsadm</code></p>
<pre><code class="language-bash">$ cat config-bootstrap.yaml

apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  bootstrapUser:
    enable: true

$ clusterawsadm bootstrap iam create-cloudformation-stack
Attempting to create AWS CloudFormation stack cluster-api-provider-aws-sigs-k8s-io
</code></pre>
<h4><a class="header" href="#customizing-the-bootstrap-permission" id="customizing-the-bootstrap-permission">Customizing the bootstrap permission</a></h4>
<p>The IAM permissions can be customized by using a configuration file with <strong>clusterawsadm</strong>. For example, to create the default IAM role for use with managed machine pools:</p>
<pre><code class="language-bash">$ cat config-bootstrap.yaml
apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  bootstrapUser:
    enable: true
  eks:
    iamRoleCreation: false # Set to true if you plan to use the EKSEnableIAM feature flag to enable automatic creation of IAM roles
    managedMachinePool:
      disable: false # Set to false to enable creation of the default node role for managed machine pools
</code></pre>
<p>Use the configuration file to create the additional IAM role:</p>
<pre><code class="language-bash">$ clusterawsadm bootstrap iam create-cloudformation-stack --config=config-bootstrap.yaml
Attempting to create AWS CloudFormation stack cluster-api-provider-aws-sigs-k8s-io
</code></pre>
<blockquote>
<p>If you don’t plan on using EKS then see the <a href="development/../topics/eks/disabling.html">documentation on disabling EKS support</a>.</p>
</blockquote>
<h4><a class="header" href="#sample-output" id="sample-output">Sample Output</a></h4>
<p>When creating the CloudFormation stack using <strong>clusterawsadm</strong> you will see output similar to this:</p>
<pre><code class="language-bash">Following resources are in the stack:

Resource                  |Type                                                                                |Status
AWS::IAM::Group           |cluster-api-provider-aws-s-AWSIAMGroupBootstrapper-ME9XZVCO2491                     |CREATE_COMPLETE
AWS::IAM::InstanceProfile |control-plane.cluster-api-provider-aws.sigs.k8s.io                                  |CREATE_COMPLETE
AWS::IAM::InstanceProfile |controllers.cluster-api-provider-aws.sigs.k8s.io                                    |CREATE_COMPLETE
AWS::IAM::InstanceProfile |nodes.cluster-api-provider-aws.sigs.k8s.io                                          |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::xxx:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::xxx:policy/nodes.cluster-api-provider-aws.sigs.k8s.io         |CREATE_COMPLETE
AWS::IAM::ManagedPolicy   |arn:aws:iam::xxx:policy/controllers.cluster-api-provider-aws.sigs.k8s.io   |CREATE_COMPLETE
AWS::IAM::Role            |control-plane.cluster-api-provider-aws.sigs.k8s.io                                  |CREATE_COMPLETE
AWS::IAM::Role            |controllers.cluster-api-provider-aws.sigs.k8s.io                                    |CREATE_COMPLETE
AWS::IAM::Role            |eks-controlplane.cluster-api-provider-aws.sigs.k8s.io                               |CREATE_COMPLETE
AWS::IAM::Role            |eks-nodegroup.cluster-api-provider-aws.sigs.k8s.io                                  |CREATE_COMPLETE
AWS::IAM::Role            |nodes.cluster-api-provider-aws.sigs.k8s.io                                          |CREATE_COMPLETE
AWS::IAM::User            |bootstrapper.cluster-api-provider-aws.sigs.k8s.io                                   |CREATE_COMPLETE
</code></pre>
<h3><a class="header" href="#set-environment-variables" id="set-environment-variables">Set Environment Variables</a></h3>
<ul>
<li>
<p>Create a security credentials in the <code>bootstrapper.cluster-api-provider-aws.sigs.k8s.io</code> IAM user that is created by cloud-formation stack and copy the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRETS_ACCESS_KEY</code>.
(Or use admin user credentials instead)</p>
</li>
<li>
<p>Set AWS_B64ENCODED_CREDENTIALS environment variable</p>
<pre><code class="language-bash">export AWS_ACCESS_KEY_ID=AKIATEST
export AWS_SECRET_ACCESS_KEY=TESTTEST
export AWS_REGION=eu-west-1
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)
</code></pre>
</li>
</ul>
<h2><a class="header" href="#running-local-management-cluster-for-development" id="running-local-management-cluster-for-development">Running local management cluster for development</a></h2>
<p>Before the next steps, make sure <a href="development/development.html#initial-setup-for-development-environment">initial setup for development environment</a> steps are complete.</p>
<p>There are two ways to build aws manager from local cluster-api-provider-aws source and run it in local kind cluster:</p>
<h3><a class="header" href="#option-1-setting-up-development-environment-with-tilt" id="option-1-setting-up-development-environment-with-tilt">Option 1: Setting up Development Environment with Tilt</a></h3>
<p><a href="https://tilt.dev">Tilt</a> is a tool for quickly building, pushing, and reloading Docker containers as part of a Kubernetes deployment.
Many of the Cluster API engineers use it for quick iteration. Please see our <a href="development/../development/tilt-setup.html">Tilt instructions</a> to get started.</p>
<h3><a class="header" href="#option-2-the-old-fashioned-way" id="option-2-the-old-fashioned-way">Option 2: The Old-fashioned way</a></h3>
<p>Running cluster-api and cluster-api-provider-aws controllers in a kind cluster:</p>
<ol>
<li>Create a local kind cluster
<ul>
<li><code>kind create cluster</code></li>
</ul>
</li>
<li>Install core cluster-api controllers (the version must match the cluster-api version in <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/master/go.mod">go.mod</a>)
<ul>
<li><code>clusterctl init --core cluster-api:v0.3.16 --bootstrap kubeadm:v0.3.16 --control-plane kubeadm:v0.3.16</code></li>
</ul>
</li>
<li>Build cluster-api-provider-aws docker images
<ul>
<li><code>make e2e-image</code></li>
</ul>
</li>
<li>Release manifests under <code>./out</code> directory
<ul>
<li><code>RELEASE_TAG=&quot;e2e&quot; make release-manifests</code></li>
</ul>
</li>
<li>Apply the manifests
<ul>
<li><code>kubectl apply -f ./out/infrastructure.yaml</code></li>
</ul>
</li>
</ol>
<h1><a class="header" href="#developing-cluster-api-provider-aws--with-tilt" id="developing-cluster-api-provider-aws--with-tilt">Developing Cluster API Provider AWS  with Tilt</a></h1>
<p>This document describes how to use kind and <a href="https://tilt.dev">Tilt</a> for a simplified workflow that offers easy deployments and rapid iterative builds.
Before the next steps, make sure <a href="development/./development.html">initial setup for development environment</a> steps are complete.</p>
<p>Also, visit the <a href="https://cluster-api.sigs.k8s.io/developer/tilt.html">Cluster API documentation on Tilt</a> for more information on how to set up your development environment.</p>
<h2><a class="header" href="#create-a-kind-cluster" id="create-a-kind-cluster">Create a kind cluster</a></h2>
<p>First, make sure you have a kind cluster and that your <code>KUBECONFIG</code> is set up correctly:</p>
<pre><code class="language-bash">kind create cluster --name=capi-test
</code></pre>
<p>This local cluster will be running all the cluster api controllers and become the management cluster which then can be used to spin up workload clusters on AWS.</p>
<h2><a class="header" href="#get-the-source-1" id="get-the-source-1">Get the source</a></h2>
<p>Get the source for core cluster-api for development with Tilt along with cluster-api-provider-aws.</p>
<pre><code class="language-bash">cd &quot;$(go env GOPATH)&quot;/src
mkdir sigs.k8s.io
cd sigs.k8s.io/
git clone git@github.com:kubernetes-sigs/cluster-api.git
cd cluster-api
git fetch upstream
</code></pre>
<h2><a class="header" href="#create-a-tilt-settingsjson-file" id="create-a-tilt-settingsjson-file">Create a tilt-settings.json file</a></h2>
<p>Next, create a <code>tilt-settings.json</code> file and place it in your local copy of <code>cluster-api</code>. Here is an example:</p>
<p><strong>Example <code>tilt-settings.json</code> for CAPA clusters:</strong></p>
<pre><code class="language-json">{
  &quot;enable_providers&quot;: [
    &quot;kubeadm-bootstrap&quot;,
    &quot;kubeadm-control-plane&quot;,
    &quot;aws&quot;
  ],
  &quot;default_registry&quot;: &quot;gcr.io/your-project-name-here&quot;,
  &quot;provider_repos&quot;: [
    &quot;/Users/username/go/src/sigs.k8s.io/cluster-api-provider-aws/v2&quot;
  ],
  &quot;kustomize_substitutions&quot;: {
    &quot;EXP_CLUSTER_RESOURCE_SET&quot;: &quot;true&quot;,
    &quot;EXP_MACHINE_POOL&quot;: &quot;true&quot;,
    &quot;EXP_MACHINE_POOL_MACHINES&quot;: &quot;true&quot;,
    &quot;EVENT_BRIDGE_INSTANCE_STATE&quot;: &quot;true&quot;,
    &quot;AWS_B64ENCODED_CREDENTIALS&quot;: &quot;W2RlZmFZSZnRg==&quot;,
    &quot;EXP_EKS_FARGATE&quot;: &quot;false&quot;,
    &quot;CAPA_EKS_IAM&quot;: &quot;false&quot;,
    &quot;CAPA_EKS_ADD_ROLES&quot;: &quot;false&quot;,
    &quot;EXP_BOOTSTRAP_FORMAT_IGNITION&quot;: &quot;true&quot;
  },
  &quot;extra_args&quot;: {
    &quot;aws&quot;: [&quot;--v=2&quot;]
  }
}
</code></pre>
<p><strong>Example <code>tilt-settings.json</code> for EKS managed clusters prior to CAPA v0.7.0:</strong></p>
<pre><code class="language-json">{
    &quot;default_registry&quot;: &quot;gcr.io/your-project-name-here&quot;,
    &quot;provider_repos&quot;: [&quot;../cluster-api-provider-aws&quot;],
    &quot;enable_providers&quot;: [&quot;eks-bootstrap&quot;, &quot;eks-controlplane&quot;, &quot;kubeadm-bootstrap&quot;, &quot;kubeadm-control-plane&quot;, &quot;aws&quot;],
    &quot;kustomize_substitutions&quot;: {
        &quot;AWS_B64ENCODED_CREDENTIALS&quot;: &quot;W2RlZmFZSZnRg==&quot;,
        &quot;EXP_EKS&quot;: &quot;true&quot;,
        &quot;EXP_EKS_IAM&quot;: &quot;true&quot;,
        &quot;EXP_MACHINE_POOL&quot;: &quot;true&quot;
    },
    &quot;extra_args&quot;: {
        &quot;aws&quot;: [&quot;--v=2&quot;],
        &quot;eks-bootstrap&quot;: [&quot;--v=2&quot;],
        &quot;eks-controlplane&quot;: [&quot;--v=2&quot;]
    }
  }
</code></pre>
<h3><a class="header" href="#debugging" id="debugging">Debugging</a></h3>
<p>If you would like to debug CAPA (or core CAPI / another provider) you can run the provider with delve. This will then allow you to attach to delve and debug.</p>
<p>To do this you need to use the <strong>debug</strong> configuration in <strong>tilt-settings.json</strong>. Full details of the options can be seen <a href="https://cluster-api.sigs.k8s.io/developer/tilt.html">here</a>.</p>
<p>An example <strong>tilt-settings.json</strong>:</p>
<pre><code class="language-json">{
  &quot;enable_providers&quot;: [
    &quot;kubeadm-bootstrap&quot;,
    &quot;kubeadm-control-plane&quot;,
    &quot;aws&quot;
  ],
  &quot;default_registry&quot;: &quot;gcr.io/your-project-name-here&quot;,
  &quot;provider_repos&quot;: [
    &quot;/Users/username/go/src/sigs.k8s.io/cluster-api-provider-aws/v2&quot;
  ],
  &quot;kustomize_substitutions&quot;: {
    &quot;EXP_CLUSTER_RESOURCE_SET&quot;: &quot;true&quot;,
    &quot;EXP_MACHINE_POOL&quot;: &quot;true&quot;,
    &quot;EVENT_BRIDGE_INSTANCE_STATE&quot;: &quot;true&quot;,
    &quot;AWS_B64ENCODED_CREDENTIALS&quot;: &quot;W2RlZmFZSZnRg==&quot;,
    &quot;EXP_EKS_FARGATE&quot;: &quot;false&quot;,
    &quot;CAPA_EKS_IAM&quot;: &quot;false&quot;,
    &quot;CAPA_EKS_ADD_ROLES&quot;: &quot;false&quot;
  },
  &quot;extra_args&quot;: {
    &quot;aws&quot;: [&quot;--v=2&quot;]
  }
  &quot;debug&quot;: {
    &quot;aws&quot;: {
      &quot;continue&quot;: true,
      &quot;port&quot;: 30000
    }
  }
}
</code></pre>
<p>Once you have run tilt (see section below) you will be able to connect to the running instance of delve.</p>
<p>For vscode, you can use the a launch configuration like this:</p>
<pre><code class="language-json">    {
        &quot;name&quot;: &quot;Connect to CAPA&quot;,
        &quot;type&quot;: &quot;go&quot;,
        &quot;request&quot;: &quot;attach&quot;,
        &quot;mode&quot;: &quot;remote&quot;,
        &quot;remotePath&quot;: &quot;&quot;,
        &quot;port&quot;: 30000,
        &quot;host&quot;: &quot;127.0.0.1&quot;,
        &quot;showLog&quot;: true,
        &quot;trace&quot;: &quot;log&quot;,
        &quot;logOutput&quot;: &quot;rpc&quot;
    }
</code></pre>
<p>For GoLand/IntelliJ add a new run configuration following <a href="https://www.jetbrains.com/help/go/attach-to-running-go-processes-with-debugger.html#step-3-create-the-remote-run-debug-configuration-on-the-client-computer">these instructions</a>.</p>
<p>Or you could use delve directly from the CLI using a command similar to this:</p>
<pre><code class="language-bash">dlv-dap connect 127.0.0.1:3000
</code></pre>
<h2><a class="header" href="#run-tilt" id="run-tilt">Run Tilt!</a></h2>
<p>To launch your development environment, run:</p>
<pre><code class="language-bash">tilt up
</code></pre>
<p>kind cluster becomes a management cluster after this point, check the pods running on the kind cluster <code>kubectl get pods -A</code>.</p>
<h2><a class="header" href="#create-workload-clusters" id="create-workload-clusters">Create workload clusters</a></h2>
<p>Set the following variables for both CAPA and EKS managed clusters:</p>
<pre><code class="language-bash">export AWS_SSH_KEY_NAME=&lt;sshkeypair&gt;
export KUBERNETES_VERSION=v1.20.2
export CLUSTER_NAME=capi-&lt;test-clustename&gt;
export CONTROL_PLANE_MACHINE_COUNT=1
export AWS_CONTROL_PLANE_MACHINE_TYPE=t3.large
export WORKER_MACHINE_COUNT=1
export AWS_NODE_MACHINE_TYPE=t3.large
</code></pre>
<p>Set the following variables for only EKS managed clusters:</p>
<pre><code class="language-bash">export AWS_EKS_ROLE_ARN=arn:aws:iam::&lt;accountid&gt;:role/aws-service-role/eks.amazonaws.com/AWSServiceRoleForAmazonEKS
export EKS_KUBERNETES_VERSION=v1.15
</code></pre>
<p><strong>Create CAPA managed workload cluster:</strong></p>
<pre><code class="language-bash">cat templates/cluster-template.yaml
cat templates/cluster-template.yaml | $HOME/go/bin/envsubst &gt; test-cluster.yaml
kubectl apply -f test-cluster.yaml
</code></pre>
<p><strong>Create EKS workload cluster:</strong></p>
<pre><code class="language-bash">cat templates/cluster-template-eks.yaml
cat templates/cluster-template-eks.yaml | $HOME/go/bin/envsubst &gt; test-cluster.yaml
kubectl apply -f test-cluster.yaml
</code></pre>
<p>Check the tilt logs and wait for the clusters to be created.</p>
<h2><a class="header" href="#clean-up-1" id="clean-up-1">Clean up</a></h2>
<p>Before deleting the kind cluster, make sure you delete all the workload clusters.</p>
<pre><code class="language-bash">kubectl delete cluster &lt;clustername&gt;
tilt up (ctrl-c)
kind delete cluster
</code></pre>
<h2><a class="header" href="#troubleshooting-4" id="troubleshooting-4">Troubleshooting</a></h2>
<ul>
<li>Make sure you have at least three available spaces EIP and NAT Gateways to be created</li>
<li>If your git starts throwing this error</li>
</ul>
<pre><code class="language-bash">flag provided but not defined: -variables
Usage: envsubst [options...] &lt;input&gt;
</code></pre>
<p>you might need to reinstall the system <code>envsubst</code></p>
<pre><code class="language-bash">brew install gettetxt
# or
brew reinstall gettext
</code></pre>
<p>Make sure you specify which <code>envsubst</code> you are using</p>
<h1><a class="header" href="#developing-e2e-tests" id="developing-e2e-tests">Developing E2E tests</a></h1>
<p>Visit the <a href="https://cluster-api.sigs.k8s.io/developer/core/e2e">Cluster API documentation on E2E</a> for information on how to develop and run e2e tests. </p>
<h2><a class="header" href="#set-up" id="set-up">Set up</a></h2>
<p>It’s recommended to create a separate AWS account to run E2E tests. This ensures it does not conflict with
your other cluster API environment.</p>
<h2><a class="header" href="#running-from-cli" id="running-from-cli">Running from CLI</a></h2>
<p>e2e tests can be run using Makefile targets:</p>
<pre><code class="language-bash">$ make test-e2e
$ make test-e2e-eks
</code></pre>
<p>The following useful env variables can help to speed up the runs:</p>
<ul>
<li><code>E2E_ARGS=&quot;--skip-cloudformation-creation --skip-cloudformation-deletion&quot;</code> - in case the cloudformation stack is already properly set up, this ensures a quicker start and tear down.</li>
<li><code>GINKGO_FOCUS='\[PR-Blocking\]'</code> - only run a subset of tests</li>
<li><code>USE_EXISTING_CLUSTER</code> - use an existing management cluster (useful if you have a <a href="development/./tilt-setup.html">Tilt</a> setup)</li>
</ul>
<h2><a class="header" href="#running-in-ides" id="running-in-ides">Running in IDEs</a></h2>
<p>The following example assumes you run a management cluster locally (e.g. using <a href="development/./tilt-setup.html">Tilt</a>). </p>
<h3><a class="header" href="#intellijgoland" id="intellijgoland">IntelliJ/GoLand</a></h3>
<p>The following run configuration can be used:</p>
<pre><code class="language-xml">&lt;component name=&quot;ProjectRunConfigurationManager&quot;&gt;
  &lt;configuration default=&quot;false&quot; name=&quot;capa e2e: unmanaged PR-Blocking&quot; type=&quot;GoTestRunConfiguration&quot; factoryName=&quot;Go Test&quot;&gt;
    &lt;module name=&quot;cluster-api-provider-aws&quot; /&gt;
    &lt;working_directory value=&quot;$PROJECT_DIR$/test/e2e/suites/unmanaged&quot; /&gt;
    &lt;parameters value=&quot;-ginkgo.focus=&amp;quot;\[PR-Blocking\]&amp;quot; -ginkgo.v=true -artifacts-folder=$PROJECT_DIR$/_artifacts --data-folder=$PROJECT_DIR$/test/e2e/data -use-existing-cluster=true -config-path=$PROJECT_DIR$/test/e2e/data/e2e_conf.yaml&quot; /&gt;
    &lt;envs&gt;
      &lt;env name=&quot;AWS_REGION&quot; value=&quot;SET_AWS_REGION&quot; /&gt;
      &lt;env name=&quot;AWS_PROFILE&quot; value=&quot;IF_YOU_HAVE_MULTIPLE_PROFILES&quot; /&gt;
      &lt;env name=&quot;AWS_ACCESS_KEY_ID&quot; value=&quot;REPLACE_ACCESS_KEY&quot; /&gt;
      &lt;env name=&quot;AWS_SECRET_ACCESS_KEY&quot; value=&quot;2W2RlZmFZSZnRg==&quot; /&gt;
    &lt;/envs&gt;
    &lt;kind value=&quot;PACKAGE&quot; /&gt;
    &lt;package value=&quot;sigs.k8s.io/cluster-api-provider-aws/v2/test/e2e/suites/unmanaged&quot; /&gt;
    &lt;directory value=&quot;$PROJECT_DIR$&quot; /&gt;
    &lt;filePath value=&quot;$PROJECT_DIR$&quot; /&gt;
    &lt;framework value=&quot;gotest&quot; /&gt;
    &lt;pattern value=&quot;^\QTestE2E\E$&quot; /&gt;
    &lt;method v=&quot;2&quot; /&gt;
  &lt;/configuration&gt;
&lt;/component&gt;
</code></pre>
<h3><a class="header" href="#visual-studio-code" id="visual-studio-code">Visual Studio Code</a></h3>
<p>With the example above, you can configure a <a href="https://go.microsoft.com/fwlink/?linkid=830387">launch configuration for VSCode</a>. </p>
<h1><a class="header" href="#coding-conventions" id="coding-conventions">Coding Conventions</a></h1>
<p>Below is a collection of conventions, guidlines and general tips for writing code for this project.</p>
<h2><a class="header" href="#api-definitions" id="api-definitions">API Definitions</a></h2>
<h3><a class="header" href="#dont-expose-3rd-party-package-types" id="dont-expose-3rd-party-package-types">Don’t Expose 3rd Party Package Types</a></h3>
<p>When adding new or modifying API types don’t expose 3rd party package types/enums via the CAPA API definitions. Instead create our own versions and where provide mapping functions.</p>
<p>For example:</p>
<ul>
<li>AWS SDK <a href="https://docs.aws.amazon.com/sdk-for-go/api/service/ec2/">InstanceState</a></li>
<li>CAPA <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/api/v1beta1/types.go#L560:L581">InstanceState</a></li>
</ul>
<h3><a class="header" href="#dont-use-struct-pointer-slices" id="dont-use-struct-pointer-slices">Don’t use struct pointer slices</a></h3>
<p>When adding new fields to an API type don’t use a slice of struct pointers. This can cause issues with the code generator for the conversion functions. Instead use struct slices.</p>
<p>For example:</p>
<p>Instead of this</p>
<pre><code class="language-go">	// Configuration options for the non root storage volumes.
	// +optional
	NonRootVolumes []*Volume `json:&quot;nonRootVolumes,omitempty&quot;`
</code></pre>
<p>use</p>
<pre><code class="language-go">	// Configuration options for the non root storage volumes.
	// +optional
	NonRootVolumes []Volume `json:&quot;nonRootVolumes,omitempty&quot;`
</code></pre>
<p>And then within the code you can check the length or range over the slice.</p>
<h2><a class="header" href="#tests" id="tests">Tests</a></h2>
<p>There are three types of tests written for CAPA controllers in this repo:</p>
<ul>
<li>Unit tests</li>
<li>Integration tests</li>
<li>E2E tests</li>
</ul>
<p>In these tests, we use <a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/client/fake">fakeclient</a>, <a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest">envtest</a> and <a href="https://pkg.go.dev/github.com/golang/mock/gomock">gomock</a> libraries based on the requirements of individual test types.</p>
<p>If any new unit, integration or E2E tests has to be added in this repo,we should follow the below conventions.</p>
<h3><a class="header" href="#unit-tests" id="unit-tests">Unit tests</a></h3>
<p>These tests are meant to verify the functions inside the same controller file where we perform sanity checks, functionality checks etc.
These tests go into the file with suffix *_unit_test.go.</p>
<h3><a class="header" href="#integration-tests" id="integration-tests">Integration tests</a></h3>
<p>These tests are meant to verify the overall flow of the reconcile calls in the controllers to test the flows for all the services/subcomponents of controllers as a whole.
These tests go into the file with suffix *_test.go.</p>
<h3><a class="header" href="#e2e-tests" id="e2e-tests">E2E tests</a></h3>
<p>These tests are meant to verify the proper functioning of a CAPA cluster in an environment that resembles a real production environment. For details, refer <a href="https://cluster-api-aws.sigs.k8s.io/development/e2e.html">here</a>.</p>
<h1><a class="header" href="#nightly-builds" id="nightly-builds">Nightly Builds</a></h1>
<p>Nightly builds are regular automated builds of the CAPA source code that occur every night.</p>
<p>These builds are generated directly from the latest commit of source code on the main branch.</p>
<p>Nightly builds serve several purposes:</p>
<ul>
<li><strong>Early Testing</strong>: They provide an opportunity for developers and testers to access the most recent changes in the codebase and identify any issues or bugs that may have been introduced.</li>
<li><strong>Feedback Loop</strong>: They facilitate a rapid feedback loop, enabling developers to receive feedback on their changes quickly, allowing them to iterate and improve the code more efficiently.</li>
<li><strong>Preview of New Features</strong>: Users and can get a preview of upcoming features or changes by testing nightly builds, although these builds may not always be stable enough for production use.</li>
</ul>
<p>Overall, nightly builds play a crucial role in software development by promoting user testing, early bug detection, and rapid iteration.</p>
<p>CAPA Nightly build jobs run in Prow. For details on how this is configured you can check the <a href="development/../topics/reference/jobs.html#periodics">Periodics Jobs section</a>.</p>
<h2><a class="header" href="#usage-1" id="usage-1">Usage</a></h2>
<p>To try a nightly build, you can download the latest built nightly CAPA manifests, you can find the available ones by executing the following command:</p>
<pre><code class="language-bash">curl -sL -H 'Accept: application/json' &quot;https://storage.googleapis.com/storage/v1/b/k8s-staging-cluster-api-aws/o&quot; | jq -r '.items | map(select(.name | startswith(&quot;components/nightly_main&quot;))) | .[] | [.timeCreated,.mediaLink] | @tsv'
</code></pre>
<p>The output should look something like this:</p>
<pre><code>2024-05-03T08:03:09.087Z        https://storage.googleapis.com/download/storage/v1/b/k8s-staging-cluster-api-aws/o/components%2Fnightly_main_2024050x?generation=1714723389033961&amp;alt=media
2024-05-04T08:02:52.517Z        https://storage.googleapis.com/download/storage/v1/b/k8s-staging-cluster-api-aws/o/components%2Fnightly_main_2024050y?generation=1714809772486582&amp;alt=media
2024-05-05T08:02:45.840Z        https://storage.googleapis.com/download/storage/v1/b/k8s-staging-cluster-api-aws/o/components%2Fnightly_main_2024050z?generation=1714896165803510&amp;alt=media
</code></pre>
<p>Now visit the link for the manifest you want to download. This will automatically download the manifest for you.</p>
<p>Once downloaded you can apply the manifest directly to your testing CAPI management cluster/namespace (e.g. with kubectl), as the downloaded CAPA manifest
will already contain the correct, corresponding CAPA nightly image reference.</p>
<h1><a class="header" href="#publish-amis" id="publish-amis">Publish AMIs</a></h1>
<p>Publishing new AMIs is done via manually invoking a GitHub Actions workflow. </p>
<blockquote>
<p>NOTE: the plan is to ultimately fully automate the process in the future (see <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/1982">this issue</a> for progress).</p>
</blockquote>
<blockquote>
<p>NOTE: there are some issues with the RHEL based images at present.</p>
</blockquote>
<h2><a class="header" href="#get-build-inputs" id="get-build-inputs">Get build inputs</a></h2>
<p>For a new Kubernetes version that you want to build an AMI for you will need to determine the following values:</p>
<table><thead><tr><th>Input</th><th>Description</th></tr></thead><tbody>
<tr><td>kubernetes_semver</td><td>The semver version of k8s you want to build an AMI for. In format vMAJOR.MINOR.PATCH.</td></tr>
<tr><td>kubernetes_series</td><td>The release series for the Kubernetes version. In format vMAJOR.MINOR.</td></tr>
<tr><td>kubernetes_deb_version</td><td>The version of the debian package for the release.</td></tr>
<tr><td>kubernetes_rpm_version</td><td>The version of the rpm package for the release</td></tr>
<tr><td>kubernetes_cni_semver</td><td>The version of CNI to include. It needs to match the k8s release.</td></tr>
<tr><td>kubernetes_cni_deb_version</td><td>The version of the debian package for the CNI release to use</td></tr>
<tr><td>crictl_version</td><td>The vesion of the cri-tools package to install into the AMI</td></tr>
</tbody></table>
<p>You can determine these values directly or by looking at the publish debian apt repositories for the k8s release.</p>
<p>A quick way to get these values is by using the find-ami-publishing-inputs script.
The script lives under <code>/scripts</code> in the root directory of CAPA, and can be invoked like so (supposing you want to publish k8s v1.32 AMIs):</p>
<pre><code>./scripts/find-ami-publishing-inputs.sh v1.32
</code></pre>
<h2><a class="header" href="#build" id="build">Build</a></h2>
<h3><a class="header" href="#using-github-actions-workflow" id="using-github-actions-workflow">Using GitHub Actions Workflow</a></h3>
<p>To build the AMI using GitHub actions you must have write access to the CAPA repository (i.e. be a maintainer or part of release team).</p>
<p>To build the new version:</p>
<ol>
<li>Got to the GitHub Action</li>
<li>Click the <strong>Start Workflow</strong> button</li>
<li>Fill in the details of the build</li>
<li>Click <strong>Run</strong></li>
</ol>
<h3><a class="header" href="#manually" id="manually">Manually</a></h3>
<blockquote>
<p>**WARNING: the manual process should only be followed in exceptional circumstances.</p>
</blockquote>
<p>To build manually you must have admin access to the CNCF AWS account used for the AMIs.</p>
<p>The steps to build manually are:</p>
<ol>
<li>Clone <a href="https://github.com/kubernetes-sigs/image-builder">image-builder</a></li>
<li>Open a terminal</li>
<li>Set the AWS environment variables for the CAPA AMI account</li>
<li>Change directory into <code>images/capi</code></li>
<li>Create a new file called <code>vars.json</code> with the following content (substituing the values with the build inputs):</li>
</ol>
<pre><code class="language-json">{
    &quot;kubernetes_rpm_version&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;kubernetes_semver&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;kubernetes_series&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;kubernetes_deb_version&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;kubernetes_cni_semver&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;kubernetes_cni_deb_version&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;,
    &quot;crictl_version&quot;: &quot;&lt;INSERT_INPUT_VALUE&gt;&quot;
}
</code></pre>
<ol start="6">
<li>Install dependencies by running:</li>
</ol>
<pre><code class="language-shell">make deps-ami
</code></pre>
<ol start="7">
<li>Build the AMIs using:</li>
</ol>
<pre><code class="language-shell">PACKER_VAR_FILES=vars.json make build-ami-ubuntu-2204
PACKER_VAR_FILES=vars.json make build-ami-ubuntu-2404
PACKER_VAR_FILES=vars.json make build-ami-flatcar
PACKER_VAR_FILES=vars.json make build-ami-rhel-8
</code></pre>
<h2><a class="header" href="#additional-information-1" id="additional-information-1">Additional Information</a></h2>
<ul>
<li>The AMIs are hosted in a CNCF owned AWS account (819546954734).</li>
<li>The AWS resources that are needed to support the GitHub Actions workflow are created via terraform. Source is <a href="https://github.com/kubernetes/k8s.io/tree/main/infra/aws/terraform/cncf-k8s-infra-aws-capa-ami">here</a>.</li>
<li>OIDC and IAM Roles are used to grant access via short lived credentials to the GitHub Action workflow instance when it runs.</li>
</ul>
<p>Packages:</p>
<ul>
<li>
<a href="crd/index.html#ami.aws.infrastructure.cluster.x-k8s.io%2fv1beta1">ami.aws.infrastructure.cluster.x-k8s.io/v1beta1</a>
</li>
<li>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io%2fv1alpha1">bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1</a>
</li>
<li>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io%2fv1beta1">bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1</a>
</li>
<li>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io%2fv1beta1">bootstrap.cluster.x-k8s.io/v1beta1</a>
</li>
<li>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io%2fv1beta2">bootstrap.cluster.x-k8s.io/v1beta2</a>
</li>
<li>
<a href="crd/index.html#controlplane.cluster.x-k8s.io%2fv1beta1">controlplane.cluster.x-k8s.io/v1beta1</a>
</li>
<li>
<a href="crd/index.html#controlplane.cluster.x-k8s.io%2fv1beta2">controlplane.cluster.x-k8s.io/v1beta2</a>
</li>
<li>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io%2fv1beta1">infrastructure.cluster.x-k8s.io/v1beta1</a>
</li>
<li>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io%2fv1beta2">infrastructure.cluster.x-k8s.io/v1beta2</a>
</li>
</ul>
<h2 id="ami.aws.infrastructure.cluster.x-k8s.io/v1beta1">ami.aws.infrastructure.cluster.x-k8s.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains API Schema definitions for the AMI v1beta1 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="ami.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSAMI">AWSAMI
</h3>
<p>
<p>AWSAMI defines an AMI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#ami.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSAMISpec">
AWSAMISpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>os</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>imageID</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>kubernetesVersion</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="ami.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSAMISpec">AWSAMISpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#ami.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSAMI">AWSAMI</a>)
</p>
<p>
<p>AWSAMISpec defines an AMI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>os</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>imageID</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>kubernetesVersion</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1">bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1</h2>
<p>
<p>Package v1alpha1 contains API Schema definitions for the bootstrap v1alpha1 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfiguration">AWSIAMConfiguration
</h3>
<p>
<p>AWSIAMConfiguration controls the creation of AWS Identity and Access Management (IAM) resources for use
by Kubernetes clusters and Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">
AWSIAMConfigurationSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>namePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NamePrefix will be prepended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to &ldquo;&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>nameSuffix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NameSuffix will be appended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to
&ldquo;.cluster-api-provider-aws.sigs.k8s.io&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlane</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ControlPlane">
ControlPlane
</a>
</em>
</td>
<td>
<p>ControlPlane controls the configuration of the AWS IAM role for a Kubernetes cluster&rsquo;s control plane nodes.</p>
</td>
</tr>
<tr>
<td>
<code>clusterAPIControllers</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ClusterAPIControllers">
ClusterAPIControllers
</a>
</em>
</td>
<td>
<p>ClusterAPIControllers controls the configuration of an IAM role and policy specifically for Kubernetes Cluster API Provider AWS.</p>
</td>
</tr>
<tr>
<td>
<code>nodes</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.Nodes">
Nodes
</a>
</em>
</td>
<td>
<p>Nodes controls the configuration of the AWS IAM role for all nodes in a Kubernetes cluster.</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapUser</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.BootstrapUser">
BootstrapUser
</a>
</em>
</td>
<td>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>StackName defines the name of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>Region controls which region the control-plane is created in if not specified on the command line or
via environment variables.</p>
</td>
</tr>
<tr>
<td>
<code>eks</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EKSConfig">
EKSConfig
</a>
</em>
</td>
<td>
<p>EKS controls the configuration related to EKS. Settings in here affect the control plane
and nodes roles</p>
</td>
</tr>
<tr>
<td>
<code>eventBridge</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EventBridgeConfig">
EventBridgeConfig
</a>
</em>
</td>
<td>
<p>EventBridge controls configuration for consuming EventBridge events</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretBackends</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">
[]SecretBackend
</a>
</em>
</td>
<td>
<p>SecureSecretsBackend, when set to parameter-store will create AWS Systems Manager
Parameter Storage policies. By default or with the value of secrets-manager,
will generate AWS Secrets Manager policies instead.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfiguration">AWSIAMConfiguration</a>)
</p>
<p>
<p>AWSIAMConfigurationSpec defines the specification of the AWSIAMConfiguration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>namePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NamePrefix will be prepended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to &ldquo;&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>nameSuffix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NameSuffix will be appended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to
&ldquo;.cluster-api-provider-aws.sigs.k8s.io&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlane</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ControlPlane">
ControlPlane
</a>
</em>
</td>
<td>
<p>ControlPlane controls the configuration of the AWS IAM role for a Kubernetes cluster&rsquo;s control plane nodes.</p>
</td>
</tr>
<tr>
<td>
<code>clusterAPIControllers</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ClusterAPIControllers">
ClusterAPIControllers
</a>
</em>
</td>
<td>
<p>ClusterAPIControllers controls the configuration of an IAM role and policy specifically for Kubernetes Cluster API Provider AWS.</p>
</td>
</tr>
<tr>
<td>
<code>nodes</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.Nodes">
Nodes
</a>
</em>
</td>
<td>
<p>Nodes controls the configuration of the AWS IAM role for all nodes in a Kubernetes cluster.</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapUser</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.BootstrapUser">
BootstrapUser
</a>
</em>
</td>
<td>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>StackName defines the name of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>Region controls which region the control-plane is created in if not specified on the command line or
via environment variables.</p>
</td>
</tr>
<tr>
<td>
<code>eks</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EKSConfig">
EKSConfig
</a>
</em>
</td>
<td>
<p>EKS controls the configuration related to EKS. Settings in here affect the control plane
and nodes roles</p>
</td>
</tr>
<tr>
<td>
<code>eventBridge</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EventBridgeConfig">
EventBridgeConfig
</a>
</em>
</td>
<td>
<p>EventBridge controls configuration for consuming EventBridge events</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretBackends</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">
[]SecretBackend
</a>
</em>
</td>
<td>
<p>SecureSecretsBackend, when set to parameter-store will create AWS Systems Manager
Parameter Storage policies. By default or with the value of secrets-manager,
will generate AWS Secrets Manager policies instead.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">AWSIAMRoleSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ClusterAPIControllers">ClusterAPIControllers</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ControlPlane">ControlPlane</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EKSConfig">EKSConfig</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.Nodes">Nodes</a>)
</p>
<p>
<p>AWSIAMRoleSpec defines common configuration for AWS IAM roles created by
Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable if set to true will not create the AWS IAM role. Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>extraPolicyAttachments</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraPolicyAttachments is a list of additional policies to be attached to the IAM role.</p>
</td>
</tr>
<tr>
<td>
<code>extraStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>ExtraStatements are additional IAM statements to be included inline for the role.</p>
</td>
</tr>
<tr>
<td>
<code>trustStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>TrustStatements is an IAM PolicyDocument defining what identities are allowed to assume this role.
See &ldquo;sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/iam/v1beta1&rdquo; for more documentation.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags to be applied to the AWS IAM role.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.BootstrapUser">BootstrapUser
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Enable controls whether or not a bootstrap AWS IAM user will be created.
This can be used to scope down the initial credentials used to bootstrap the
cluster.
Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>userName</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserName controls the username of the bootstrap user. Defaults to
&ldquo;bootstrapper.cluster-api-provider-aws.sigs.k8s.io&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>groupName</code><br/>
<em>
string
</em>
</td>
<td>
<p>GroupName controls the group the user will belong to. Defaults to
&ldquo;bootstrapper.cluster-api-provider-aws.sigs.k8s.io&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>extraPolicyAttachments</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraPolicyAttachments is a list of additional policies to be attached to the IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>extraGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraGroups is a list of groups to add this user to.</p>
</td>
</tr>
<tr>
<td>
<code>extraStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>ExtraStatements are additional AWS IAM policy document statements to be included inline for the user.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags to be applied to the AWS IAM user.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ClusterAPIControllers">ClusterAPIControllers
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>ClusterAPIControllers controls the configuration of the AWS IAM role for
the Kubernetes Cluster API Provider AWS controller.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>allowedEC2InstanceProfiles</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AllowedEC2InstanceProfiles controls which EC2 roles are allowed to be
consumed by Cluster API when creating an ec2 instance. Defaults to
*.<suffix>, where suffix is defaulted to .cluster-api-provider-aws.sigs.k8s.io</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.ControlPlane">ControlPlane
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>ControlPlane controls the configuration of the AWS IAM role for
the control plane of provisioned Kubernetes clusters.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>disableClusterAPIControllerPolicyAttachment</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableClusterAPIControllerPolicyAttachment, if set to true, will not attach the AWS IAM policy for Cluster
API Provider AWS to the control plane role. Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>disableCloudProviderPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableCloudProviderPolicy if set to true, will not generate and attach the AWS IAM policy for the AWS Cloud Provider.</p>
</td>
</tr>
<tr>
<td>
<code>enableCSIPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EnableCSIPolicy if set to true, will generate and attach the AWS IAM policy for the EBS CSI Driver.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EKSConfig">EKSConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>EKSConfig represents the EKS related configuration config.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable controls whether EKS-related permissions are granted</p>
</td>
</tr>
<tr>
<td>
<code>iamRoleCreation</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AllowIAMRoleCreation controls whether the EKS controllers have permissions for creating IAM
roles per cluster</p>
</td>
</tr>
<tr>
<td>
<code>enableUserEKSConsolePolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EnableUserEKSConsolePolicy controls the creation of the policy to view EKS nodes and workloads.</p>
</td>
</tr>
<tr>
<td>
<code>defaultControlPlaneRole</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>DefaultControlPlaneRole controls the configuration of the AWS IAM role for
the EKS control plane. This is the default role that will be used if
no role is included in the spec and automatic creation of the role
isn&rsquo;t enabled</p>
</td>
</tr>
<tr>
<td>
<code>managedMachinePool</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>ManagedMachinePool controls the configuration of the AWS IAM role for
used by EKS managed machine pools.</p>
</td>
</tr>
<tr>
<td>
<code>fargate</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>Fargate controls the configuration of the AWS IAM role for
used by EKS managed machine pools.</p>
</td>
</tr>
<tr>
<td>
<code>kmsAliasPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>KMSAliasPrefix is prefix to use to restrict permission to KMS keys to only those that have an alias
name that is prefixed by this.
Defaults to cluster-api-provider-aws-*</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.EventBridgeConfig">EventBridgeConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>EventBridgeConfig represents configuration for enabling experimental feature to consume
EventBridge EC2 events.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Enable controls whether permissions are granted to consume EC2 events</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.Nodes">Nodes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>Nodes controls the configuration of the AWS IAM role for worker nodes
in a cluster created by Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>disableCloudProviderPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableCloudProviderPolicy if set to true, will not generate and attach the policy for the AWS Cloud Provider.
Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>ec2ContainerRegistryReadOnly</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EC2ContainerRegistryReadOnly controls whether the node has read-only access to the
EC2 container registry</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1">bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains API Schema definitions for the bootstrap v1beta1 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfiguration">AWSIAMConfiguration
</h3>
<p>
<p>AWSIAMConfiguration controls the creation of AWS Identity and Access Management (IAM) resources for use
by Kubernetes clusters and Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">
AWSIAMConfigurationSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>namePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NamePrefix will be prepended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to &ldquo;&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>nameSuffix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NameSuffix will be appended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to
&ldquo;.cluster-api-provider-aws.sigs.k8s.io&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlane</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ControlPlane">
ControlPlane
</a>
</em>
</td>
<td>
<p>ControlPlane controls the configuration of the AWS IAM role for a Kubernetes cluster&rsquo;s control plane nodes.</p>
</td>
</tr>
<tr>
<td>
<code>clusterAPIControllers</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ClusterAPIControllers">
ClusterAPIControllers
</a>
</em>
</td>
<td>
<p>ClusterAPIControllers controls the configuration of an IAM role and policy specifically for Kubernetes Cluster API Provider AWS.</p>
</td>
</tr>
<tr>
<td>
<code>nodes</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.Nodes">
Nodes
</a>
</em>
</td>
<td>
<p>Nodes controls the configuration of the AWS IAM role for all nodes in a Kubernetes cluster.</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapUser</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.BootstrapUser">
BootstrapUser
</a>
</em>
</td>
<td>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>StackName defines the name of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>stackTags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StackTags defines the tags of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>Region controls which region the control-plane is created in if not specified on the command line or
via environment variables.</p>
</td>
</tr>
<tr>
<td>
<code>eks</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EKSConfig">
EKSConfig
</a>
</em>
</td>
<td>
<p>EKS controls the configuration related to EKS. Settings in here affect the control plane
and nodes roles</p>
</td>
</tr>
<tr>
<td>
<code>eventBridge</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EventBridgeConfig">
EventBridgeConfig
</a>
</em>
</td>
<td>
<p>EventBridge controls configuration for consuming EventBridge events</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretBackends</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">
[]SecretBackend
</a>
</em>
</td>
<td>
<p>SecureSecretsBackend, when set to parameter-store will create AWS Systems Manager
Parameter Storage policies. By default or with the value of secrets-manager,
will generate AWS Secrets Manager policies instead.</p>
</td>
</tr>
<tr>
<td>
<code>s3Buckets</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.S3Buckets">
S3Buckets
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Buckets, when enabled, will add controller nodes permissions to
create S3 Buckets for workload clusters.
TODO: This field could be a pointer, but it seems it breaks setting default values?</p>
</td>
</tr>
<tr>
<td>
<code>allowAssumeRole</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AllowAssumeRole enables the sts:AssumeRole permission within the CAPA policies</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfiguration">AWSIAMConfiguration</a>)
</p>
<p>
<p>AWSIAMConfigurationSpec defines the specification of the AWSIAMConfiguration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>namePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NamePrefix will be prepended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to &ldquo;&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>nameSuffix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NameSuffix will be appended to every AWS IAM role, user and policy created by clusterawsadm. Defaults to
&ldquo;.cluster-api-provider-aws.sigs.k8s.io&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlane</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ControlPlane">
ControlPlane
</a>
</em>
</td>
<td>
<p>ControlPlane controls the configuration of the AWS IAM role for a Kubernetes cluster&rsquo;s control plane nodes.</p>
</td>
</tr>
<tr>
<td>
<code>clusterAPIControllers</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ClusterAPIControllers">
ClusterAPIControllers
</a>
</em>
</td>
<td>
<p>ClusterAPIControllers controls the configuration of an IAM role and policy specifically for Kubernetes Cluster API Provider AWS.</p>
</td>
</tr>
<tr>
<td>
<code>nodes</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.Nodes">
Nodes
</a>
</em>
</td>
<td>
<p>Nodes controls the configuration of the AWS IAM role for all nodes in a Kubernetes cluster.</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapUser</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.BootstrapUser">
BootstrapUser
</a>
</em>
</td>
<td>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>StackName defines the name of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>stackTags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>StackTags defines the tags of the AWS CloudFormation stack.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>Region controls which region the control-plane is created in if not specified on the command line or
via environment variables.</p>
</td>
</tr>
<tr>
<td>
<code>eks</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EKSConfig">
EKSConfig
</a>
</em>
</td>
<td>
<p>EKS controls the configuration related to EKS. Settings in here affect the control plane
and nodes roles</p>
</td>
</tr>
<tr>
<td>
<code>eventBridge</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EventBridgeConfig">
EventBridgeConfig
</a>
</em>
</td>
<td>
<p>EventBridge controls configuration for consuming EventBridge events</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretBackends</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">
[]SecretBackend
</a>
</em>
</td>
<td>
<p>SecureSecretsBackend, when set to parameter-store will create AWS Systems Manager
Parameter Storage policies. By default or with the value of secrets-manager,
will generate AWS Secrets Manager policies instead.</p>
</td>
</tr>
<tr>
<td>
<code>s3Buckets</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.S3Buckets">
S3Buckets
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Buckets, when enabled, will add controller nodes permissions to
create S3 Buckets for workload clusters.
TODO: This field could be a pointer, but it seems it breaks setting default values?</p>
</td>
</tr>
<tr>
<td>
<code>allowAssumeRole</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AllowAssumeRole enables the sts:AssumeRole permission within the CAPA policies</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">AWSIAMRoleSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ClusterAPIControllers">ClusterAPIControllers</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ControlPlane">ControlPlane</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EKSConfig">EKSConfig</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.Nodes">Nodes</a>)
</p>
<p>
<p>AWSIAMRoleSpec defines common configuration for AWS IAM roles created by
Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable if set to true will not create the AWS IAM role. Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>extraPolicyAttachments</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraPolicyAttachments is a list of additional policies to be attached to the IAM role.</p>
</td>
</tr>
<tr>
<td>
<code>extraStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>ExtraStatements are additional IAM statements to be included inline for the role.</p>
</td>
</tr>
<tr>
<td>
<code>path</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Path sets the path to the role.</p>
</td>
</tr>
<tr>
<td>
<code>permissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PermissionsBoundary sets the ARN of the managed policy that is used to set the permissions boundary for the role.</p>
</td>
</tr>
<tr>
<td>
<code>trustStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>TrustStatements is an IAM PolicyDocument defining what identities are allowed to assume this role.
See &ldquo;sigs.k8s.io/cluster-api-provider-aws/v2/cmd/clusterawsadm/api/iam/v1beta1&rdquo; for more documentation.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags to be applied to the AWS IAM role.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.BootstrapUser">BootstrapUser
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>BootstrapUser contains a list of elements that is specific
to the configuration and enablement of an IAM user.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Enable controls whether or not a bootstrap AWS IAM user will be created.
This can be used to scope down the initial credentials used to bootstrap the
cluster.
Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>userName</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserName controls the username of the bootstrap user. Defaults to
&ldquo;bootstrapper.cluster-api-provider-aws.sigs.k8s.io&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>groupName</code><br/>
<em>
string
</em>
</td>
<td>
<p>GroupName controls the group the user will belong to. Defaults to
&ldquo;bootstrapper.cluster-api-provider-aws.sigs.k8s.io&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>extraPolicyAttachments</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraPolicyAttachments is a list of additional policies to be attached to the IAM user.</p>
</td>
</tr>
<tr>
<td>
<code>extraGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ExtraGroups is a list of groups to add this user to.</p>
</td>
</tr>
<tr>
<td>
<code>extraStatements</code><br/>
<em>
[]Cluster API AWS iam/api/v1beta1.StatementEntry
</em>
</td>
<td>
<p>ExtraStatements are additional AWS IAM policy document statements to be included inline for the user.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags to be applied to the AWS IAM user.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ClusterAPIControllers">ClusterAPIControllers
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>ClusterAPIControllers controls the configuration of the AWS IAM role for
the Kubernetes Cluster API Provider AWS controller.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>allowedEC2InstanceProfiles</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AllowedEC2InstanceProfiles controls which EC2 roles are allowed to be
consumed by Cluster API when creating an ec2 instance. Defaults to
*.<suffix>, where suffix is defaulted to .cluster-api-provider-aws.sigs.k8s.io</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.ControlPlane">ControlPlane
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>ControlPlane controls the configuration of the AWS IAM role for
the control plane of provisioned Kubernetes clusters.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>disableClusterAPIControllerPolicyAttachment</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableClusterAPIControllerPolicyAttachment, if set to true, will not attach the AWS IAM policy for Cluster
API Provider AWS to the control plane role. Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>disableCloudProviderPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableCloudProviderPolicy if set to true, will not generate and attach the AWS IAM policy for the AWS Cloud Provider.</p>
</td>
</tr>
<tr>
<td>
<code>enableCSIPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EnableCSIPolicy if set to true, will generate and attach the AWS IAM policy for the EBS CSI Driver.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EKSConfig">EKSConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>EKSConfig represents the EKS related configuration config.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable controls whether EKS-related permissions are granted</p>
</td>
</tr>
<tr>
<td>
<code>iamRoleCreation</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AllowIAMRoleCreation controls whether the EKS controllers have permissions for creating IAM
roles per cluster</p>
</td>
</tr>
<tr>
<td>
<code>enableUserEKSConsolePolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EnableUserEKSConsolePolicy controls the creation of the policy to view EKS nodes and workloads.</p>
</td>
</tr>
<tr>
<td>
<code>defaultControlPlaneRole</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>DefaultControlPlaneRole controls the configuration of the AWS IAM role for
the EKS control plane. This is the default role that will be used if
no role is included in the spec and automatic creation of the role
isn&rsquo;t enabled</p>
</td>
</tr>
<tr>
<td>
<code>managedMachinePool</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>ManagedMachinePool controls the configuration of the AWS IAM role for
used by EKS managed machine pools.</p>
</td>
</tr>
<tr>
<td>
<code>fargate</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>Fargate controls the configuration of the AWS IAM role for
used by EKS managed machine pools.</p>
</td>
</tr>
<tr>
<td>
<code>kmsAliasPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>KMSAliasPrefix is prefix to use to restrict permission to KMS keys to only those that have an alias
name that is prefixed by this.
Defaults to cluster-api-provider-aws-*</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.EventBridgeConfig">EventBridgeConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>EventBridgeConfig represents configuration for enabling experimental feature to consume
EventBridge EC2 events.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Enable controls whether permissions are granted to consume EC2 events</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.Nodes">Nodes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>Nodes controls the configuration of the AWS IAM role for worker nodes
in a cluster created by Kubernetes Cluster API Provider AWS.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSIAMRoleSpec</code><br/>
<em>
<a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">
AWSIAMRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSIAMRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>disableCloudProviderPolicy</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableCloudProviderPolicy if set to true, will not generate and attach the policy for the AWS Cloud Provider.
Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>ec2ContainerRegistryReadOnly</code><br/>
<em>
bool
</em>
</td>
<td>
<p>EC2ContainerRegistryReadOnly controls whether the node has read-only access to the
EC2 container registry</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.S3Buckets">S3Buckets
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>S3Buckets controls the configuration of the AWS IAM role for S3 buckets
which can be created for storing bootstrap data for nodes requiring it.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Enable controls whether permissions are granted to manage S3 buckets.</p>
</td>
</tr>
<tr>
<td>
<code>namePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>NamePrefix will be prepended to every AWS IAM role bucket name. Defaults to &ldquo;cluster-api-provider-aws-&rdquo;.
AWSCluster S3 Bucket name must be prefixed with the same prefix.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="bootstrap.cluster.x-k8s.io/v1beta1">bootstrap.cluster.x-k8s.io/v1beta1</h2>
Resource Types:
<ul></ul>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfig">EKSConfig
</h3>
<p>
<p>EKSConfig is the schema for the Amazon EKS Machine Bootstrap Configuration API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigSpec">
EKSConfigSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigStatus">
EKSConfigStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigSpec">EKSConfigSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfig">EKSConfig</a>, <a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateResource">EKSConfigTemplateResource</a>)
</p>
<p>
<p>EKSConfigSpec defines the desired state of Amazon EKS Bootstrap Configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigStatus">EKSConfigStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfig">EKSConfig</a>)
</p>
<p>
<p>EKSConfigStatus defines the observed state of the Amazon EKS Bootstrap Configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready indicates the BootstrapData secret is ready to be consumed</p>
</td>
</tr>
<tr>
<td>
<code>dataSecretName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DataSecretName is the name of the secret that stores the bootstrap data script.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set on non-retryable errors</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set on non-retryable errors</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>ObservedGeneration is the latest generation observed by the controller.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the EKSConfig.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplate">EKSConfigTemplate
</h3>
<p>
<p>EKSConfigTemplate is the Amazon EKS Bootstrap Configuration Template API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateSpec">
EKSConfigTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateResource">
EKSConfigTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateResource">EKSConfigTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateSpec">EKSConfigTemplateSpec</a>)
</p>
<p>
<p>EKSConfigTemplateResource defines the Template structure.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigSpec">
EKSConfigSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateSpec">EKSConfigTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplate">EKSConfigTemplate</a>)
</p>
<p>
<p>EKSConfigTemplateSpec defines the desired state of templated EKSConfig Amazon EKS Bootstrap Configuration resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigTemplateResource">
EKSConfigTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta1.PauseContainer">PauseContainer
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta1.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>PauseContainer contains details of pause container.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountNumber</code><br/>
<em>
string
</em>
</td>
<td>
<p>AccountNumber is the AWS account number to pull the pause container from.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the tag of the pause container to use.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="bootstrap.cluster.x-k8s.io/v1beta2">bootstrap.cluster.x-k8s.io/v1beta2</h2>
<p>
<p>Package v1beta2 contains API Schema definitions for the Amazon EKS Bootstrap v1beta2 API group.</p>
</p>
Resource Types:
<ul></ul>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">DiskSetup
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>DiskSetup defines input for generated disk_setup and fs_setup in cloud-init.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>partitions</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.Partition">
[]Partition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partitions specifies the list of the partitions to setup.</p>
</td>
</tr>
<tr>
<td>
<code>filesystems</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.Filesystem">
[]Filesystem
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filesystems specifies the list of file systems to setup.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfig">EKSConfig
</h3>
<p>
<p>EKSConfig is the schema for the Amazon EKS Machine Bootstrap Configuration API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">
EKSConfigSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
<tr>
<td>
<code>preBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreBootstrapCommands specifies extra commands to run before bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>postBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PostBootstrapCommands specifies extra commands to run after bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>boostrapCommandOverride</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BootstrapCommandOverride allows you to override the bootstrap command to use for EKS nodes.</p>
</td>
</tr>
<tr>
<td>
<code>files</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.File">
[]File
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Files specifies extra files to be passed to user_data upon creation.</p>
</td>
</tr>
<tr>
<td>
<code>diskSetup</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">
DiskSetup
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSetup specifies options for the creation of partition tables and file systems on devices.</p>
</td>
</tr>
<tr>
<td>
<code>mounts</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.MountPoints">
[]MountPoints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Mounts specifies a list of mount points to be setup.</p>
</td>
</tr>
<tr>
<td>
<code>users</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.User">
[]User
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Users specifies extra users to add</p>
</td>
</tr>
<tr>
<td>
<code>ntp</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.NTP">
NTP
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NTP specifies NTP configuration</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigStatus">
EKSConfigStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfig">EKSConfig</a>, <a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateResource">EKSConfigTemplateResource</a>)
</p>
<p>
<p>EKSConfigSpec defines the desired state of Amazon EKS Bootstrap Configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
<tr>
<td>
<code>preBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreBootstrapCommands specifies extra commands to run before bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>postBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PostBootstrapCommands specifies extra commands to run after bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>boostrapCommandOverride</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BootstrapCommandOverride allows you to override the bootstrap command to use for EKS nodes.</p>
</td>
</tr>
<tr>
<td>
<code>files</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.File">
[]File
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Files specifies extra files to be passed to user_data upon creation.</p>
</td>
</tr>
<tr>
<td>
<code>diskSetup</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">
DiskSetup
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSetup specifies options for the creation of partition tables and file systems on devices.</p>
</td>
</tr>
<tr>
<td>
<code>mounts</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.MountPoints">
[]MountPoints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Mounts specifies a list of mount points to be setup.</p>
</td>
</tr>
<tr>
<td>
<code>users</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.User">
[]User
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Users specifies extra users to add</p>
</td>
</tr>
<tr>
<td>
<code>ntp</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.NTP">
NTP
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NTP specifies NTP configuration</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigStatus">EKSConfigStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfig">EKSConfig</a>)
</p>
<p>
<p>EKSConfigStatus defines the observed state of the Amazon EKS Bootstrap Configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready indicates the BootstrapData secret is ready to be consumed</p>
</td>
</tr>
<tr>
<td>
<code>dataSecretName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DataSecretName is the name of the secret that stores the bootstrap data script.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set on non-retryable errors</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set on non-retryable errors</p>
</td>
</tr>
<tr>
<td>
<code>observedGeneration</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>ObservedGeneration is the latest generation observed by the controller.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the EKSConfig.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplate">EKSConfigTemplate
</h3>
<p>
<p>EKSConfigTemplate is the Amazon EKS Bootstrap Configuration Template API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateSpec">
EKSConfigTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateResource">
EKSConfigTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateResource">EKSConfigTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateSpec">EKSConfigTemplateSpec</a>)
</p>
<p>
<p>EKSConfigTemplateResource defines the Template structure.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">
EKSConfigSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>kubeletExtraArgs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubeletExtraArgs passes the specified kubelet args into the Amazon EKS machine bootstrap script</p>
</td>
</tr>
<tr>
<td>
<code>containerRuntime</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContainerRuntime specify the container runtime to use when bootstrapping EKS.</p>
</td>
</tr>
<tr>
<td>
<code>dnsClusterIP</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DNSClusterIP overrides the IP address to use for DNS queries within the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>dockerConfigJson</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DockerConfigJson is used for the contents of the /etc/docker/daemon.json file. Useful if you want a custom config differing from the default one in the AMI.
This is expected to be a json string.</p>
</td>
</tr>
<tr>
<td>
<code>apiRetryAttempts</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>APIRetryAttempts is the number of retry attempts for AWS API call.</p>
</td>
</tr>
<tr>
<td>
<code>pauseContainer</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.PauseContainer">
PauseContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PauseContainer allows customization of the pause container to use.</p>
</td>
</tr>
<tr>
<td>
<code>useMaxPods</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UseMaxPods  sets &ndash;max-pods for the kubelet when true.</p>
</td>
</tr>
<tr>
<td>
<code>serviceIPV6Cidr</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceIPV6Cidr is the ipv6 cidr range of the cluster. If this is specified then
the ip family will be set to ipv6.</p>
</td>
</tr>
<tr>
<td>
<code>preBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreBootstrapCommands specifies extra commands to run before bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>postBootstrapCommands</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PostBootstrapCommands specifies extra commands to run after bootstrapping nodes to the cluster</p>
</td>
</tr>
<tr>
<td>
<code>boostrapCommandOverride</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BootstrapCommandOverride allows you to override the bootstrap command to use for EKS nodes.</p>
</td>
</tr>
<tr>
<td>
<code>files</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.File">
[]File
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Files specifies extra files to be passed to user_data upon creation.</p>
</td>
</tr>
<tr>
<td>
<code>diskSetup</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">
DiskSetup
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSetup specifies options for the creation of partition tables and file systems on devices.</p>
</td>
</tr>
<tr>
<td>
<code>mounts</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.MountPoints">
[]MountPoints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Mounts specifies a list of mount points to be setup.</p>
</td>
</tr>
<tr>
<td>
<code>users</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.User">
[]User
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Users specifies extra users to add</p>
</td>
</tr>
<tr>
<td>
<code>ntp</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.NTP">
NTP
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NTP specifies NTP configuration</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateSpec">EKSConfigTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplate">EKSConfigTemplate</a>)
</p>
<p>
<p>EKSConfigTemplateSpec defines the desired state of templated EKSConfig Amazon EKS Bootstrap Configuration resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigTemplateResource">
EKSConfigTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.Encoding">Encoding
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.File">File</a>)
</p>
<p>
<p>Encoding specifies the cloud-init file encoding.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;base64&#34;</p></td>
<td><p>Base64 implies the contents of the file are encoded as base64.</p>
</td>
</tr><tr><td><p>&#34;gzip&#34;</p></td>
<td><p>Gzip implies the contents of the file are encoded with gzip.</p>
</td>
</tr><tr><td><p>&#34;gzip&#43;base64&#34;</p></td>
<td><p>GzipBase64 implies the contents of the file are first base64 encoded and then gzip encoded.</p>
</td>
</tr></tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.File">File
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>File defines the input for generating write_files in cloud-init.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>path</code><br/>
<em>
string
</em>
</td>
<td>
<p>Path specifies the full path on disk where to store the file.</p>
</td>
</tr>
<tr>
<td>
<code>owner</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Owner specifies the ownership of the file, e.g. &ldquo;root:root&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>permissions</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Permissions specifies the permissions to assign to the file, e.g. &ldquo;0640&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>encoding</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.Encoding">
Encoding
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encoding specifies the encoding of the file contents.</p>
</td>
</tr>
<tr>
<td>
<code>append</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Append specifies whether to append Content to existing file if Path exists.</p>
</td>
</tr>
<tr>
<td>
<code>content</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Content is the actual content of the file.</p>
</td>
</tr>
<tr>
<td>
<code>contentFrom</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.FileSource">
FileSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ContentFrom is a referenced source of content to populate the file.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.FileSource">FileSource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.File">File</a>)
</p>
<p>
<p>FileSource is a union of all possible external source types for file data.
Only one field may be populated in any given instance. Developers adding new
sources of data for target systems should add them here.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>secret</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.SecretFileSource">
SecretFileSource
</a>
</em>
</td>
<td>
<p>Secret represents a secret that should populate this file.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.Filesystem">Filesystem
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">DiskSetup</a>)
</p>
<p>
<p>Filesystem defines the file systems to be created.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>device</code><br/>
<em>
string
</em>
</td>
<td>
<p>Device specifies the device name</p>
</td>
</tr>
<tr>
<td>
<code>filesystem</code><br/>
<em>
string
</em>
</td>
<td>
<p>Filesystem specifies the file system type.</p>
</td>
</tr>
<tr>
<td>
<code>label</code><br/>
<em>
string
</em>
</td>
<td>
<p>Label specifies the file system label to be used. If set to None, no label is used.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition specifies the partition to use. The valid options are: &ldquo;auto|any&rdquo;, &ldquo;auto&rdquo;, &ldquo;any&rdquo;, &ldquo;none&rdquo;, and <NUM>, where NUM is the actual partition number.</p>
</td>
</tr>
<tr>
<td>
<code>overwrite</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Overwrite defines whether or not to overwrite any existing filesystem.
If true, any pre-existing file system will be destroyed. Use with Caution.</p>
</td>
</tr>
<tr>
<td>
<code>extraOpts</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ExtraOpts defined extra options to add to the command for creating the file system.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.MountPoints">MountPoints
(<code>[]string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>MountPoints defines input for generated mounts in cloud-init.</p>
</p>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.NTP">NTP
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>NTP defines input for generated ntp in cloud-init.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>servers</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Servers specifies which NTP servers to use</p>
</td>
</tr>
<tr>
<td>
<code>enabled</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enabled specifies whether NTP should be enabled</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.Partition">Partition
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.DiskSetup">DiskSetup</a>)
</p>
<p>
<p>Partition defines how to create and layout a partition.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>device</code><br/>
<em>
string
</em>
</td>
<td>
<p>Device is the name of the device.</p>
</td>
</tr>
<tr>
<td>
<code>layout</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Layout specifies the device layout.
If it is true, a single partition will be created for the entire device.
When layout is false, it means don&rsquo;t partition or ignore existing partitioning.</p>
</td>
</tr>
<tr>
<td>
<code>overwrite</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Overwrite describes whether to skip checks and create the partition if a partition or filesystem is found on the device.
Use with caution. Default is &lsquo;false&rsquo;.</p>
</td>
</tr>
<tr>
<td>
<code>tableType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>TableType specifies the tupe of partition table. The following are supported:
&lsquo;mbr&rsquo;: default and setups a MS-DOS partition table
&lsquo;gpt&rsquo;: setups a GPT partition table</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.PasswdSource">PasswdSource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.User">User</a>)
</p>
<p>
<p>PasswdSource is a union of all possible external source types for passwd data.
Only one field may be populated in any given instance. Developers adding new
sources of data for target systems should add them here.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>secret</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.SecretPasswdSource">
SecretPasswdSource
</a>
</em>
</td>
<td>
<p>Secret represents a secret that should populate this password.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.PauseContainer">PauseContainer
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>PauseContainer contains details of pause container.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountNumber</code><br/>
<em>
string
</em>
</td>
<td>
<p>AccountNumber is the AWS account number to pull the pause container from.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the tag of the pause container to use.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.SecretFileSource">SecretFileSource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.FileSource">FileSource</a>)
</p>
<p>
<p>SecretFileSource adapts a Secret into a FileSource.</p>
<p>The contents of the target Secret&rsquo;s Data field will be presented
as files using the keys in the Data field as the file names.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the secret in the KubeadmBootstrapConfig&rsquo;s namespace to use.</p>
</td>
</tr>
<tr>
<td>
<code>key</code><br/>
<em>
string
</em>
</td>
<td>
<p>Key is the key in the secret&rsquo;s data map for this value.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.SecretPasswdSource">SecretPasswdSource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.PasswdSource">PasswdSource</a>)
</p>
<p>
<p>SecretPasswdSource adapts a Secret into a PasswdSource.</p>
<p>The contents of the target Secret&rsquo;s Data field will be presented
as passwd using the keys in the Data field as the file names.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the secret in the KubeadmBootstrapConfig&rsquo;s namespace to use.</p>
</td>
</tr>
<tr>
<td>
<code>key</code><br/>
<em>
string
</em>
</td>
<td>
<p>Key is the key in the secret&rsquo;s data map for this value.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="bootstrap.cluster.x-k8s.io/v1beta2.User">User
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.EKSConfigSpec">EKSConfigSpec</a>)
</p>
<p>
<p>User defines the input for a generated user in cloud-init.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name specifies the username</p>
</td>
</tr>
<tr>
<td>
<code>gecos</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Gecos specifies the gecos to use for the user</p>
</td>
</tr>
<tr>
<td>
<code>groups</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Groups specifies the additional groups for the user</p>
</td>
</tr>
<tr>
<td>
<code>homeDir</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HomeDir specifies the home directory to use for the user</p>
</td>
</tr>
<tr>
<td>
<code>inactive</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Inactive specifies whether to mark the user as inactive</p>
</td>
</tr>
<tr>
<td>
<code>shell</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Shell specifies the user&rsquo;s shell</p>
</td>
</tr>
<tr>
<td>
<code>passwd</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Passwd specifies a hashed password for the user</p>
</td>
</tr>
<tr>
<td>
<code>passwdFrom</code><br/>
<em>
<a href="crd/index.html#bootstrap.cluster.x-k8s.io/v1beta2.PasswdSource">
PasswdSource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PasswdFrom is a referenced source of passwd to populate the passwd.</p>
</td>
</tr>
<tr>
<td>
<code>primaryGroup</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrimaryGroup specifies the primary group for the user</p>
</td>
</tr>
<tr>
<td>
<code>lockPassword</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>LockPassword specifies if password login should be disabled</p>
</td>
</tr>
<tr>
<td>
<code>sudo</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Sudo specifies a sudo role for the user</p>
</td>
</tr>
<tr>
<td>
<code>sshAuthorizedKeys</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHAuthorizedKeys specifies a list of ssh authorized keys for the user</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="controlplane.cluster.x-k8s.io/v1beta1">controlplane.cluster.x-k8s.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains API Schema definitions for the controlplane v1beta1 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlane">AWSManagedControlPlane
</h3>
<p>
<p>AWSManagedControlPlane is the schema for the Amazon EKS Managed Control Plane API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">
AWSManagedControlPlaneSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>eksClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSClusterName allows you to specify the name of the EKS cluster in
AWS. If you don&rsquo;t specify a name then a default name will be created
based on the namespace and name of the managed control plane.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlock is the additional CIDR range to use for pod IPs.
Must be within the 100.64.0.0/10 or 198.19.0.0/16 range.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the desired Kubernetes version. If no version number
is supplied then the latest version of Kubernetes that EKS supports
will be used.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role that gives EKS
permission to make API calls. If the role is pre-existing
we will treat it as unmanaged and not delete it on
deletion. If the EKSEnableIAM feature flag is true
and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the control plane role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>logging</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.ControlPlaneLoggingSpec">
ControlPlaneLoggingSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Logging specifies which EKS Cluster logs should be enabled. Entries for
each of the enabled logs will be sent to CloudWatch</p>
</td>
</tr>
<tr>
<td>
<code>encryptionConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EncryptionConfig">
EncryptionConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionConfig specifies the encryption configuration for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>iamAuthenticatorConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.IAMAuthenticatorConfig">
IAMAuthenticatorConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMAuthenticatorConfig allows the specification of any additional user or role mappings
for use when generating the aws-iam-authenticator configuration. If this is nil the
default configuration is still generated for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EndpointAccess">
EndpointAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Endpoints specifies access to this cluster&rsquo;s control plane endpoints</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>tokenMethod</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EKSTokenMethod">
EKSTokenMethod
</a>
</em>
</td>
<td>
<p>TokenMethod is used to specify the method for obtaining a client token for communicating with EKS
iam-authenticator - obtains a client token using iam-authentictor
aws-cli - obtains a client token using the AWS CLI
Defaults to iam-authenticator</p>
</td>
</tr>
<tr>
<td>
<code>associateOIDCProvider</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AssociateOIDCProvider can be enabled to automatically create an identity
provider for the controller for use with IAM roles for service accounts</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta1.Addon">
[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta1.Addon
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons defines the EKS addons to enable with the EKS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>oidcIdentityProviderConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.OIDCIdentityProviderConfig">
OIDCIdentityProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityProviderconfig is used to specify the oidc provider config
to be attached with this eks cluster</p>
</td>
</tr>
<tr>
<td>
<code>disableVPCCNI</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableVPCCNI indicates that the Amazon VPC CNI should be disabled. With EKS clusters the
Amazon VPC CNI is automatically installed into the cluster. For clusters where you want
to use an alternate CNI this option provides a way to specify that the Amazon VPC CNI
should be deleted. You cannot set this to true if you are using the
Amazon VPC CNI addon.</p>
</td>
</tr>
<tr>
<td>
<code>vpcCni</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.VpcCni">
VpcCni
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VpcCni is used to set configuration options for the VPC CNI plugin</p>
</td>
</tr>
<tr>
<td>
<code>kubeProxy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.KubeProxy">
KubeProxy
</a>
</em>
</td>
<td>
<p>KubeProxy defines managed attributes of the kube-proxy daemonset</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">
AWSManagedControlPlaneStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlane">AWSManagedControlPlane</a>)
</p>
<p>
<p>AWSManagedControlPlaneSpec defines the desired state of an Amazon EKS Cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>eksClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSClusterName allows you to specify the name of the EKS cluster in
AWS. If you don&rsquo;t specify a name then a default name will be created
based on the namespace and name of the managed control plane.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlock is the additional CIDR range to use for pod IPs.
Must be within the 100.64.0.0/10 or 198.19.0.0/16 range.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the desired Kubernetes version. If no version number
is supplied then the latest version of Kubernetes that EKS supports
will be used.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role that gives EKS
permission to make API calls. If the role is pre-existing
we will treat it as unmanaged and not delete it on
deletion. If the EKSEnableIAM feature flag is true
and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the control plane role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>logging</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.ControlPlaneLoggingSpec">
ControlPlaneLoggingSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Logging specifies which EKS Cluster logs should be enabled. Entries for
each of the enabled logs will be sent to CloudWatch</p>
</td>
</tr>
<tr>
<td>
<code>encryptionConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EncryptionConfig">
EncryptionConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionConfig specifies the encryption configuration for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>iamAuthenticatorConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.IAMAuthenticatorConfig">
IAMAuthenticatorConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMAuthenticatorConfig allows the specification of any additional user or role mappings
for use when generating the aws-iam-authenticator configuration. If this is nil the
default configuration is still generated for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EndpointAccess">
EndpointAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Endpoints specifies access to this cluster&rsquo;s control plane endpoints</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>tokenMethod</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.EKSTokenMethod">
EKSTokenMethod
</a>
</em>
</td>
<td>
<p>TokenMethod is used to specify the method for obtaining a client token for communicating with EKS
iam-authenticator - obtains a client token using iam-authentictor
aws-cli - obtains a client token using the AWS CLI
Defaults to iam-authenticator</p>
</td>
</tr>
<tr>
<td>
<code>associateOIDCProvider</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AssociateOIDCProvider can be enabled to automatically create an identity
provider for the controller for use with IAM roles for service accounts</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta1.Addon">
[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta1.Addon
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons defines the EKS addons to enable with the EKS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>oidcIdentityProviderConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.OIDCIdentityProviderConfig">
OIDCIdentityProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityProviderconfig is used to specify the oidc provider config
to be attached with this eks cluster</p>
</td>
</tr>
<tr>
<td>
<code>disableVPCCNI</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableVPCCNI indicates that the Amazon VPC CNI should be disabled. With EKS clusters the
Amazon VPC CNI is automatically installed into the cluster. For clusters where you want
to use an alternate CNI this option provides a way to specify that the Amazon VPC CNI
should be deleted. You cannot set this to true if you are using the
Amazon VPC CNI addon.</p>
</td>
</tr>
<tr>
<td>
<code>vpcCni</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.VpcCni">
VpcCni
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VpcCni is used to set configuration options for the VPC CNI plugin</p>
</td>
</tr>
<tr>
<td>
<code>kubeProxy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.KubeProxy">
KubeProxy
</a>
</em>
</td>
<td>
<p>KubeProxy defines managed attributes of the kube-proxy daemonset</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlane">AWSManagedControlPlane</a>)
</p>
<p>
<p>AWSManagedControlPlaneStatus defines the observed state of an Amazon EKS Cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>networkStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">
NetworkStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Networks holds details about the AWS networking resources used by the control plane</p>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureDomains specifies a list fo available availability zones that can be used</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">
Instance
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion holds details of the instance that is used as a bastion jump box</p>
</td>
</tr>
<tr>
<td>
<code>oidcProvider</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.OIDCProviderStatus">
OIDCProviderStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCProvider holds the status of the identity provider for this cluster</p>
</td>
</tr>
<tr>
<td>
<code>externalManagedControlPlane</code><br/>
<em>
bool
</em>
</td>
<td>
<p>ExternalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service such as AKS, EKS, GKE, etc.</p>
</td>
</tr>
<tr>
<td>
<code>initialized</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Initialized denotes whether or not the control plane has the
uploaded kubernetes config-map.</p>
</td>
</tr>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the AWSManagedControlPlane API Server is ready to
receive requests and that the VPC infra is ready.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ErrorMessage indicates that there is a terminal problem reconciling the
state, and will be set to a descriptive error message.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<p>Conditions specifies the cpnditions for the managed control plane</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AddonState">
[]AddonState
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons holds the current status of the EKS addons</p>
</td>
</tr>
<tr>
<td>
<code>identityProviderStatus</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.IdentityProviderStatus">
IdentityProviderStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityProviderStatus holds the status for
associated identity provider</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.Addon">Addon
</h3>
<p>
<p>Addon represents a EKS addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the addon</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the version of the addon to use</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration of the EKS addon</p>
</td>
</tr>
<tr>
<td>
<code>conflictResolution</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AddonResolution">
AddonResolution
</a>
</em>
</td>
<td>
<p>ConflictResolution is used to declare what should happen if there
are parameter conflicts. Defaults to none</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountRoleArn is the ARN of an IAM role to bind to the addons service account</p>
</td>
</tr>
<tr>
<td>
<code>preserveOnDelete</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreserveOnDelete indicates that the addon resources should be
preserved in the cluster on delete.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AddonIssue">AddonIssue
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AddonState">AddonState</a>)
</p>
<p>
<p>AddonIssue represents an issue with an addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>code</code><br/>
<em>
string
</em>
</td>
<td>
<p>Code is the issue code</p>
</td>
</tr>
<tr>
<td>
<code>message</code><br/>
<em>
string
</em>
</td>
<td>
<p>Message is the textual description of the issue</p>
</td>
</tr>
<tr>
<td>
<code>resourceIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ResourceIDs is a list of resource ids for the issue</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AddonResolution">AddonResolution
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.Addon">Addon</a>)
</p>
<p>
<p>AddonResolution defines the method for resolving parameter conflicts.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AddonState">AddonState
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>AddonState represents the state of an addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the addon</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the version of the addon to use</p>
</td>
</tr>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN is the AWS ARN of the addon</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>ServiceAccountRoleArn is the ARN of the IAM role used for the service account</p>
</td>
</tr>
<tr>
<td>
<code>createdAt</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>CreatedAt is the date and time the addon was created at</p>
</td>
</tr>
<tr>
<td>
<code>modifiedAt</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>ModifiedAt is the date and time the addon was last modified</p>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
string
</em>
</td>
<td>
<p>Status is the status of the addon</p>
</td>
</tr>
<tr>
<td>
<code>issues</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AddonIssue">
[]AddonIssue
</a>
</em>
</td>
<td>
<p>Issues is a list of issue associated with the addon</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.AddonStatus">AddonStatus
(<code>string</code> alias)</p></h3>
<p>
<p>AddonStatus defines the status for an addon.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.ControlPlaneLoggingSpec">ControlPlaneLoggingSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>ControlPlaneLoggingSpec defines what EKS control plane logs that should be enabled.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiServer</code><br/>
<em>
bool
</em>
</td>
<td>
<p>APIServer indicates if the Kubernetes API Server log (kube-apiserver) shoulkd be enabled</p>
</td>
</tr>
<tr>
<td>
<code>audit</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Audit indicates if the Kubernetes API audit log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>authenticator</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Authenticator indicates if the iam authenticator log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>controllerManager</code><br/>
<em>
bool
</em>
</td>
<td>
<p>ControllerManager indicates if the controller manager (kube-controller-manager) log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>scheduler</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Scheduler indicates if the Kubernetes scheduler (kube-scheduler) log should be enabled</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.EKSTokenMethod">EKSTokenMethod
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EKSTokenMethod defines the method for obtaining a client token to use when connecting to EKS.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.EncryptionConfig">EncryptionConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EncryptionConfig specifies the encryption configuration for the EKS clsuter.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provider</code><br/>
<em>
string
</em>
</td>
<td>
<p>Provider specifies the ARN or alias of the CMK (in AWS KMS)</p>
</td>
</tr>
<tr>
<td>
<code>resources</code><br/>
<em>
[]*string
</em>
</td>
<td>
<p>Resources specifies the resources to be encrypted</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.EndpointAccess">EndpointAccess
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EndpointAccess specifies how control plane endpoints are accessible.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>public</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Public controls whether control plane endpoints are publicly accessible</p>
</td>
</tr>
<tr>
<td>
<code>publicCIDRs</code><br/>
<em>
[]*string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicCIDRs specifies which blocks can access the public endpoint</p>
</td>
</tr>
<tr>
<td>
<code>private</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Private points VPC-internal control plane access to the private endpoint</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.IAMAuthenticatorConfig">IAMAuthenticatorConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>IAMAuthenticatorConfig represents an aws-iam-authenticator configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>mapRoles</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.RoleMapping">
[]RoleMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleMappings is a list of role mappings</p>
</td>
</tr>
<tr>
<td>
<code>mapUsers</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.UserMapping">
[]UserMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UserMappings is a list of user mappings</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.IdentityProviderStatus">IdentityProviderStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>IdentityProviderStatus holds the status for associated identity provider</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN holds the ARN of associated identity provider</p>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
string
</em>
</td>
<td>
<p>Status holds current status of associated identity provider</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.KubeProxy">KubeProxy
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>KubeProxy specifies how the kube-proxy daemonset is managed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable set to true indicates that kube-proxy should be disabled. With EKS clusters
kube-proxy is automatically installed into the cluster. For clusters where you want
to use kube-proxy functionality that is provided with an alternate CNI, this option
provides a way to specify that the kube-proxy daemonset should be deleted. You cannot
set this to true if you are using the Amazon kube-proxy addon.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.KubernetesMapping">KubernetesMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.RoleMapping">RoleMapping</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.UserMapping">UserMapping</a>)
</p>
<p>
<p>KubernetesMapping represents the kubernetes RBAC mapping.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>username</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserName is a kubernetes RBAC user subject</p>
</td>
</tr>
<tr>
<td>
<code>groups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Groups is a list of kubernetes RBAC groups</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.OIDCIdentityProviderConfig">OIDCIdentityProviderConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>OIDCIdentityProviderConfig defines the configuration for an OIDC identity provider.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientId</code><br/>
<em>
string
</em>
</td>
<td>
<p>This is also known as audience. The ID for the client application that makes
authentication requests to the OpenID identity provider.</p>
</td>
</tr>
<tr>
<td>
<code>groupsClaim</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The JWT claim that the provider uses to return your groups.</p>
</td>
</tr>
<tr>
<td>
<code>groupsPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The prefix that is prepended to group claims to prevent clashes with existing
names (such as system: groups). For example, the valueoidc: will create group
names like oidc:engineering and oidc:infra.</p>
</td>
</tr>
<tr>
<td>
<code>identityProviderConfigName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the OIDC provider configuration.</p>
<p>IdentityProviderConfigName is a required field</p>
</td>
</tr>
<tr>
<td>
<code>issuerUrl</code><br/>
<em>
string
</em>
</td>
<td>
<p>The URL of the OpenID identity provider that allows the API server to discover
public signing keys for verifying tokens. The URL must begin with https://
and should correspond to the iss claim in the provider&rsquo;s OIDC ID tokens.
Per the OIDC standard, path components are allowed but query parameters are
not. Typically the URL consists of only a hostname, like <a href="https://server.example.org">https://server.example.org</a>
or <a href="https://example.com">https://example.com</a>. This URL should point to the level below .well-known/openid-configuration
and must be publicly accessible over the internet.</p>
</td>
</tr>
<tr>
<td>
<code>requiredClaims</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The key value pairs that describe required claims in the identity token.
If set, each claim is verified to be present in the token with a matching
value. For the maximum number of claims that you can require, see Amazon
EKS service quotas (<a href="https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html">https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html</a>)
in the Amazon EKS User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>usernameClaim</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The JSON Web Token (JWT) claim to use as the username. The default is sub,
which is expected to be a unique identifier of the end user. You can choose
other claims, such as email or name, depending on the OpenID identity provider.
Claims other than email are prefixed with the issuer URL to prevent naming
clashes with other plug-ins.</p>
</td>
</tr>
<tr>
<td>
<code>usernamePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The prefix that is prepended to username claims to prevent clashes with existing
names. If you do not provide this field, and username is a value other than
email, the prefix defaults to issuerurl#. You can use the value - to disable
all prefixing.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tags to apply to oidc identity provider association</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.OIDCProviderStatus">OIDCProviderStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>OIDCProviderStatus holds the status of the AWS OIDC identity provider.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN holds the ARN of the provider</p>
</td>
</tr>
<tr>
<td>
<code>trustPolicy</code><br/>
<em>
string
</em>
</td>
<td>
<p>TrustPolicy contains the boilerplate IAM trust policy to use for IRSA</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.RoleMapping">RoleMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.IAMAuthenticatorConfig">IAMAuthenticatorConfig</a>)
</p>
<p>
<p>RoleMapping represents a mapping from a IAM role to Kubernetes users and groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rolearn</code><br/>
<em>
string
</em>
</td>
<td>
<p>RoleARN is the AWS ARN for the role to map</p>
</td>
</tr>
<tr>
<td>
<code>KubernetesMapping</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.KubernetesMapping">
KubernetesMapping
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubernetesMapping</code> are embedded into this type.)
</p>
<p>KubernetesMapping holds the RBAC details for the mapping</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.UserMapping">UserMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.IAMAuthenticatorConfig">IAMAuthenticatorConfig</a>)
</p>
<p>
<p>UserMapping represents a mapping from an IAM user to Kubernetes users and groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>userarn</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserARN is the AWS ARN for the user to map</p>
</td>
</tr>
<tr>
<td>
<code>KubernetesMapping</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.KubernetesMapping">
KubernetesMapping
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubernetesMapping</code> are embedded into this type.)
</p>
<p>KubernetesMapping holds the RBAC details for the mapping</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta1.VpcCni">VpcCni
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>VpcCni specifies configuration related to the VPC CNI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>env</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#envvar-v1-core">
[]Kubernetes core/v1.EnvVar
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Env defines a list of environment variables to apply to the <code>aws-node</code> DaemonSet</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="controlplane.cluster.x-k8s.io/v1beta2">controlplane.cluster.x-k8s.io/v1beta2</h2>
<p>
<p>Package v1beta2 contains API Schema definitions for the controlplane v1beta2 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlane">AWSManagedControlPlane
</h3>
<p>
<p>AWSManagedControlPlane is the schema for the Amazon EKS Managed Control Plane API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">
AWSManagedControlPlaneSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>eksClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSClusterName allows you to specify the name of the EKS cluster in
AWS. If you don&rsquo;t specify a name then a default name will be created
based on the namespace and name of the managed control plane.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlock is the additional CIDR range to use for pod IPs.
Must be within the 100.64.0.0/10 or 198.19.0.0/16 range.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the desired Kubernetes version. If no version number
is supplied then the latest version of Kubernetes that EKS supports
will be used.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role that gives EKS
permission to make API calls. If the role is pre-existing
we will treat it as unmanaged and not delete it on
deletion. If the EKSEnableIAM feature flag is true
and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the control plane role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>logging</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ControlPlaneLoggingSpec">
ControlPlaneLoggingSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Logging specifies which EKS Cluster logs should be enabled. Entries for
each of the enabled logs will be sent to CloudWatch</p>
</td>
</tr>
<tr>
<td>
<code>encryptionConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EncryptionConfig">
EncryptionConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionConfig specifies the encryption configuration for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>iamAuthenticatorConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">
IAMAuthenticatorConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMAuthenticatorConfig allows the specification of any additional user or role mappings
for use when generating the aws-iam-authenticator configuration. If this is nil the
default configuration is still generated for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EndpointAccess">
EndpointAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Endpoints specifies access to this cluster&rsquo;s control plane endpoints</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>tokenMethod</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EKSTokenMethod">
EKSTokenMethod
</a>
</em>
</td>
<td>
<p>TokenMethod is used to specify the method for obtaining a client token for communicating with EKS
iam-authenticator - obtains a client token using iam-authentictor
aws-cli - obtains a client token using the AWS CLI
Defaults to iam-authenticator</p>
</td>
</tr>
<tr>
<td>
<code>associateOIDCProvider</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AssociateOIDCProvider can be enabled to automatically create an identity
provider for the controller for use with IAM roles for service accounts</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon">
[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons defines the EKS addons to enable with the EKS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>oidcIdentityProviderConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCIdentityProviderConfig">
OIDCIdentityProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCIdentityProviderConfig is used to specify the OIDC provider config
to be attached with this eks cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessConfig">
AccessConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessConfig specifies the access configuration information for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessEntries</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">
[]AccessEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessEntries specifies the access entries for the cluster
Access entries require AuthenticationMode to be either &ldquo;api&rdquo; or &ldquo;api_and_config_map&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>vpcCni</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.VpcCni">
VpcCni
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VpcCni is used to set configuration options for the VPC CNI plugin</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapSelfManagedAddons</code><br/>
<em>
bool
</em>
</td>
<td>
<p>BootstrapSelfManagedAddons is used to set configuration options for
bare EKS cluster without EKS default networking addons
If you set this value to false when creating a cluster, the default networking add-ons will not be installed</p>
</td>
</tr>
<tr>
<td>
<code>restrictPrivateSubnets</code><br/>
<em>
bool
</em>
</td>
<td>
<p>RestrictPrivateSubnets indicates that the EKS control plane should only use private subnets.</p>
</td>
</tr>
<tr>
<td>
<code>kubeProxy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.KubeProxy">
KubeProxy
</a>
</em>
</td>
<td>
<p>KubeProxy defines managed attributes of the kube-proxy daemonset</p>
</td>
</tr>
<tr>
<td>
<code>upgradePolicy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UpgradePolicy">
UpgradePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The cluster upgrade policy to use for the cluster.
(Official AWS docs for this policy: <a href="https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html">https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html</a>)
<code>extended</code> upgrade policy indicates that the cluster will enter into extended support once the Kubernetes version reaches end of standard support. You will incur extended support charges with this setting. You can upgrade your cluster to a standard supported Kubernetes version to stop incurring extended support charges.
<code>standard</code> upgrade policy indicates that the cluster is eligible for automatic upgrade at the end of standard support. You will not incur extended support charges with this setting but your EKS cluster will automatically upgrade to the next supported Kubernetes version in standard support.
If omitted, new clusters will use the AWS default upgrade policy (which at the time of writing is &ldquo;extended&rdquo;) and existing clusters will have their upgrade policy unchanged.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">
AWSManagedControlPlaneStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlane">AWSManagedControlPlane</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateResource">AWSManagedControlPlaneTemplateResource</a>)
</p>
<p>
<p>AWSManagedControlPlaneSpec defines the desired state of an Amazon EKS Cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>eksClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSClusterName allows you to specify the name of the EKS cluster in
AWS. If you don&rsquo;t specify a name then a default name will be created
based on the namespace and name of the managed control plane.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlock is the additional CIDR range to use for pod IPs.
Must be within the 100.64.0.0/10 or 198.19.0.0/16 range.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the desired Kubernetes version. If no version number
is supplied then the latest version of Kubernetes that EKS supports
will be used.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role that gives EKS
permission to make API calls. If the role is pre-existing
we will treat it as unmanaged and not delete it on
deletion. If the EKSEnableIAM feature flag is true
and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the control plane role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>logging</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ControlPlaneLoggingSpec">
ControlPlaneLoggingSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Logging specifies which EKS Cluster logs should be enabled. Entries for
each of the enabled logs will be sent to CloudWatch</p>
</td>
</tr>
<tr>
<td>
<code>encryptionConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EncryptionConfig">
EncryptionConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionConfig specifies the encryption configuration for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>iamAuthenticatorConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">
IAMAuthenticatorConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMAuthenticatorConfig allows the specification of any additional user or role mappings
for use when generating the aws-iam-authenticator configuration. If this is nil the
default configuration is still generated for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EndpointAccess">
EndpointAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Endpoints specifies access to this cluster&rsquo;s control plane endpoints</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>tokenMethod</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EKSTokenMethod">
EKSTokenMethod
</a>
</em>
</td>
<td>
<p>TokenMethod is used to specify the method for obtaining a client token for communicating with EKS
iam-authenticator - obtains a client token using iam-authentictor
aws-cli - obtains a client token using the AWS CLI
Defaults to iam-authenticator</p>
</td>
</tr>
<tr>
<td>
<code>associateOIDCProvider</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AssociateOIDCProvider can be enabled to automatically create an identity
provider for the controller for use with IAM roles for service accounts</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon">
[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons defines the EKS addons to enable with the EKS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>oidcIdentityProviderConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCIdentityProviderConfig">
OIDCIdentityProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCIdentityProviderConfig is used to specify the OIDC provider config
to be attached with this eks cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessConfig">
AccessConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessConfig specifies the access configuration information for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessEntries</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">
[]AccessEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessEntries specifies the access entries for the cluster
Access entries require AuthenticationMode to be either &ldquo;api&rdquo; or &ldquo;api_and_config_map&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>vpcCni</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.VpcCni">
VpcCni
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VpcCni is used to set configuration options for the VPC CNI plugin</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapSelfManagedAddons</code><br/>
<em>
bool
</em>
</td>
<td>
<p>BootstrapSelfManagedAddons is used to set configuration options for
bare EKS cluster without EKS default networking addons
If you set this value to false when creating a cluster, the default networking add-ons will not be installed</p>
</td>
</tr>
<tr>
<td>
<code>restrictPrivateSubnets</code><br/>
<em>
bool
</em>
</td>
<td>
<p>RestrictPrivateSubnets indicates that the EKS control plane should only use private subnets.</p>
</td>
</tr>
<tr>
<td>
<code>kubeProxy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.KubeProxy">
KubeProxy
</a>
</em>
</td>
<td>
<p>KubeProxy defines managed attributes of the kube-proxy daemonset</p>
</td>
</tr>
<tr>
<td>
<code>upgradePolicy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UpgradePolicy">
UpgradePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The cluster upgrade policy to use for the cluster.
(Official AWS docs for this policy: <a href="https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html">https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html</a>)
<code>extended</code> upgrade policy indicates that the cluster will enter into extended support once the Kubernetes version reaches end of standard support. You will incur extended support charges with this setting. You can upgrade your cluster to a standard supported Kubernetes version to stop incurring extended support charges.
<code>standard</code> upgrade policy indicates that the cluster is eligible for automatic upgrade at the end of standard support. You will not incur extended support charges with this setting but your EKS cluster will automatically upgrade to the next supported Kubernetes version in standard support.
If omitted, new clusters will use the AWS default upgrade policy (which at the time of writing is &ldquo;extended&rdquo;) and existing clusters will have their upgrade policy unchanged.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlane">AWSManagedControlPlane</a>)
</p>
<p>
<p>AWSManagedControlPlaneStatus defines the observed state of an Amazon EKS Cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>networkStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">
NetworkStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Networks holds details about the AWS networking resources used by the control plane</p>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureDomains specifies a list fo available availability zones that can be used</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">
Instance
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion holds details of the instance that is used as a bastion jump box</p>
</td>
</tr>
<tr>
<td>
<code>oidcProvider</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCProviderStatus">
OIDCProviderStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCProvider holds the status of the identity provider for this cluster</p>
</td>
</tr>
<tr>
<td>
<code>externalManagedControlPlane</code><br/>
<em>
bool
</em>
</td>
<td>
<p>ExternalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service such as AKS, EKS, GKE, etc.</p>
</td>
</tr>
<tr>
<td>
<code>initialized</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Initialized denotes whether or not the control plane has the
uploaded kubernetes config-map.</p>
</td>
</tr>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the AWSManagedControlPlane API Server is ready to
receive requests and that the VPC infra is ready.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ErrorMessage indicates that there is a terminal problem reconciling the
state, and will be set to a descriptive error message.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<p>Conditions specifies the cpnditions for the managed control plane</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AddonState">
[]AddonState
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons holds the current status of the EKS addons</p>
</td>
</tr>
<tr>
<td>
<code>identityProviderStatus</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IdentityProviderStatus">
IdentityProviderStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityProviderStatus holds the status for
associated identity provider</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version represents the minimum Kubernetes version for the control plane machines
in the cluster.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplate">AWSManagedControlPlaneTemplate
</h3>
<p>
<p>AWSManagedControlPlaneTemplate is the Schema for the AWSManagedControlPlaneTemplates API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateSpec">
AWSManagedControlPlaneTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateResource">
AWSManagedControlPlaneTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateResource">AWSManagedControlPlaneTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateSpec">AWSManagedControlPlaneTemplateSpec</a>)
</p>
<p>
<p>AWSManagedControlPlaneTemplateResource describes the data needed to create an AWSManagedCluster from a template.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">
AWSManagedControlPlaneSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>eksClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSClusterName allows you to specify the name of the EKS cluster in
AWS. If you don&rsquo;t specify a name then a default name will be created
based on the namespace and name of the managed control plane.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlock is the additional CIDR range to use for pod IPs.
Must be within the 100.64.0.0/10 or 198.19.0.0/16 range.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the desired Kubernetes version. If no version number
is supplied then the latest version of Kubernetes that EKS supports
will be used.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role that gives EKS
permission to make API calls. If the role is pre-existing
we will treat it as unmanaged and not delete it on
deletion. If the EKSEnableIAM feature flag is true
and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the control plane role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>logging</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ControlPlaneLoggingSpec">
ControlPlaneLoggingSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Logging specifies which EKS Cluster logs should be enabled. Entries for
each of the enabled logs will be sent to CloudWatch</p>
</td>
</tr>
<tr>
<td>
<code>encryptionConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EncryptionConfig">
EncryptionConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionConfig specifies the encryption configuration for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>iamAuthenticatorConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">
IAMAuthenticatorConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMAuthenticatorConfig allows the specification of any additional user or role mappings
for use when generating the aws-iam-authenticator configuration. If this is nil the
default configuration is still generated for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EndpointAccess">
EndpointAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Endpoints specifies access to this cluster&rsquo;s control plane endpoints</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>tokenMethod</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EKSTokenMethod">
EKSTokenMethod
</a>
</em>
</td>
<td>
<p>TokenMethod is used to specify the method for obtaining a client token for communicating with EKS
iam-authenticator - obtains a client token using iam-authentictor
aws-cli - obtains a client token using the AWS CLI
Defaults to iam-authenticator</p>
</td>
</tr>
<tr>
<td>
<code>associateOIDCProvider</code><br/>
<em>
bool
</em>
</td>
<td>
<p>AssociateOIDCProvider can be enabled to automatically create an identity
provider for the controller for use with IAM roles for service accounts</p>
</td>
</tr>
<tr>
<td>
<code>addons</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon">
[]sigs.k8s.io/cluster-api-provider-aws/v2/controlplane/eks/api/v1beta2.Addon
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Addons defines the EKS addons to enable with the EKS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>oidcIdentityProviderConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCIdentityProviderConfig">
OIDCIdentityProviderConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCIdentityProviderConfig is used to specify the OIDC provider config
to be attached with this eks cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessConfig">
AccessConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessConfig specifies the access configuration information for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>accessEntries</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">
[]AccessEntry
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessEntries specifies the access entries for the cluster
Access entries require AuthenticationMode to be either &ldquo;api&rdquo; or &ldquo;api_and_config_map&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>vpcCni</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.VpcCni">
VpcCni
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VpcCni is used to set configuration options for the VPC CNI plugin</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapSelfManagedAddons</code><br/>
<em>
bool
</em>
</td>
<td>
<p>BootstrapSelfManagedAddons is used to set configuration options for
bare EKS cluster without EKS default networking addons
If you set this value to false when creating a cluster, the default networking add-ons will not be installed</p>
</td>
</tr>
<tr>
<td>
<code>restrictPrivateSubnets</code><br/>
<em>
bool
</em>
</td>
<td>
<p>RestrictPrivateSubnets indicates that the EKS control plane should only use private subnets.</p>
</td>
</tr>
<tr>
<td>
<code>kubeProxy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.KubeProxy">
KubeProxy
</a>
</em>
</td>
<td>
<p>KubeProxy defines managed attributes of the kube-proxy daemonset</p>
</td>
</tr>
<tr>
<td>
<code>upgradePolicy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UpgradePolicy">
UpgradePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The cluster upgrade policy to use for the cluster.
(Official AWS docs for this policy: <a href="https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html">https://docs.aws.amazon.com/eks/latest/userguide/view-upgrade-policy.html</a>)
<code>extended</code> upgrade policy indicates that the cluster will enter into extended support once the Kubernetes version reaches end of standard support. You will incur extended support charges with this setting. You can upgrade your cluster to a standard supported Kubernetes version to stop incurring extended support charges.
<code>standard</code> upgrade policy indicates that the cluster is eligible for automatic upgrade at the end of standard support. You will not incur extended support charges with this setting but your EKS cluster will automatically upgrade to the next supported Kubernetes version in standard support.
If omitted, new clusters will use the AWS default upgrade policy (which at the time of writing is &ldquo;extended&rdquo;) and existing clusters will have their upgrade policy unchanged.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateSpec">AWSManagedControlPlaneTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplate">AWSManagedControlPlaneTemplate</a>)
</p>
<p>
<p>AWSManagedControlPlaneTemplateSpec defines the desired state of AWSManagedControlPlaneTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneTemplateResource">
AWSManagedControlPlaneTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessConfig">AccessConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>AccessConfig represents the access configuration information for the cluster</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>authenticationMode</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.EKSAuthenticationMode">
EKSAuthenticationMode
</a>
</em>
</td>
<td>
<p>AuthenticationMode specifies the desired authentication mode for the cluster
Defaults to config_map</p>
</td>
</tr>
<tr>
<td>
<code>bootstrapClusterCreatorAdminPermissions</code><br/>
<em>
bool
</em>
</td>
<td>
<p>BootstrapClusterCreatorAdminPermissions grants cluster admin permissions
to the IAM identity creating the cluster. Only applied during creation,
ignored when updating existing clusters. Defaults to true.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">AccessEntry
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>AccessEntry represents an AWS EKS access entry for IAM principals</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>principalARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>PrincipalARN is the Amazon Resource Name (ARN) of the IAM principal</p>
</td>
</tr>
<tr>
<td>
<code>type</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntryType">
AccessEntryType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type is the type of access entry. Defaults to standard if not specified.</p>
</td>
</tr>
<tr>
<td>
<code>kubernetesGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>KubernetesGroups represents the Kubernetes groups for the access entry
Cannot be specified if Type is &ldquo;ec2_linux&rdquo; or &ldquo;ec2_windows&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>username</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Username is the username for the access entry</p>
</td>
</tr>
<tr>
<td>
<code>accessPolicies</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessPolicyReference">
[]AccessPolicyReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AccessPolicies specifies the policies to associate with this access entry
Cannot be specified if Type is &ldquo;ec2_linux&rdquo; or &ldquo;ec2_windows&rdquo;</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessEntryType">AccessEntryType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">AccessEntry</a>)
</p>
<p>
<p>AccessEntryType represents the different types of access entries that can be used in an Amazon EKS cluster</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessPolicyReference">AccessPolicyReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessEntry">AccessEntry</a>)
</p>
<p>
<p>AccessPolicyReference represents a reference to an AWS EKS access policy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>policyARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>PolicyARN is the Amazon Resource Name (ARN) of the access policy</p>
</td>
</tr>
<tr>
<td>
<code>accessScope</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessScope">
AccessScope
</a>
</em>
</td>
<td>
<p>AccessScope specifies the scope for the policy</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessScope">AccessScope
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessPolicyReference">AccessPolicyReference</a>)
</p>
<p>
<p>AccessScope represents the scope for an access policy</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessScopeType">
AccessScopeType
</a>
</em>
</td>
<td>
<p>Type is the type of access scope. Defaults to &ldquo;cluster&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>namespaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Namespaces are the namespaces for the access scope
Only valid when Type is namespace</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AccessScopeType">AccessScopeType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessScope">AccessScope</a>)
</p>
<p>
<p>AccessScopeType defines the scope type for an access policy</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.Addon">Addon
</h3>
<p>
<p>Addon represents a EKS addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the addon</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the version of the addon to use</p>
</td>
</tr>
<tr>
<td>
<code>configuration</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration of the EKS addon</p>
</td>
</tr>
<tr>
<td>
<code>conflictResolution</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AddonResolution">
AddonResolution
</a>
</em>
</td>
<td>
<p>ConflictResolution is used to declare what should happen if there
are parameter conflicts. Defaults to overwrite</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountRoleArn is the ARN of an IAM role to bind to the addons service account</p>
</td>
</tr>
<tr>
<td>
<code>preserveOnDelete</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreserveOnDelete indicates that the addon resources should be
preserved in the cluster on delete.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AddonIssue">AddonIssue
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AddonState">AddonState</a>)
</p>
<p>
<p>AddonIssue represents an issue with an addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>code</code><br/>
<em>
string
</em>
</td>
<td>
<p>Code is the issue code</p>
</td>
</tr>
<tr>
<td>
<code>message</code><br/>
<em>
string
</em>
</td>
<td>
<p>Message is the textual description of the issue</p>
</td>
</tr>
<tr>
<td>
<code>resourceIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>ResourceIDs is a list of resource ids for the issue</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AddonResolution">AddonResolution
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.Addon">Addon</a>)
</p>
<p>
<p>AddonResolution defines the method for resolving parameter conflicts.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AddonState">AddonState
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>AddonState represents the state of an addon.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the addon</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version is the version of the addon to use</p>
</td>
</tr>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN is the AWS ARN of the addon</p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>ServiceAccountRoleArn is the ARN of the IAM role used for the service account</p>
</td>
</tr>
<tr>
<td>
<code>createdAt</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>CreatedAt is the date and time the addon was created at</p>
</td>
</tr>
<tr>
<td>
<code>modifiedAt</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<p>ModifiedAt is the date and time the addon was last modified</p>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
string
</em>
</td>
<td>
<p>Status is the status of the addon</p>
</td>
</tr>
<tr>
<td>
<code>issues</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AddonIssue">
[]AddonIssue
</a>
</em>
</td>
<td>
<p>Issues is a list of issue associated with the addon</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AddonStatus">AddonStatus
(<code>string</code> alias)</p></h3>
<p>
<p>AddonStatus defines the status for an addon.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.ControlPlaneLoggingSpec">ControlPlaneLoggingSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>ControlPlaneLoggingSpec defines what EKS control plane logs that should be enabled.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>apiServer</code><br/>
<em>
bool
</em>
</td>
<td>
<p>APIServer indicates if the Kubernetes API Server log (kube-apiserver) shoulkd be enabled</p>
</td>
</tr>
<tr>
<td>
<code>audit</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Audit indicates if the Kubernetes API audit log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>authenticator</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Authenticator indicates if the iam authenticator log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>controllerManager</code><br/>
<em>
bool
</em>
</td>
<td>
<p>ControllerManager indicates if the controller manager (kube-controller-manager) log should be enabled</p>
</td>
</tr>
<tr>
<td>
<code>scheduler</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Scheduler indicates if the Kubernetes scheduler (kube-scheduler) log should be enabled</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.EKSAuthenticationMode">EKSAuthenticationMode
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AccessConfig">AccessConfig</a>)
</p>
<p>
<p>EKSAuthenticationMode defines the authentication mode for the cluster</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.EKSTokenMethod">EKSTokenMethod
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EKSTokenMethod defines the method for obtaining a client token to use when connecting to EKS.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.EncryptionConfig">EncryptionConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EncryptionConfig specifies the encryption configuration for the EKS clsuter.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>provider</code><br/>
<em>
string
</em>
</td>
<td>
<p>Provider specifies the ARN or alias of the CMK (in AWS KMS)</p>
</td>
</tr>
<tr>
<td>
<code>resources</code><br/>
<em>
[]*string
</em>
</td>
<td>
<p>Resources specifies the resources to be encrypted</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.EndpointAccess">EndpointAccess
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>EndpointAccess specifies how control plane endpoints are accessible.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>public</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Public controls whether control plane endpoints are publicly accessible</p>
</td>
</tr>
<tr>
<td>
<code>publicCIDRs</code><br/>
<em>
[]*string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicCIDRs specifies which blocks can access the public endpoint</p>
</td>
</tr>
<tr>
<td>
<code>private</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Private points VPC-internal control plane access to the private endpoint</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">IAMAuthenticatorConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>IAMAuthenticatorConfig represents an aws-iam-authenticator configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>mapRoles</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RoleMapping">
[]RoleMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleMappings is a list of role mappings</p>
</td>
</tr>
<tr>
<td>
<code>mapUsers</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UserMapping">
[]UserMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UserMappings is a list of user mappings</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.IdentityProviderStatus">IdentityProviderStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>IdentityProviderStatus holds the status for associated identity provider.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN holds the ARN of associated identity provider</p>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
string
</em>
</td>
<td>
<p>Status holds current status of associated identity provider</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.KubeProxy">KubeProxy
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>KubeProxy specifies how the kube-proxy daemonset is managed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable set to true indicates that kube-proxy should be disabled. With EKS clusters
kube-proxy is automatically installed into the cluster. For clusters where you want
to use kube-proxy functionality that is provided with an alternate CNI, this option
provides a way to specify that the kube-proxy daemonset should be deleted. You cannot
set this to true if you are using the Amazon kube-proxy addon.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.KubernetesMapping">KubernetesMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RoleMapping">RoleMapping</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UserMapping">UserMapping</a>)
</p>
<p>
<p>KubernetesMapping represents the kubernetes RBAC mapping.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>username</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserName is a kubernetes RBAC user subject</p>
</td>
</tr>
<tr>
<td>
<code>groups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Groups is a list of kubernetes RBAC groups</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.OIDCIdentityProviderConfig">OIDCIdentityProviderConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>OIDCIdentityProviderConfig represents the configuration for an OIDC identity provider.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clientId</code><br/>
<em>
string
</em>
</td>
<td>
<p>This is also known as audience. The ID for the client application that makes
authentication requests to the OpenID identity provider.</p>
</td>
</tr>
<tr>
<td>
<code>groupsClaim</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The JWT claim that the provider uses to return your groups.</p>
</td>
</tr>
<tr>
<td>
<code>groupsPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The prefix that is prepended to group claims to prevent clashes with existing
names (such as system: groups). For example, the valueoidc: will create group
names like oidc:engineering and oidc:infra.</p>
</td>
</tr>
<tr>
<td>
<code>identityProviderConfigName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the OIDC provider configuration.</p>
<p>IdentityProviderConfigName is a required field</p>
</td>
</tr>
<tr>
<td>
<code>issuerUrl</code><br/>
<em>
string
</em>
</td>
<td>
<p>The URL of the OpenID identity provider that allows the API server to discover
public signing keys for verifying tokens. The URL must begin with https://
and should correspond to the iss claim in the provider&rsquo;s OIDC ID tokens.
Per the OIDC standard, path components are allowed but query parameters are
not. Typically the URL consists of only a hostname, like <a href="https://server.example.org">https://server.example.org</a>
or <a href="https://example.com">https://example.com</a>. This URL should point to the level below .well-known/openid-configuration
and must be publicly accessible over the internet.</p>
</td>
</tr>
<tr>
<td>
<code>requiredClaims</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The key value pairs that describe required claims in the identity token.
If set, each claim is verified to be present in the token with a matching
value. For the maximum number of claims that you can require, see Amazon
EKS service quotas (<a href="https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html">https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html</a>)
in the Amazon EKS User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>usernameClaim</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The JSON Web Token (JWT) claim to use as the username. The default is sub,
which is expected to be a unique identifier of the end user. You can choose
other claims, such as email or name, depending on the OpenID identity provider.
Claims other than email are prefixed with the issuer URL to prevent naming
clashes with other plug-ins.</p>
</td>
</tr>
<tr>
<td>
<code>usernamePrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The prefix that is prepended to username claims to prevent clashes with existing
names. If you do not provide this field, and username is a value other than
email, the prefix defaults to issuerurl#. You can use the value - to disable
all prefixing.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>tags to apply to oidc identity provider association</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.OIDCProviderStatus">OIDCProviderStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>OIDCProviderStatus holds the status of the AWS OIDC identity provider.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN holds the ARN of the provider</p>
</td>
</tr>
<tr>
<td>
<code>trustPolicy</code><br/>
<em>
string
</em>
</td>
<td>
<p>TrustPolicy contains the boilerplate IAM trust policy to use for IRSA</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RoleMapping">RoleMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">IAMAuthenticatorConfig</a>)
</p>
<p>
<p>RoleMapping represents a mapping from a IAM role to Kubernetes users and groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rolearn</code><br/>
<em>
string
</em>
</td>
<td>
<p>RoleARN is the AWS ARN for the role to map</p>
</td>
</tr>
<tr>
<td>
<code>KubernetesMapping</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.KubernetesMapping">
KubernetesMapping
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubernetesMapping</code> are embedded into this type.)
</p>
<p>KubernetesMapping holds the RBAC details for the mapping</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.UpgradePolicy">UpgradePolicy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>UpgradePolicy defines the support policy to use for the cluster.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.UserMapping">UserMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.IAMAuthenticatorConfig">IAMAuthenticatorConfig</a>)
</p>
<p>
<p>UserMapping represents a mapping from an IAM user to Kubernetes users and groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>userarn</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserARN is the AWS ARN for the user to map</p>
</td>
</tr>
<tr>
<td>
<code>KubernetesMapping</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.KubernetesMapping">
KubernetesMapping
</a>
</em>
</td>
<td>
<p>
(Members of <code>KubernetesMapping</code> are embedded into this type.)
</p>
<p>KubernetesMapping holds the RBAC details for the mapping</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.VpcCni">VpcCni
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>VpcCni specifies configuration related to the VPC CNI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Disable indicates that the Amazon VPC CNI should be disabled. With EKS clusters the
Amazon VPC CNI is automatically installed into the cluster. For clusters where you want
to use an alternate CNI this option provides a way to specify that the Amazon VPC CNI
should be deleted. You cannot set this to true if you are using the
Amazon VPC CNI addon.</p>
</td>
</tr>
<tr>
<td>
<code>env</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#envvar-v1-core">
[]Kubernetes core/v1.EnvVar
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Env defines a list of environment variables to apply to the <code>aws-node</code> DaemonSet</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AWSRolesRef">AWSRolesRef
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigStatus">ROSARoleConfigStatus</a>)
</p>
<p>
<p>AWSRolesRef contains references to various AWS IAM roles required for operators to make calls against the AWS API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ingressARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>The referenced role must have a trust relationship that allows it to be assumed via web identity.
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a>.
Example:
{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Principal&rdquo;: {
&ldquo;Federated&rdquo;: &ldquo;{{ .ProviderARN }}&rdquo;
},
&ldquo;Action&rdquo;: &ldquo;sts:AssumeRoleWithWebIdentity&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;StringEquals&rdquo;: {
&ldquo;{{ .ProviderName }}:sub&rdquo;: {{ .ServiceAccounts }}
}
}
}
]
}</p>
<p>IngressARN is an ARN value referencing a role appropriate for the Ingress Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:aws:route53:::PUBLIC_ZONE_ID&rdquo;,
&ldquo;arn:aws:route53:::PRIVATE_ZONE_ID&rdquo;
]
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>imageRegistryARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageRegistryARN is an ARN value referencing a role appropriate for the Image Registry Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;s3:CreateBucket&rdquo;,
&ldquo;s3:DeleteBucket&rdquo;,
&ldquo;s3:PutBucketTagging&rdquo;,
&ldquo;s3:GetBucketTagging&rdquo;,
&ldquo;s3:PutBucketPublicAccessBlock&rdquo;,
&ldquo;s3:GetBucketPublicAccessBlock&rdquo;,
&ldquo;s3:PutEncryptionConfiguration&rdquo;,
&ldquo;s3:GetEncryptionConfiguration&rdquo;,
&ldquo;s3:PutLifecycleConfiguration&rdquo;,
&ldquo;s3:GetLifecycleConfiguration&rdquo;,
&ldquo;s3:GetBucketLocation&rdquo;,
&ldquo;s3:ListBucket&rdquo;,
&ldquo;s3:GetObject&rdquo;,
&ldquo;s3:PutObject&rdquo;,
&ldquo;s3:DeleteObject&rdquo;,
&ldquo;s3:ListBucketMultipartUploads&rdquo;,
&ldquo;s3:AbortMultipartUpload&rdquo;,
&ldquo;s3:ListMultipartUploadParts&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>storageARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>StorageARN is an ARN value referencing a role appropriate for the Storage Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:CreateSnapshot&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:DeleteSnapshot&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeSnapshots&rdquo;,
&ldquo;ec2:DescribeTags&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DescribeVolumesModifications&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>networkARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>NetworkARN is an ARN value referencing a role appropriate for the Network Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInstanceStatus&rdquo;,
&ldquo;ec2:DescribeInstanceTypes&rdquo;,
&ldquo;ec2:UnassignPrivateIpAddresses&rdquo;,
&ldquo;ec2:AssignPrivateIpAddresses&rdquo;,
&ldquo;ec2:UnassignIpv6Addresses&rdquo;,
&ldquo;ec2:AssignIpv6Addresses&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>kubeCloudControllerARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>KubeCloudControllerARN is an ARN value referencing a role appropriate for the KCM/KCC.
Source: <a href="https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies">https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies</a></p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;autoscaling:DescribeAutoScalingGroups&rdquo;,
&ldquo;autoscaling:DescribeLaunchConfigurations&rdquo;,
&ldquo;autoscaling:DescribeTags&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeRegions&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:CreateVolume&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyVolume&rdquo;,
&ldquo;ec2:AttachVolume&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:DeleteRoute&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteVolume&rdquo;,
&ldquo;ec2:DetachVolume&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:AttachLoadBalancerToSubnets&rdquo;,
&ldquo;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerPolicy&rdquo;,
&ldquo;elasticloadbalancing:CreateLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:ConfigureHealthCheck&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:DeleteLoadBalancerListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancers&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:DetachLoadBalancerFromSubnets&rdquo;,
&ldquo;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:ModifyLoadBalancerAttributes&rdquo;,
&ldquo;elasticloadbalancing:RegisterInstancesWithLoadBalancer&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&rdquo;,
&ldquo;elasticloadbalancing:AddTags&rdquo;,
&ldquo;elasticloadbalancing:CreateListener&rdquo;,
&ldquo;elasticloadbalancing:CreateTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeleteListener&rdquo;,
&ldquo;elasticloadbalancing:DeleteTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:DeregisterTargets&rdquo;,
&ldquo;elasticloadbalancing:DescribeListeners&rdquo;,
&ldquo;elasticloadbalancing:DescribeLoadBalancerPolicies&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetGroups&rdquo;,
&ldquo;elasticloadbalancing:DescribeTargetHealth&rdquo;,
&ldquo;elasticloadbalancing:ModifyListener&rdquo;,
&ldquo;elasticloadbalancing:ModifyTargetGroup&rdquo;,
&ldquo;elasticloadbalancing:RegisterTargets&rdquo;,
&ldquo;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&rdquo;,
&ldquo;iam:CreateServiceLinkedRole&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;*&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>nodePoolManagementARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>NodePoolManagementARN is an ARN value referencing a role appropriate for the CAPI Controller.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Action&rdquo;: [
&ldquo;ec2:AssociateRouteTable&rdquo;,
&ldquo;ec2:AttachInternetGateway&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:CreateInternetGateway&rdquo;,
&ldquo;ec2:CreateNatGateway&rdquo;,
&ldquo;ec2:CreateRoute&rdquo;,
&ldquo;ec2:CreateRouteTable&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:CreateSubnet&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;ec2:DeleteInternetGateway&rdquo;,
&ldquo;ec2:DeleteNatGateway&rdquo;,
&ldquo;ec2:DeleteRouteTable&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:DeleteSubnet&rdquo;,
&ldquo;ec2:DeleteTags&rdquo;,
&ldquo;ec2:DescribeAccountAttributes&rdquo;,
&ldquo;ec2:DescribeAddresses&rdquo;,
&ldquo;ec2:DescribeAvailabilityZones&rdquo;,
&ldquo;ec2:DescribeImages&rdquo;,
&ldquo;ec2:DescribeInstances&rdquo;,
&ldquo;ec2:DescribeInternetGateways&rdquo;,
&ldquo;ec2:DescribeNatGateways&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaces&rdquo;,
&ldquo;ec2:DescribeNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:DescribeRouteTables&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeSubnets&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
&ldquo;ec2:DescribeVpcAttribute&rdquo;,
&ldquo;ec2:DescribeVolumes&rdquo;,
&ldquo;ec2:DetachInternetGateway&rdquo;,
&ldquo;ec2:DisassociateRouteTable&rdquo;,
&ldquo;ec2:DisassociateAddress&rdquo;,
&ldquo;ec2:ModifyInstanceAttribute&rdquo;,
&ldquo;ec2:ModifyNetworkInterfaceAttribute&rdquo;,
&ldquo;ec2:ModifySubnetAttribute&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RunInstances&rdquo;,
&ldquo;ec2:TerminateInstances&rdquo;,
&ldquo;tag:GetResources&rdquo;,
&ldquo;ec2:CreateLaunchTemplate&rdquo;,
&ldquo;ec2:CreateLaunchTemplateVersion&rdquo;,
&ldquo;ec2:DescribeLaunchTemplates&rdquo;,
&ldquo;ec2:DescribeLaunchTemplateVersions&rdquo;,
&ldquo;ec2:DeleteLaunchTemplate&rdquo;,
&ldquo;ec2:DeleteLaunchTemplateVersions&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;<em>&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Condition&rdquo;: {
&ldquo;StringLike&rdquo;: {
&ldquo;iam:AWSServiceName&rdquo;: &ldquo;elasticloadbalancing.amazonaws.com&rdquo;
}
},
&ldquo;Action&rdquo;: [
&ldquo;iam:CreateServiceLinkedRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Action&rdquo;: [
&ldquo;iam:PassRole&rdquo;
],
&ldquo;Resource&rdquo;: [
&ldquo;arn:</em>:iam::<em>:role/</em>-worker-role&rdquo;
],
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:Decrypt&rdquo;,
&ldquo;kms:ReEncrypt&rdquo;,
&ldquo;kms:GenerateDataKeyWithoutPlainText&rdquo;,
&ldquo;kms:DescribeKey&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;<em>&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;kms:CreateGrant&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;</em>&rdquo;,
&ldquo;Condition&rdquo;: {
&ldquo;Bool&rdquo;: {
&ldquo;kms:GrantIsForAWSResource&rdquo;: true
}
}
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneOperatorARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneOperatorARN  is an ARN value referencing a role appropriate for the Control Plane Operator.</p>
<p>The following is an example of a valid policy document:</p>
<p>{
&ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;,
&ldquo;Statement&rdquo;: [
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;ec2:CreateVpcEndpoint&rdquo;,
&ldquo;ec2:DescribeVpcEndpoints&rdquo;,
&ldquo;ec2:ModifyVpcEndpoint&rdquo;,
&ldquo;ec2:DeleteVpcEndpoints&rdquo;,
&ldquo;ec2:CreateTags&rdquo;,
&ldquo;route53:ListHostedZones&rdquo;,
&ldquo;ec2:CreateSecurityGroup&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupIngress&rdquo;,
&ldquo;ec2:AuthorizeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DeleteSecurityGroup&rdquo;,
&ldquo;ec2:RevokeSecurityGroupIngress&rdquo;,
&ldquo;ec2:RevokeSecurityGroupEgress&rdquo;,
&ldquo;ec2:DescribeSecurityGroups&rdquo;,
&ldquo;ec2:DescribeVpcs&rdquo;,
],
&ldquo;Resource&rdquo;: &ldquo;*&rdquo;
},
{
&ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;,
&ldquo;Action&rdquo;: [
&ldquo;route53:ChangeResourceRecordSets&rdquo;,
&ldquo;route53:ListResourceRecordSets&rdquo;
],
&ldquo;Resource&rdquo;: &ldquo;arn:aws:route53:::%s&rdquo;
}
]
}</p>
</td>
</tr>
<tr>
<td>
<code>kmsProviderARN</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AutoNode">AutoNode
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>AutoNode set the AutoNode mode and AutoNode role ARN.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>mode</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoNodeMode">
AutoNodeMode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>mode specifies the mode for the AutoNode. Setting Enable/Disable mode will allows/disallow karpenter AutoNode scaling.</p>
</td>
</tr>
<tr>
<td>
<code>roleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>roleARN sets the autoNode role ARN, which includes the IAM policy and cluster-specific role that grant the necessary permissions to the Karpenter controller.
The role must be attached with the same OIDC-ID that is used with the ROSA-HCP cluster.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AutoNodeMode">AutoNodeMode
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoNode">AutoNode</a>)
</p>
<p>
<p>AutoNodeMode specifies the AutoNode mode for the ROSA Control Plane.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Disabled&#34;</p></td>
<td><p>AutoNodeModeDisabled Disabled AutoNode</p>
</td>
</tr><tr><td><p>&#34;Enabled&#34;</p></td>
<td><p>AutoNodeModeEnabled enable AutoNode</p>
</td>
</tr></tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.AutoScaling">AutoScaling
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.DefaultMachinePoolSpec">DefaultMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">RosaMachinePoolSpec</a>)
</p>
<p>
<p>AutoScaling specifies scaling options.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>minReplicas</code><br/>
<em>
int
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>maxReplicas</code><br/>
<em>
int
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.ChannelGroupType">ChannelGroupType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>ChannelGroupType specifies the OpenShift version channel group.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;candidate&#34;</p></td>
<td><p>Candidate channel group is for testing candidate builds.</p>
</td>
</tr><tr><td><p>&#34;eus&#34;</p></td>
<td><p>Eus channel group is for eus channel releases.</p>
</td>
</tr><tr><td><p>&#34;fast&#34;</p></td>
<td><p>Fast channel group is for fast channel releases.</p>
</td>
</tr><tr><td><p>&#34;nightly&#34;</p></td>
<td><p>Nightly channel group is for testing nigtly builds.</p>
</td>
</tr><tr><td><p>&#34;stable&#34;</p></td>
<td><p>Stable channel group is the default channel group for stable releases.</p>
</td>
</tr></tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.DefaultMachinePoolSpec">DefaultMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>DefaultMachinePoolSpec defines the configuration for the required worker nodes provisioned as part of the cluster creation.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The instance type to use, for example <code>r5.xlarge</code>. Instance type ref; <a href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a></p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoScaling">
AutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto scaling behaviour for the default MachinePool. Autoscaling min/max value
must be equal or multiple of the availability zones count.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSize</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>VolumeSize set the disk volume size for the default workers machine pool in Gib. The default is 300 GiB.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ExternalAuthProvider
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>ExternalAuthProvider is an external OIDC identity provider that can issue tokens for this cluster</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the OIDC provider</p>
</td>
</tr>
<tr>
<td>
<code>issuer</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenIssuer">
TokenIssuer
</a>
</em>
</td>
<td>
<p>Issuer describes attributes of the OIDC token issuer</p>
</td>
</tr>
<tr>
<td>
<code>oidcClients</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCClientConfig">
[]OIDCClientConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCClients contains configuration for the platform&rsquo;s clients that
need to request tokens from the issuer</p>
</td>
</tr>
<tr>
<td>
<code>claimMappings</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimMappings">
TokenClaimMappings
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClaimMappings describes rules on how to transform information from an
ID token into a cluster identity</p>
</td>
</tr>
<tr>
<td>
<code>claimValidationRules</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimValidationRule">
[]TokenClaimValidationRule
</a>
</em>
</td>
<td>
<p>ClaimValidationRules are rules that are applied to validate token claims to authenticate users.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.LocalObjectReference">LocalObjectReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCClientConfig">OIDCClientConfig</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenIssuer">TokenIssuer</a>)
</p>
<p>
<p>LocalObjectReference references an object in the same namespace.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the metadata.name of the referenced object.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>NetworkSpec for ROSA-HCP.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>machineCIDR</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IP addresses block used by OpenShift while installing the cluster, for example &ldquo;10.0.0.0/16&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>podCIDR</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IP address block from which to assign pod IP addresses, for example <code>10.128.0.0/14</code>.</p>
</td>
</tr>
<tr>
<td>
<code>serviceCIDR</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IP address block from which to assign service IP addresses, for example <code>172.30.0.0/16</code>.</p>
</td>
</tr>
<tr>
<td>
<code>hostPrefix</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network host prefix which is defaulted to <code>23</code> if not specified.</p>
</td>
</tr>
<tr>
<td>
<code>networkType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The CNI network type default is OVNKubernetes.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.OIDCClientConfig">OIDCClientConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ExternalAuthProvider</a>)
</p>
<p>
<p>OIDCClientConfig contains configuration for the platform&rsquo;s client that
need to request tokens from the issuer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>componentName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ComponentName is the name of the component that is supposed to consume this
client configuration</p>
</td>
</tr>
<tr>
<td>
<code>componentNamespace</code><br/>
<em>
string
</em>
</td>
<td>
<p>ComponentNamespace is the namespace of the component that is supposed to consume this
client configuration</p>
</td>
</tr>
<tr>
<td>
<code>clientID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClientID is the identifier of the OIDC client from the OIDC provider</p>
</td>
</tr>
<tr>
<td>
<code>clientSecret</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.LocalObjectReference">
LocalObjectReference
</a>
</em>
</td>
<td>
<p>ClientSecret refers to a secret that
contains the client secret in the <code>clientSecret</code> key of the <code>.data</code> field</p>
</td>
</tr>
<tr>
<td>
<code>extraScopes</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ExtraScopes is an optional set of scopes to request tokens with.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.PrefixedClaimMapping">PrefixedClaimMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimMappings">TokenClaimMappings</a>)
</p>
<p>
<p>PrefixedClaimMapping defines claims with a prefix.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>claim</code><br/>
<em>
string
</em>
</td>
<td>
<p>Claim is a JWT token claim to be used in the mapping</p>
</td>
</tr>
<tr>
<td>
<code>prefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>Prefix is a string to prefix the value from the token in the result of the
claim mapping.</p>
<p>By default, no prefixing occurs.</p>
<p>Example: if <code>prefix</code> is set to &ldquo;myoidc:&rdquo;&rdquo; and the <code>claim</code> in JWT contains
an array of strings &ldquo;a&rdquo;, &ldquo;b&rdquo; and  &ldquo;c&rdquo;, the mapping will result in an
array of string &ldquo;myoidc:a&rdquo;, &ldquo;myoidc:b&rdquo; and &ldquo;myoidc:c&rdquo;.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.ROSAControlPlane">ROSAControlPlane
</h3>
<p>
<p>ROSAControlPlane is the Schema for the ROSAControlPlanes API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">
RosaControlPlaneSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>rosaClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>Cluster name must be valid DNS-1035 label, so it must consist of lower case alphanumeric
characters or &lsquo;-&rsquo;, start with an alphabetic character, end with an alphanumeric character
and have a max length of 54 characters.</p>
</td>
</tr>
<tr>
<td>
<code>domainPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DomainPrefix is an optional prefix added to the cluster&rsquo;s domain name. It will be used
when generating a sub-domain for the cluster on openshiftapps domain. It must be valid DNS-1035 label
consisting of lower case alphanumeric characters or &lsquo;-&rsquo;, start with an alphabetic character
end with an alphanumeric character and have a max length of 15 characters.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The Subnet IDs to use when installing the cluster.
SubnetIDs should come in pairs; two per availability zone, one private and one public.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZones describe AWS AvailabilityZones of the worker nodes.
should match the AvailabilityZones of the provided Subnets.
a machinepool will be created for each availabilityZone.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>OpenShift semantic version, for example &ldquo;4.14.5&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>channelGroup</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ChannelGroupType">
ChannelGroupType
</a>
</em>
</td>
<td>
<p>OpenShift version channel group, default is stable.</p>
</td>
</tr>
<tr>
<td>
<code>versionGate</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.VersionGateAckType">
VersionGateAckType
</a>
</em>
</td>
<td>
<p>VersionGate requires acknowledgment when upgrading ROSA-HCP y-stream versions (e.g., from 4.15 to 4.16).
Default is WaitForAcknowledge.
WaitForAcknowledge: If acknowledgment is required, the upgrade will not proceed until VersionGate is set to Acknowledge or AlwaysAcknowledge.
Acknowledge: If acknowledgment is required, apply it for the upgrade. After upgrade is done set the version gate to WaitForAcknowledge.
AlwaysAcknowledge: If acknowledgment is required, apply it and proceed with the upgrade.</p>
</td>
</tr>
<tr>
<td>
<code>rosaRoleConfigRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RosaRoleConfigRef is a reference to a RosaRoleConfig resource that contains account roles, operator roles and OIDC configuration.
RosaRoleConfigRef and role fields such as installerRoleARN, supportRoleARN, workerRoleARN, rolesRef and oidcID are mutually exclusive.</p>
</td>
</tr>
<tr>
<td>
<code>rolesRef</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS IAM roles used to perform credential requests by the openshift operators.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>oidcID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ID of the internal OpenID Connect Provider.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>enableExternalAuthProviders</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableExternalAuthProviders enables external authentication configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>externalAuthProviders</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">
[]ExternalAuthProvider
</a>
</em>
</td>
<td>
<p>ExternalAuthProviders are external OIDC identity providers that can issue tokens for this cluster.
Can only be set if &ldquo;enableExternalAuthProviders&rdquo; is set to &ldquo;True&rdquo;.</p>
<p>At most one provider can be configured.</p>
</td>
</tr>
<tr>
<td>
<code>installerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstallerRoleARN is an AWS IAM role that OpenShift Cluster Manager will assume to create the cluster.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>supportRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SupportRoleARN is an AWS IAM role used by Red Hat SREs to enable
access to the cluster account in order to provide support.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>workerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>WorkerRoleARN is an AWS IAM role that will be attached to worker instances.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>billingAccount</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BillingAccount is an optional AWS account to use for billing the subscription fees for ROSA HCP clusters.
The cost of running each ROSA HCP cluster will be billed to the infrastructure account in which the cluster
is running.</p>
</td>
</tr>
<tr>
<td>
<code>defaultMachinePoolSpec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.DefaultMachinePoolSpec">
DefaultMachinePoolSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DefaultMachinePoolSpec defines the configuration for the default machinepool(s) provisioned as part of the cluster creation.
One MachinePool will be created with this configuration per AvailabilityZone. Those default machinepools are required for openshift cluster operators
to work properly.
As these machinepool not created using ROSAMachinePool CR, they will not be visible/managed by ROSA CAPI provider.
<code>rosa list machinepools -c &lt;rosaClusterName&gt;</code> can be used to view those machinepools.</p>
<p>This field will be removed in the future once the current limitation is resolved.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network config for the ROSA HCP cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaEndpointAccessType">
RosaEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EndpointAccess specifies the publishing scope of cluster endpoints. The
default is Public.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags are user-defined tags to be added on the AWS resources associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>etcdEncryptionKMSARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EtcdEncryptionKMSARN is the ARN of the KMS key used to encrypt etcd. The key itself needs to be
created out-of-band by the user and tagged with <code>red-hat:true</code>.</p>
</td>
</tr>
<tr>
<td>
<code>auditLogRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditLogRoleARN defines the role that is used to forward audit logs to AWS CloudWatch.
If not set, audit log forwarding is disabled.</p>
</td>
</tr>
<tr>
<td>
<code>provisionShardID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProvisionShardID defines the shard where ROSA hosted control plane components will be hosted.</p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CredentialsSecretRef references a secret with necessary credentials to connect to the OCM API.
The secret should contain the following data keys:
- ocmToken: eyJhbGciOiJIUzI1NiIsI&hellip;.
- ocmApiUrl: Optional, defaults to &lsquo;<a href="https://api.openshift.com'">https://api.openshift.com&rsquo;</a></p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>clusterRegistryConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistryConfig">
RegistryConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterRegistryConfig represents registry config used with the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoNode set the autoNode mode and roleARN.</p>
</td>
</tr>
<tr>
<td>
<code>rosaNetworkRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ROSANetworkRef references ROSANetwork custom resource that contains the networking infrastructure
for the ROSA HCP cluster.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneStatus">
RosaControlPlaneStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RegistryConfig">RegistryConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>RegistryConfig for ROSA-HCP cluster</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>additionalTrustedCAs</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTrustedCAs containing the registry hostname as the key, and the PEM-encoded certificate as the value,
for each additional registry CA to trust.</p>
</td>
</tr>
<tr>
<td>
<code>allowedRegistriesForImport</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistryLocation">
[]RegistryLocation
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedRegistriesForImport limits the container image registries that normal users may import
images from. Set this list to the registries that you trust to contain valid Docker
images and that you want applications to be able to import from.</p>
</td>
</tr>
<tr>
<td>
<code>registrySources</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistrySources">
RegistrySources
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RegistrySources contains configuration that determines how the container runtime
should treat individual registries when accessing images. It does not contain configuration
for the internal cluster registry. AllowedRegistries, BlockedRegistries are mutually exclusive.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RegistryLocation">RegistryLocation
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistryConfig">RegistryConfig</a>)
</p>
<p>
<p>RegistryLocation contains a location of the registry specified by the registry domain name.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>domainName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>domainName specifies a domain name for the registry. The domain name might include wildcards, like &lsquo;*&rsquo; or &lsquo;??&rsquo;.
In case the registry use non-standard (80 or 443) port, the port should be included in the domain name as well.</p>
</td>
</tr>
<tr>
<td>
<code>insecure</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>insecure indicates whether the registry is secure (https) or insecure (http), default is secured.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RegistrySources">RegistrySources
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistryConfig">RegistryConfig</a>)
</p>
<p>
<p>RegistrySources contains registries configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>allowedRegistries</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedRegistries are the registries for which image pull and push actions are allowed.
To specify all subdomains, add the asterisk (*) wildcard character as a prefix to the domain name,
For example, *.example.com.
You can specify an individual repository within a registry, For example: reg1.io/myrepo/myapp:latest.
All other registries are blocked.</p>
</td>
</tr>
<tr>
<td>
<code>blockedRegistries</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BlockedRegistries are the registries for which image pull and push actions are denied.
To specify all subdomains, add the asterisk (*) wildcard character as a prefix to the domain name,
For example, *.example.com.
You can specify an individual repository within a registry, For example: reg1.io/myrepo/myapp:latest.
All other registries are allowed.</p>
</td>
</tr>
<tr>
<td>
<code>insecureRegistries</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InsecureRegistries are registries which do not have a valid TLS certificate or only support HTTP connections.
To specify all subdomains, add the asterisk (*) wildcard character as a prefix to the domain name,
For example, *.example.com.
You can specify an individual repository within a registry, For example: reg1.io/myrepo/myapp:latest.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ROSAControlPlane">ROSAControlPlane</a>)
</p>
<p>
<p>RosaControlPlaneSpec defines the desired state of ROSAControlPlane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rosaClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>Cluster name must be valid DNS-1035 label, so it must consist of lower case alphanumeric
characters or &lsquo;-&rsquo;, start with an alphabetic character, end with an alphanumeric character
and have a max length of 54 characters.</p>
</td>
</tr>
<tr>
<td>
<code>domainPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DomainPrefix is an optional prefix added to the cluster&rsquo;s domain name. It will be used
when generating a sub-domain for the cluster on openshiftapps domain. It must be valid DNS-1035 label
consisting of lower case alphanumeric characters or &lsquo;-&rsquo;, start with an alphabetic character
end with an alphanumeric character and have a max length of 15 characters.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The Subnet IDs to use when installing the cluster.
SubnetIDs should come in pairs; two per availability zone, one private and one public.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZones describe AWS AvailabilityZones of the worker nodes.
should match the AvailabilityZones of the provided Subnets.
a machinepool will be created for each availabilityZone.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>OpenShift semantic version, for example &ldquo;4.14.5&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>channelGroup</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ChannelGroupType">
ChannelGroupType
</a>
</em>
</td>
<td>
<p>OpenShift version channel group, default is stable.</p>
</td>
</tr>
<tr>
<td>
<code>versionGate</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.VersionGateAckType">
VersionGateAckType
</a>
</em>
</td>
<td>
<p>VersionGate requires acknowledgment when upgrading ROSA-HCP y-stream versions (e.g., from 4.15 to 4.16).
Default is WaitForAcknowledge.
WaitForAcknowledge: If acknowledgment is required, the upgrade will not proceed until VersionGate is set to Acknowledge or AlwaysAcknowledge.
Acknowledge: If acknowledgment is required, apply it for the upgrade. After upgrade is done set the version gate to WaitForAcknowledge.
AlwaysAcknowledge: If acknowledgment is required, apply it and proceed with the upgrade.</p>
</td>
</tr>
<tr>
<td>
<code>rosaRoleConfigRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RosaRoleConfigRef is a reference to a RosaRoleConfig resource that contains account roles, operator roles and OIDC configuration.
RosaRoleConfigRef and role fields such as installerRoleARN, supportRoleARN, workerRoleARN, rolesRef and oidcID are mutually exclusive.</p>
</td>
</tr>
<tr>
<td>
<code>rolesRef</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWS IAM roles used to perform credential requests by the openshift operators.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>oidcID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ID of the internal OpenID Connect Provider.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>enableExternalAuthProviders</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableExternalAuthProviders enables external authentication configuration for the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>externalAuthProviders</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">
[]ExternalAuthProvider
</a>
</em>
</td>
<td>
<p>ExternalAuthProviders are external OIDC identity providers that can issue tokens for this cluster.
Can only be set if &ldquo;enableExternalAuthProviders&rdquo; is set to &ldquo;True&rdquo;.</p>
<p>At most one provider can be configured.</p>
</td>
</tr>
<tr>
<td>
<code>installerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstallerRoleARN is an AWS IAM role that OpenShift Cluster Manager will assume to create the cluster.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>supportRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SupportRoleARN is an AWS IAM role used by Red Hat SREs to enable
access to the cluster account in order to provide support.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>workerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>WorkerRoleARN is an AWS IAM role that will be attached to worker instances.
Required if RosaRoleConfigRef is not specified.</p>
</td>
</tr>
<tr>
<td>
<code>billingAccount</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>BillingAccount is an optional AWS account to use for billing the subscription fees for ROSA HCP clusters.
The cost of running each ROSA HCP cluster will be billed to the infrastructure account in which the cluster
is running.</p>
</td>
</tr>
<tr>
<td>
<code>defaultMachinePoolSpec</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.DefaultMachinePoolSpec">
DefaultMachinePoolSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DefaultMachinePoolSpec defines the configuration for the default machinepool(s) provisioned as part of the cluster creation.
One MachinePool will be created with this configuration per AvailabilityZone. Those default machinepools are required for openshift cluster operators
to work properly.
As these machinepool not created using ROSAMachinePool CR, they will not be visible/managed by ROSA CAPI provider.
<code>rosa list machinepools -c &lt;rosaClusterName&gt;</code> can be used to view those machinepools.</p>
<p>This field will be removed in the future once the current limitation is resolved.</p>
</td>
</tr>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Network config for the ROSA HCP cluster.</p>
</td>
</tr>
<tr>
<td>
<code>endpointAccess</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaEndpointAccessType">
RosaEndpointAccessType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EndpointAccess specifies the publishing scope of cluster endpoints. The
default is Public.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags are user-defined tags to be added on the AWS resources associated with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>etcdEncryptionKMSARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EtcdEncryptionKMSARN is the ARN of the KMS key used to encrypt etcd. The key itself needs to be
created out-of-band by the user and tagged with <code>red-hat:true</code>.</p>
</td>
</tr>
<tr>
<td>
<code>auditLogRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AuditLogRoleARN defines the role that is used to forward audit logs to AWS CloudWatch.
If not set, audit log forwarding is disabled.</p>
</td>
</tr>
<tr>
<td>
<code>provisionShardID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProvisionShardID defines the shard where ROSA hosted control plane components will be hosted.</p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CredentialsSecretRef references a secret with necessary credentials to connect to the OCM API.
The secret should contain the following data keys:
- ocmToken: eyJhbGciOiJIUzI1NiIsI&hellip;.
- ocmApiUrl: Optional, defaults to &lsquo;<a href="https://api.openshift.com'">https://api.openshift.com&rsquo;</a></p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>clusterRegistryConfig</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RegistryConfig">
RegistryConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ClusterRegistryConfig represents registry config used with the cluster.</p>
</td>
</tr>
<tr>
<td>
<code>autoNode</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoNode">
AutoNode
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>autoNode set the autoNode mode and roleARN.</p>
</td>
</tr>
<tr>
<td>
<code>rosaNetworkRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ROSANetworkRef references ROSANetwork custom resource that contains the networking infrastructure
for the ROSA HCP cluster.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneStatus">RosaControlPlaneStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ROSAControlPlane">ROSAControlPlane</a>)
</p>
<p>
<p>RosaControlPlaneStatus defines the observed state of ROSAControlPlane.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>externalManagedControlPlane</code><br/>
<em>
bool
</em>
</td>
<td>
<p>ExternalManagedControlPlane indicates to cluster-api that the control plane
is managed by an external service such as AKS, EKS, GKE, etc.</p>
</td>
</tr>
<tr>
<td>
<code>initialized</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Initialized denotes whether or not the control plane has the
uploaded kubernetes config-map.</p>
</td>
</tr>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the ROSAControlPlane API Server is ready to receive requests.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the state and will be set to a descriptive error message.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the spec or the configuration of
the controller, and that manual intervention is required.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<p>Conditions specifies the conditions for the managed control plane</p>
</td>
</tr>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is the cluster ID given by ROSA.</p>
</td>
</tr>
<tr>
<td>
<code>consoleURL</code><br/>
<em>
string
</em>
</td>
<td>
<p>ConsoleURL is the url for the openshift console.</p>
</td>
</tr>
<tr>
<td>
<code>oidcEndpointURL</code><br/>
<em>
string
</em>
</td>
<td>
<p>OIDCEndpointURL is the endpoint url for the managed OIDC provider.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>OpenShift semantic version, for example &ldquo;4.14.5&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>availableUpgrades</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Available upgrades for the ROSA hosted control plane.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.RosaEndpointAccessType">RosaEndpointAccessType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>RosaEndpointAccessType specifies the publishing scope of cluster endpoints.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Private&#34;</p></td>
<td><p>Private endpoint access allows only private API server access and private
node communication with the control plane.</p>
</td>
</tr><tr><td><p>&#34;Public&#34;</p></td>
<td><p>Public endpoint access allows public API server access and
private node communication with the control plane.</p>
</td>
</tr></tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenAudience">TokenAudience
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenIssuer">TokenIssuer</a>)
</p>
<p>
<p>TokenAudience is the audience that the token was issued for.</p>
</p>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenClaimMappings">TokenClaimMappings
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ExternalAuthProvider</a>)
</p>
<p>
<p>TokenClaimMappings describes rules on how to transform information from an
ID token into a cluster identity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>username</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UsernameClaimMapping">
UsernameClaimMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Username is a name of the claim that should be used to construct
usernames for the cluster identity.</p>
<p>Default value: &ldquo;sub&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>groups</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.PrefixedClaimMapping">
PrefixedClaimMapping
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Groups is a name of the claim that should be used to construct
groups for the cluster identity.
The referenced claim must use array of strings values.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenClaimValidationRule">TokenClaimValidationRule
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ExternalAuthProvider</a>)
</p>
<p>
<p>TokenClaimValidationRule validates token claims to authenticate users.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenValidationRuleType">
TokenValidationRuleType
</a>
</em>
</td>
<td>
<p>Type sets the type of the validation rule</p>
</td>
</tr>
<tr>
<td>
<code>requiredClaim</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenRequiredClaim">
TokenRequiredClaim
</a>
</em>
</td>
<td>
<p>RequiredClaim allows configuring a required claim name and its expected value</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenIssuer">TokenIssuer
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.ExternalAuthProvider">ExternalAuthProvider</a>)
</p>
<p>
<p>TokenIssuer describes attributes of the OIDC token issuer</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>issuerURL</code><br/>
<em>
string
</em>
</td>
<td>
<p>URL is the serving URL of the token issuer.
Must use the https:// scheme.</p>
</td>
</tr>
<tr>
<td>
<code>audiences</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenAudience">
[]TokenAudience
</a>
</em>
</td>
<td>
<p>Audiences is an array of audiences that the token was issued for.
Valid tokens must include at least one of these values in their
&ldquo;aud&rdquo; claim.
Must be set to exactly one value.</p>
</td>
</tr>
<tr>
<td>
<code>issuerCertificateAuthority</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.LocalObjectReference">
LocalObjectReference
</a>
</em>
</td>
<td>
<p>CertificateAuthority is a reference to a config map in the
configuration namespace. The .data of the configMap must contain
the &ldquo;ca-bundle.crt&rdquo; key.
If unset, system trust is used instead.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenRequiredClaim">TokenRequiredClaim
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimValidationRule">TokenClaimValidationRule</a>)
</p>
<p>
<p>TokenRequiredClaim allows configuring a required claim name and its expected value.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>claim</code><br/>
<em>
string
</em>
</td>
<td>
<p>Claim is a name of a required claim. Only claims with string values are
supported.</p>
</td>
</tr>
<tr>
<td>
<code>requiredValue</code><br/>
<em>
string
</em>
</td>
<td>
<p>RequiredValue is the required value for the claim.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.TokenValidationRuleType">TokenValidationRuleType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimValidationRule">TokenClaimValidationRule</a>)
</p>
<p>
<p>TokenValidationRuleType defines the type of the validation rule.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;RequiredClaim&#34;</p></td>
<td><p>TokenValidationRuleTypeRequiredClaim defines the type for RequiredClaim.</p>
</td>
</tr></tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.UsernameClaimMapping">UsernameClaimMapping
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.TokenClaimMappings">TokenClaimMappings</a>)
</p>
<p>
<p>UsernameClaimMapping defines the claim that should be used to construct usernames for the cluster identity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>claim</code><br/>
<em>
string
</em>
</td>
<td>
<p>Claim is a JWT token claim to be used in the mapping</p>
</td>
</tr>
<tr>
<td>
<code>prefixPolicy</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UsernamePrefixPolicy">
UsernamePrefixPolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrefixPolicy specifies how a prefix should apply.</p>
<p>By default, claims other than <code>email</code> will be prefixed with the issuer URL to
prevent naming clashes with other plugins.</p>
<p>Set to &ldquo;NoPrefix&rdquo; to disable prefixing.</p>
<p>Example:
(1) <code>prefix</code> is set to &ldquo;myoidc:&rdquo; and <code>claim</code> is set to &ldquo;username&rdquo;.
If the JWT claim <code>username</code> contains value <code>userA</code>, the resulting
mapped value will be &ldquo;myoidc:userA&rdquo;.
(2) <code>prefix</code> is set to &ldquo;myoidc:&rdquo; and <code>claim</code> is set to &ldquo;email&rdquo;. If the
JWT <code>email</code> claim contains value &ldquo;userA@myoidc.tld&rdquo;, the resulting
mapped value will be &ldquo;myoidc:userA@myoidc.tld&rdquo;.
(3) <code>prefix</code> is unset, <code>issuerURL</code> is set to <code>https://myoidc.tld</code>,
the JWT claims include &ldquo;username&rdquo;:&ldquo;userA&rdquo; and &ldquo;email&rdquo;:&ldquo;userA@myoidc.tld&rdquo;,
and <code>claim</code> is set to:
(a) &ldquo;username&rdquo;: the mapped value will be &ldquo;<a href="https://myoidc.tld#userA&quot;">https://myoidc.tld#userA&rdquo;</a>
(b) &ldquo;email&rdquo;: the mapped value will be &ldquo;userA@myoidc.tld&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>prefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Prefix is prepended to claim to prevent clashes with existing names.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.UsernamePrefixPolicy">UsernamePrefixPolicy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.UsernameClaimMapping">UsernameClaimMapping</a>)
</p>
<p>
<p>UsernamePrefixPolicy specifies how a prefix should apply.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;&#34;</p></td>
<td><p>NoOpinion let&rsquo;s the cluster assign prefixes.  If the username claim is email, there is no prefix
If the username claim is anything else, it is prefixed by the issuerURL</p>
</td>
</tr><tr><td><p>&#34;NoPrefix&#34;</p></td>
<td><p>NoPrefix means the username claim value will not have any  prefix</p>
</td>
</tr><tr><td><p>&#34;Prefix&#34;</p></td>
<td><p>Prefix means the prefix value must be specified.  It cannot be empty</p>
</td>
</tr></tbody>
</table>
<h3 id="controlplane.cluster.x-k8s.io/v1beta2.VersionGateAckType">VersionGateAckType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>)
</p>
<p>
<p>VersionGateAckType specifies the version gate acknowledgment.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Acknowledge&#34;</p></td>
<td><p>Acknowledge if acknowledgment is required and proceed with the upgrade.</p>
</td>
</tr><tr><td><p>&#34;AlwaysAcknowledge&#34;</p></td>
<td><p>AlwaysAcknowledge always acknowledg if required and proceed with the upgrade.</p>
</td>
</tr><tr><td><p>&#34;WaitForAcknowledge&#34;</p></td>
<td><p>WaitForAcknowledge if acknowledgment is required, wait not to proceed with the upgrade.</p>
</td>
</tr></tbody>
</table>
<hr/>
<h2 id="infrastructure.cluster.x-k8s.io/v1beta1">infrastructure.cluster.x-k8s.io/v1beta1</h2>
<p>
<p>Package v1beta1 contains the v1beta1 API implementation.</p>
</p>
Resource Types:
<ul></ul>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AMIReference">AMIReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>)
</p>
<p>
<p>AMIReference is a reference to a specific AWS resource by ID, ARN, or filters.
Only one of ID, ARN or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>eksLookupType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.EKSAMILookupType">
EKSAMILookupType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSOptimizedLookupType If specified, will look up an EKS Optimized image in SSM Parameter store</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSCluster">AWSCluster
</h3>
<p>
<p>AWSCluster is the schema for Amazon EC2 based Kubernetes Cluster API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">
AWSClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStatus">
AWSClusterStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterControllerIdentity">AWSClusterControllerIdentity
</h3>
<p>
<p>AWSClusterControllerIdentity is the Schema for the awsclustercontrolleridentities API
It is used to grant access to use Cluster API Provider AWS Controller credentials.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterControllerIdentitySpec">
AWSClusterControllerIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterControllerIdentity.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterControllerIdentitySpec">AWSClusterControllerIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterControllerIdentity">AWSClusterControllerIdentity</a>)
</p>
<p>
<p>AWSClusterControllerIdentitySpec defines the specifications for AWSClusterControllerIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">AWSClusterIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterControllerIdentitySpec">AWSClusterControllerIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStaticIdentitySpec">AWSClusterStaticIdentitySpec</a>)
</p>
<p>
<p>AWSClusterIdentitySpec defines the Spec struct for AWSClusterIdentity types.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>allowedNamespaces</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AllowedNamespaces">
AllowedNamespaces
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedNamespaces is used to identify which namespaces are allowed to use the identity from.
Namespaces can be selected either using an array of namespaces or with label selector.
An empty allowedNamespaces object indicates that AWSClusters can use this identity from any namespace.
If this object is nil, no namespaces will be allowed (default behaviour, if this field is not provided)
A namespace should be either in the NamespaceList or match with Selector to use the identity.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentity">AWSClusterRoleIdentity
</h3>
<p>
<p>AWSClusterRoleIdentity is the Schema for the awsclusterroleidentities API
It is used to assume a role using the provided sourceRef.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentitySpec">
AWSClusterRoleIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterRoleIdentity.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>AWSRoleSpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSRoleSpec">
AWSRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>externalID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A unique identifier that might be required when you assume a role in another account.
If the administrator of the account to which the role belongs provided you with an
external ID, then provide that value in the ExternalId parameter. This value can be
any string, such as a passphrase or account number. A cross-account role is usually
set up to trust everyone in an account. Therefore, the administrator of the trusting
account might send an external ID to the administrator of the trusted account. That
way, only someone with the ID can assume the role, rather than everyone in the
account. For more information about the external ID, see How to Use an External ID
When Granting Access to Your AWS Resources to a Third Party in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>sourceIdentityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>SourceIdentityRef is a reference to another identity which will be chained to do
role assumption. All identity types are accepted.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentity">AWSClusterRoleIdentity</a>)
</p>
<p>
<p>AWSClusterRoleIdentitySpec defines the specifications for AWSClusterRoleIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>AWSRoleSpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSRoleSpec">
AWSRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>externalID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A unique identifier that might be required when you assume a role in another account.
If the administrator of the account to which the role belongs provided you with an
external ID, then provide that value in the ExternalId parameter. This value can be
any string, such as a passphrase or account number. A cross-account role is usually
set up to trust everyone in an account. Therefore, the administrator of the trusting
account might send an external ID to the administrator of the trusted account. That
way, only someone with the ID can assume the role, rather than everyone in the
account. For more information about the external ID, see How to Use an External ID
When Granting Access to Your AWS Resources to a Third Party in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>sourceIdentityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>SourceIdentityRef is a reference to another identity which will be chained to do
role assumption. All identity types are accepted.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSCluster">AWSCluster</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateResource">AWSClusterTemplateResource</a>)
</p>
<p>
<p>AWSClusterSpec defines the desired state of an EC2-based Kubernetes cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStaticIdentity">AWSClusterStaticIdentity
</h3>
<p>
<p>AWSClusterStaticIdentity is the Schema for the awsclusterstaticidentities API
It represents a reference to an AWS access key ID and secret access key, stored in a secret.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStaticIdentitySpec">
AWSClusterStaticIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterStaticIdentity</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>secretRef</code><br/>
<em>
string
</em>
</td>
<td>
<p>Reference to a secret containing the credentials. The secret should
contain the following data keys:
AccessKeyID: AKIAIOSFODNN7EXAMPLE
SecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
SessionToken: Optional</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStaticIdentitySpec">AWSClusterStaticIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStaticIdentity">AWSClusterStaticIdentity</a>)
</p>
<p>
<p>AWSClusterStaticIdentitySpec defines the specifications for AWSClusterStaticIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>secretRef</code><br/>
<em>
string
</em>
</td>
<td>
<p>Reference to a secret containing the credentials. The secret should
contain the following data keys:
AccessKeyID: AKIAIOSFODNN7EXAMPLE
SecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
SessionToken: Optional</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStatus">AWSClusterStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSCluster">AWSCluster</a>)
</p>
<p>
<p>AWSClusterStatus defines the observed state of AWSCluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>networkStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkStatus">
NetworkStatus
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Instance">
Instance
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplate">AWSClusterTemplate
</h3>
<p>
<p>AWSClusterTemplate is the schema for Amazon EC2 based Kubernetes Cluster Templates.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateSpec">
AWSClusterTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateResource">
AWSClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateResource">AWSClusterTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateSpec">AWSClusterTemplateSpec</a>)
</p>
<p>
<p>AWSClusterTemplateResource defines the desired state of AWSClusterTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
Cluster API api/core/v1beta1.ObjectMeta
</em>
</td>
<td>
<em>(Optional)</em>
<p>Standard object&rsquo;s metadata.
More info: <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">
AWSClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateSpec">AWSClusterTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplate">AWSClusterTemplate</a>)
</p>
<p>
<p>AWSClusterTemplateSpec defines the desired state of AWSClusterTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterTemplateResource">
AWSClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityKind">AWSIdentityKind
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">AWSIdentityReference</a>)
</p>
<p>
<p>AWSIdentityKind defines allowed AWS identity types.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityReference">AWSIdentityReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>AWSIdentityReference specifies a identity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the identity.</p>
</td>
</tr>
<tr>
<td>
<code>kind</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSIdentityKind">
AWSIdentityKind
</a>
</em>
</td>
<td>
<p>Kind of the identity.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">AWSLoadBalancerSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>AWSLoadBalancerSpec defines the desired state of an AWS load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name sets the name of the classic ELB load balancer. As per AWS, the name must be unique
within your set of load balancers for the region, must have a maximum of 32 characters, must
contain only alphanumeric characters or hyphens, and cannot begin or end with a hyphen. Once
set, the value cannot be changed.</p>
</td>
</tr>
<tr>
<td>
<code>scheme</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBScheme">
ClassicELBScheme
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scheme sets the scheme of the load balancer (defaults to internet-facing)</p>
</td>
</tr>
<tr>
<td>
<code>crossZoneLoadBalancing</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>CrossZoneLoadBalancing enables the classic ELB cross availability zone balancing.</p>
<p>With cross-zone load balancing, each load balancer node for your Classic Load Balancer
distributes requests evenly across the registered instances in all enabled Availability Zones.
If cross-zone load balancing is disabled, each load balancer node distributes requests evenly across
the registered instances in its Availability Zone only.</p>
<p>Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets sets the subnets that should be applied to the control plane load balancer (defaults to discovered subnets for managed VPCs or an empty set for unmanaged VPCs)</p>
</td>
</tr>
<tr>
<td>
<code>healthCheckProtocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBProtocol">
ClassicELBProtocol
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>HealthCheckProtocol sets the protocol type for classic ELB health check target
default value is ClassicELBProtocolSSL</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups sets the security groups used by the load balancer. Expected to be security group IDs
This is optional - if not provided new security groups will be created for the load balancer</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachine">AWSMachine
</h3>
<p>
<p>AWSMachine is the schema for Amazon EC2 machines.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">
AWSMachineSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>failureDomain</code><br/>
<em>
string
</em>
</td>
<td>
<p>FailureDomain is the failure domain unique identifier this Machine should be attached to, as defined in Cluster API.
For this infrastructure provider, the ID is equivalent to an AWS Availability Zone.
If multiple subnets are matched for the availability zone, the first one returned is picked.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineStatus">
AWSMachineStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineProviderConditionType">AWSMachineProviderConditionType
(<code>string</code> alias)</p></h3>
<p>
<p>AWSMachineProviderConditionType is a valid value for AWSMachineProviderCondition.Type.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachine">AWSMachine</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateResource">AWSMachineTemplateResource</a>)
</p>
<p>
<p>AWSMachineSpec defines the desired state of an Amazon EC2 instance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>failureDomain</code><br/>
<em>
string
</em>
</td>
<td>
<p>FailureDomain is the failure domain unique identifier this Machine should be attached to, as defined in Cluster API.
For this infrastructure provider, the ID is equivalent to an AWS Availability Zone.
If multiple subnets are matched for the availability zone, the first one returned is picked.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineStatus">AWSMachineStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachine">AWSMachine</a>)
</p>
<p>
<p>AWSMachineStatus defines the observed state of AWSMachine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is true when the provider resource is ready.</p>
</td>
</tr>
<tr>
<td>
<code>interruptible</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Interruptible reports that this machine is using spot instances and can therefore be interrupted by CAPI when it receives a notice that the spot instance is to be terminated by AWS.
This will be set to true when SpotMarketOptions is not nil (i.e. this machine is using a spot instance).</p>
</td>
</tr>
<tr>
<td>
<code>addresses</code><br/>
<em>
[]Cluster API api/core/v1beta1.MachineAddress
</em>
</td>
<td>
<p>Addresses contains the AWS instance associated addresses.</p>
</td>
</tr>
<tr>
<td>
<code>instanceState</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.InstanceState">
InstanceState
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceState is the state of the AWS instance for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the Machine and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the Machine and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSMachine.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplate">AWSMachineTemplate
</h3>
<p>
<p>AWSMachineTemplate is the schema for the Amazon EC2 Machine Templates API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateSpec">
AWSMachineTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateResource">
AWSMachineTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateStatus">
AWSMachineTemplateStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateResource">AWSMachineTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateSpec">AWSMachineTemplateSpec</a>)
</p>
<p>
<p>AWSMachineTemplateResource describes the data needed to create am AWSMachine from a template.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
Cluster API api/core/v1beta1.ObjectMeta
</em>
</td>
<td>
<em>(Optional)</em>
<p>Standard object&rsquo;s metadata.
More info: <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">
AWSMachineSpec
</a>
</em>
</td>
<td>
<p>Spec is the specification of the desired behavior of the machine.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>failureDomain</code><br/>
<em>
string
</em>
</td>
<td>
<p>FailureDomain is the failure domain unique identifier this Machine should be attached to, as defined in Cluster API.
For this infrastructure provider, the ID is equivalent to an AWS Availability Zone.
If multiple subnets are matched for the availability zone, the first one returned is picked.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateSpec">AWSMachineTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplate">AWSMachineTemplate</a>)
</p>
<p>
<p>AWSMachineTemplateSpec defines the desired state of AWSMachineTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateResource">
AWSMachineTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplateStatus">AWSMachineTemplateStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineTemplate">AWSMachineTemplate</a>)
</p>
<p>
<p>AWSMachineTemplateStatus defines a status for an AWSMachineTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>capacity</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#resourcelist-v1-core">
Kubernetes core/v1.ResourceList
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Capacity defines the resource capacity for this machine.
This value is used for autoscaling from zero operations as defined in:
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md">https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md</a></p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">AWSResourceReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>)
</p>
<p>
<p>AWSResourceReference is a reference to a specific AWS resource by ID or filters.
Only one of ID or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ARN of resource.</p>
<p>Deprecated: This field has no function and is going to be removed in the next release.</p>
</td>
</tr>
<tr>
<td>
<code>filters</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Filter">
[]Filter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filters is a set of key/value pairs used to identify a resource
They are applied according to the rules defined by the AWS API:
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</a></p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSRoleSpec">AWSRoleSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>)
</p>
<p>
<p>AWSRoleSpec defines the specifications for all identities based around AWS roles.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>roleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>The Amazon Resource Name (ARN) of the role to assume.</p>
</td>
</tr>
<tr>
<td>
<code>sessionName</code><br/>
<em>
string
</em>
</td>
<td>
<p>An identifier for the assumed role session</p>
</td>
</tr>
<tr>
<td>
<code>durationSeconds</code><br/>
<em>
int32
</em>
</td>
<td>
<p>The duration, in seconds, of the role session before it is renewed.</p>
</td>
</tr>
<tr>
<td>
<code>inlinePolicy</code><br/>
<em>
string
</em>
</td>
<td>
<p>An IAM policy as a JSON-encoded string that you want to use as an inline session policy.</p>
</td>
</tr>
<tr>
<td>
<code>policyARNs</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>The Amazon Resource Names (ARNs) of the IAM managed policies that you want
to use as managed session policies.
The policies must exist in the same account as the role.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AZSelectionScheme">AZSelectionScheme
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>AZSelectionScheme defines the scheme of selecting AZs.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AllowedNamespaces">AllowedNamespaces
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterIdentitySpec">AWSClusterIdentitySpec</a>)
</p>
<p>
<p>AllowedNamespaces is a selector of namespaces that AWSClusters can
use this ClusterPrincipal from. This is a standard Kubernetes LabelSelector,
a label query over a set of resources. The result of matchLabels and
matchExpressions are ANDed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>list</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>An nil or empty list indicates that AWSClusters cannot use the identity from any namespace.</p>
</td>
</tr>
<tr>
<td>
<code>selector</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>An empty selector indicates that AWSClusters cannot use this
AWSClusterIdentity from any namespace.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Bastion">Bastion
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>Bastion defines a bastion host.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enabled</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enabled allows this provider to create a bastion host instance
with a public ip to access the VPC private network.</p>
</td>
</tr>
<tr>
<td>
<code>disableIngressRules</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>DisableIngressRules will ensure there are no Ingress rules in the bastion host&rsquo;s security group.
Requires AllowedCIDRBlocks to be empty.</p>
</td>
</tr>
<tr>
<td>
<code>allowedCIDRBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedCIDRBlocks is a list of CIDR blocks allowed to access the bastion host.
They are set as ingress rules for the Bastion host&rsquo;s Security Group (defaults to 0.0.0.0/0).</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType will use the specified instance type for the bastion. If not specified,
Cluster API Provider AWS will use t3.micro for all regions except us-east-1, where t2.micro
will be the default.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI will use the specified AMI to boot the bastion. If not specified,
the AMI will default to one picked out in public space.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.BuildParams">BuildParams
</h3>
<p>
<p>BuildParams is used to build tags around an aws resource.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>Lifecycle</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ResourceLifecycle">
ResourceLifecycle
</a>
</em>
</td>
<td>
<p>Lifecycle determines the resource lifecycle.</p>
</td>
</tr>
<tr>
<td>
<code>ClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the cluster associated with the resource.</p>
</td>
</tr>
<tr>
<td>
<code>ResourceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ResourceID is the unique identifier of the resource to be tagged.</p>
</td>
</tr>
<tr>
<td>
<code>Name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the resource, it&rsquo;s applied as the tag &ldquo;Name&rdquo; on AWS.</p>
</td>
</tr>
<tr>
<td>
<code>Role</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Role is the role associated to the resource.</p>
</td>
</tr>
<tr>
<td>
<code>Additional</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Any additional tags to be added to the resource.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.CNIIngressRule">CNIIngressRule
</h3>
<p>
<p>CNIIngressRule defines an AWS ingress rule for CNI requirements.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>description</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroupProtocol">
SecurityGroupProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>fromPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>toPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.CNIIngressRules">CNIIngressRules
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.CNIIngressRule</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CNISpec">CNISpec</a>)
</p>
<p>
<p>CNIIngressRules is a slice of CNIIngressRule.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.CNISpec">CNISpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>CNISpec defines configuration for CNI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cniIngressRules</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CNIIngressRules">
CNIIngressRules
</a>
</em>
</td>
<td>
<p>CNIIngressRules specify rules to apply to control plane and worker node security groups.
The source for the rule will be set to control plane and worker security group IDs.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">ClassicELB
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkStatus">NetworkStatus</a>)
</p>
<p>
<p>ClassicELB defines an AWS classic load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The name of the load balancer. It must be unique within the set of load balancers
defined in the region. It also serves as identifier.</p>
</td>
</tr>
<tr>
<td>
<code>dnsName</code><br/>
<em>
string
</em>
</td>
<td>
<p>DNSName is the dns name of the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>scheme</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBScheme">
ClassicELBScheme
</a>
</em>
</td>
<td>
<p>Scheme is the load balancer scheme, either internet-facing or private.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones in the VPC attached to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SubnetIDs is an array of subnets in the VPC attached to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SecurityGroupIDs is an array of security groups assigned to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>listeners</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBListener">
[]ClassicELBListener
</a>
</em>
</td>
<td>
<p>Listeners is an array of classic elb listeners associated with the load balancer. There must be at least one.</p>
</td>
</tr>
<tr>
<td>
<code>healthChecks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBHealthCheck">
ClassicELBHealthCheck
</a>
</em>
</td>
<td>
<p>HealthCheck is the classic elb health check associated with the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>attributes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBAttributes">
ClassicELBAttributes
</a>
</em>
</td>
<td>
<p>Attributes defines extra attributes associated with the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>Tags is a map of tags associated with the load balancer.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBAttributes">ClassicELBAttributes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">ClassicELB</a>)
</p>
<p>
<p>ClassicELBAttributes defines extra attributes associated with a classic load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>idleTimeout</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
<p>IdleTimeout is time that the connection is allowed to be idle (no data
has been sent over the connection) before it is closed by the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>crossZoneLoadBalancing</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>CrossZoneLoadBalancing enables the classic load balancer load balancing.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBHealthCheck">ClassicELBHealthCheck
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">ClassicELB</a>)
</p>
<p>
<p>ClassicELBHealthCheck defines an AWS classic load balancer health check.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>target</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>interval</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>timeout</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>healthyThreshold</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>unhealthyThreshold</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBListener">ClassicELBListener
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">ClassicELB</a>)
</p>
<p>
<p>ClassicELBListener defines an AWS classic load balancer listener.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBProtocol">
ClassicELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instanceProtocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBProtocol">
ClassicELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instancePort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBProtocol">ClassicELBProtocol
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBListener">ClassicELBListener</a>)
</p>
<p>
<p>ClassicELBProtocol defines listener protocols for a classic load balancer.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ClassicELBScheme">ClassicELBScheme
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">ClassicELB</a>)
</p>
<p>
<p>ClassicELBScheme defines the scheme of a classic load balancer.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.CloudInit">CloudInit
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>)
</p>
<p>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>insecureSkipSecretsManager</code><br/>
<em>
bool
</em>
</td>
<td>
<p>InsecureSkipSecretsManager, when set to true will not use AWS Secrets Manager
or AWS Systems Manager Parameter Store to ensure privacy of userdata.
By default, a cloud-init boothook shell script is prepended to download
the userdata from Secrets Manager and additionally delete the secret.</p>
</td>
</tr>
<tr>
<td>
<code>secretCount</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretCount is the number of secrets used to form the complete secret</p>
</td>
</tr>
<tr>
<td>
<code>secretPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretPrefix is the prefix for the secret name. This is stored
temporarily, and deleted when the machine registers as a node against
the workload cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretsBackend</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecretBackend">
SecretBackend
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecureSecretsBackend, when set to parameter-store will utilize the AWS Systems Manager
Parameter Storage to distribute secrets. By default or with the value of secrets-manager,
will use AWS Secrets Manager instead.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.EKSAMILookupType">EKSAMILookupType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AMIReference">AMIReference</a>)
</p>
<p>
<p>EKSAMILookupType specifies which AWS AMI to use for a AWSMachine and AWSMachinePool.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Filter">Filter
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSResourceReference">AWSResourceReference</a>)
</p>
<p>
<p>Filter is a filter used to identify an AWS resource.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the filter. Filter names are case-sensitive.</p>
</td>
</tr>
<tr>
<td>
<code>values</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Values includes one or more filter values. Filter values are case-sensitive.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.IPv6">IPv6
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>IPv6 contains ipv6 specific settings for the network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CidrBlock is the CIDR block provided by Amazon when VPC has enabled IPv6.</p>
</td>
</tr>
<tr>
<td>
<code>poolId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PoolID is the IP pool which must be defined in case of BYO IP is defined.</p>
</td>
</tr>
<tr>
<td>
<code>egressOnlyInternetGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EgressOnlyInternetGatewayID is the id of the egress only internet gateway associated with an IPv6 enabled VPC.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Ignition">Ignition
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>)
</p>
<p>
<p>Ignition defines options related to the bootstrapping systems where Ignition is used.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines which version of Ignition will be used to generate bootstrap data.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.IngressRule">IngressRule
</h3>
<p>
<p>IngressRule defines an AWS ingress rule for security groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>description</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroupProtocol">
SecurityGroupProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>fromPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>toPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>cidrBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of CIDR blocks to allow access from. Cannot be specified with SourceSecurityGroupID.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6CidrBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of IPv6 CIDR blocks to allow access from. Cannot be specified with SourceSecurityGroupID.</p>
</td>
</tr>
<tr>
<td>
<code>sourceSecurityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The security group id to allow access from. Cannot be specified with CidrBlocks.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.IngressRules">IngressRules
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.IngressRule</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroup">SecurityGroup</a>)
</p>
<p>
<p>IngressRules is a slice of AWS ingress rules for security groups.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Instance">Instance
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStatus">AWSClusterStatus</a>)
</p>
<p>
<p>Instance describes an AWS instance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instanceState</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.InstanceState">
InstanceState
</a>
</em>
</td>
<td>
<p>The current state of the instance.</p>
</td>
</tr>
<tr>
<td>
<code>type</code><br/>
<em>
string
</em>
</td>
<td>
<p>The instance type.</p>
</td>
</tr>
<tr>
<td>
<code>subnetId</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the subnet of the instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageId</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the AMI used to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the SSH key pair.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SecurityGroupIDs are one or more security group IDs this instance belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>userData</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserData is the raw data script passed to the instance which is run upon bootstrap.
This field must not be base64 encoded and should only be used when running a new instance.</p>
</td>
</tr>
<tr>
<td>
<code>iamProfile</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the IAM instance profile associated with the instance, if applicable.</p>
</td>
</tr>
<tr>
<td>
<code>addresses</code><br/>
<em>
[]Cluster API api/core/v1beta1.MachineAddress
</em>
</td>
<td>
<p>Addresses contains the AWS instance associated addresses.</p>
</td>
</tr>
<tr>
<td>
<code>privateIp</code><br/>
<em>
string
</em>
</td>
<td>
<p>The private IPv4 address assigned to the instance.</p>
</td>
</tr>
<tr>
<td>
<code>publicIp</code><br/>
<em>
string
</em>
</td>
<td>
<p>The public IPv4 address assigned to the instance, if applicable.</p>
</td>
</tr>
<tr>
<td>
<code>enaSupport</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Specifies whether enhanced networking with ENA is enabled.</p>
</td>
</tr>
<tr>
<td>
<code>ebsOptimized</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Indicates whether the instance is optimized for Amazon EBS I/O.</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the root storage volume.</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Specifies ENIs attached to instance</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>The tags associated with the instance.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>Availability zone of instance</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<p>SpotMarketOptions option for configuring instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
</td>
</tr>
<tr>
<td>
<code>volumeIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IDs of the instance&rsquo;s volumes</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.InstanceState">InstanceState
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineStatus">AWSMachineStatus</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Instance">Instance</a>)
</p>
<p>
<p>InstanceState describes the state of an AWS instance.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">NetworkSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>vpc</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.VPCSpec">
VPCSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VPC configuration.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Subnets">
Subnets
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets configuration.</p>
</td>
</tr>
<tr>
<td>
<code>cni</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CNISpec">
CNISpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CNI configuration</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupOverrides</code><br/>
<em>
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.SecurityGroupRole]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroupOverrides is an optional set of security groups to use for cluster instances
This is optional - if not provided new security groups will be created for the cluster</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.NetworkStatus">NetworkStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterStatus">AWSClusterStatus</a>)
</p>
<p>
<p>NetworkStatus encapsulates AWS networking resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>securityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroup">
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.SecurityGroupRole]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.SecurityGroup
</a>
</em>
</td>
<td>
<p>SecurityGroups is a map from the role/kind of the security group to its unique name, if any.</p>
</td>
</tr>
<tr>
<td>
<code>apiServerElb</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ClassicELB">
ClassicELB
</a>
</em>
</td>
<td>
<p>APIServerELB is the Kubernetes api server classic load balancer.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ResourceLifecycle">ResourceLifecycle
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.BuildParams">BuildParams</a>)
</p>
<p>
<p>ResourceLifecycle configures the lifecycle of a resource.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.RouteTable">RouteTable
</h3>
<p>
<p>RouteTable defines an AWS routing table.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.S3Bucket">S3Bucket
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>S3Bucket defines a supporting S3 bucket for the cluster, currently can be optionally used for Ignition.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlaneIAMInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<p>ControlPlaneIAMInstanceProfile is a name of the IAMInstanceProfile, which will be allowed
to read control-plane node bootstrap data from S3 Bucket.</p>
</td>
</tr>
<tr>
<td>
<code>nodesIAMInstanceProfiles</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>NodesIAMInstanceProfiles is a list of IAM instance profiles, which will be allowed to read
worker nodes bootstrap data from S3 Bucket.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name defines name of S3 Bucket to be created.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SecretBackend">SecretBackend
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CloudInit">CloudInit</a>)
</p>
<p>
<p>SecretBackend defines variants for backend secret storage.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroup">SecurityGroup
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkStatus">NetworkStatus</a>)
</p>
<p>
<p>SecurityGroup defines an AWS security group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is a unique identifier.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the security group name.</p>
</td>
</tr>
<tr>
<td>
<code>ingressRule</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.IngressRules">
IngressRules
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IngressRules is the inbound rules associated with the security group.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags associated with the security group.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroupProtocol">SecurityGroupProtocol
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.CNIIngressRule">CNIIngressRule</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.IngressRule">IngressRule</a>)
</p>
<p>
<p>SecurityGroupProtocol defines the protocol type for a security group rule.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroupRole">SecurityGroupRole
(<code>string</code> alias)</p></h3>
<p>
<p>SecurityGroupRole defines the unique role of a security group.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SpotMarketOptions">SpotMarketOptions
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Instance">Instance</a>)
</p>
<p>
<p>SpotMarketOptions defines the options available to a user when configuring
Machines to run on Spot instances.
Most users should provide an empty struct.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxPrice</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxPrice defines the maximum price the user is willing to pay for Spot VM instances</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SubnetSpec">SubnetSpec
</h3>
<p>
<p>SubnetSpec configures an AWS Subnet.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID defines a unique identifier to reference this resource.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CidrBlock is the CIDR block to be used when the provider creates a managed VPC.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6CidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6CidrBlock is the IPv6 CIDR block to be used when the provider creates a managed VPC.
A subnet can have an IPv4 and an IPv6 address.
IPv6 is only supported in managed clusters, this field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>AvailabilityZone defines the availability zone to use for this subnet in the cluster&rsquo;s region.</p>
</td>
</tr>
<tr>
<td>
<code>isPublic</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>IsPublic defines the subnet as a public subnet. A subnet is public when it is associated with a route table that has a route to an internet gateway.</p>
</td>
</tr>
<tr>
<td>
<code>isIpv6</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>IsIPv6 defines the subnet as an IPv6 subnet. A subnet is IPv6 when it is associated with a VPC that has IPv6 enabled.
IPv6 is only supported in managed clusters, this field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>routeTableId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RouteTableID is the routing table id associated with the subnet.</p>
</td>
</tr>
<tr>
<td>
<code>natGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NatGatewayID is the NAT gateway id associated with the subnet.
Ignored unless the subnet is managed by the provider, in which case this is set on the public subnet where the NAT gateway resides. It is then used to determine routes for private subnets in the same AZ as the public subnet.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a collection of tags describing the resource.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Subnets">Subnets
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta1.SubnetSpec</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>Subnets is a slice of Subnet.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Tags">Tags
(<code>map[string]string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSClusterSpec">AWSClusterSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.BuildParams">BuildParams</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SecurityGroup">SecurityGroup</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SubnetSpec">SubnetSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>Tags defines a map of tags.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.VPCSpec">VPCSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>VPCSpec configures an AWS VPC.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is the vpc-id of the VPC this provider should use to create resources.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CidrBlock is the CIDR block to be used when the provider creates a managed VPC.
Defaults to 10.0.0.0/16.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.IPv6">
IPv6
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6 contains ipv6 specific settings for the network. Supported only in managed clusters.
This field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>internetGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InternetGatewayID is the id of the internet gateway associated with the VPC.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a collection of tags describing the resource.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneUsageLimit</code><br/>
<em>
int
</em>
</td>
<td>
<p>AvailabilityZoneUsageLimit specifies the maximum number of availability zones (AZ) that
should be used in a region when automatically creating subnets. If a region has more
than this number of AZs then this number of AZs will be picked randomly when creating
default subnets. Defaults to 3</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSelection</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AZSelectionScheme">
AZSelectionScheme
</a>
</em>
</td>
<td>
<p>AvailabilityZoneSelection specifies how AZs should be selected if there are more AZs
in a region than specified by AvailabilityZoneUsageLimit. There are 2 selection schemes:
Ordered - selects based on alphabetical order
Random - selects AZs randomly in a region
Defaults to Ordered</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Volume">Volume
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Instance">Instance</a>)
</p>
<p>
<p>Volume encapsulates the configuration options for the storage device.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Device name</p>
</td>
</tr>
<tr>
<td>
<code>size</code><br/>
<em>
int64
</em>
</td>
<td>
<p>Size specifies size (in Gi) of the storage device.
Must be greater than the image snapshot size or 8 (whichever is greater).</p>
</td>
</tr>
<tr>
<td>
<code>type</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.VolumeType">
VolumeType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type is the type of the volume (e.g. gp2, io1, etc&hellip;).</p>
</td>
</tr>
<tr>
<td>
<code>iops</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>IOPS is the number of IOPS requested for the disk. Not applicable to all types.</p>
</td>
</tr>
<tr>
<td>
<code>throughput</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>Throughput to provision in MiB/s supported for the volume type. Not applicable to all types.</p>
</td>
</tr>
<tr>
<td>
<code>encrypted</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionKey is the KMS key to use to encrypt the volume. Can be either a KMS key ID or ARN.
If Encrypted is set and this is omitted, the default AWS key will be used.
The key must already exist and be accessible by the controller.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.VolumeType">VolumeType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Volume">Volume</a>)
</p>
<p>
<p>VolumeType describes the EBS volume type.
See: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ASGStatus">ASGStatus
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolStatus">AWSMachinePoolStatus</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AutoScalingGroup">AutoScalingGroup</a>)
</p>
<p>
<p>ASGStatus is a status string returned by the autoscaling API.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSFargateProfile">AWSFargateProfile
</h3>
<p>
<p>AWSFargateProfile is the Schema for the awsfargateprofiles API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileSpec">
FargateProfileSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>clusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the Cluster this object belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>profileName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProfileName specifies the profile name.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for this fargate pool
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>selectors</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateSelector">
[]FargateSelector
</a>
</em>
</td>
<td>
<p>Selectors specify fargate pod selectors.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileStatus">
FargateProfileStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">AWSLaunchTemplate
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>AWSLaunchTemplate defines the desired state of AWSLaunchTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the launch template.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name or the Amazon Resource Name (ARN) of the instance profile associated
with the IAM role for the instance. The instance profile contains the IAM
role.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string
(do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>versionNumber</code><br/>
<em>
int64
</em>
</td>
<td>
<p>VersionNumber is the version of the launch template that is applied.
Typically a new version is created when at least one of the following happens:
1) A new launch template spec is applied.
2) One or more parameters in an existing template is changed.
3) A new AMI is discovered.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instances. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<p>SpotMarketOptions are options for configuring AWSMachinePool instances to be run using AWS Spot instances.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePool">AWSMachinePool
</h3>
<p>
<p>AWSMachinePool is the Schema for the awsmachinepools API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">
AWSMachinePoolSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderID is the ARN of the associated ASG</p>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MinSize defines the minimum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MaxSize defines the maximum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets is an array of subnet configurations</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<p>AWSLaunchTemplate specifies the launch template and version to use when an instance is launched.</p>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
<p>MixedInstancesPolicy describes how multiple instance types will be used by the ASG.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the identification IDs of machine instances provided by the provider.
This field must match the provider IDs as seen on the node objects corresponding to a machine pool&rsquo;s machine instances.</p>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>refreshPreferences</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.RefreshPreferences">
RefreshPreferences
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RefreshPreferences describes set of preferences associated with the instance refresh request.</p>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enable or disable the capacity rebalance autoscaling group feature</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolStatus">
AWSMachinePoolStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolInstanceStatus">AWSMachinePoolInstanceStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolStatus">AWSMachinePoolStatus</a>)
</p>
<p>
<p>AWSMachinePoolInstanceStatus defines the status of the AWSMachinePoolInstance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceID is the identification of the Machine Instance within ASG</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the Kubernetes version for the Machine Instance</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePool">AWSMachinePool</a>)
</p>
<p>
<p>AWSMachinePoolSpec defines the desired state of AWSMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderID is the ARN of the associated ASG</p>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MinSize defines the minimum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MaxSize defines the maximum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets is an array of subnet configurations</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<p>AWSLaunchTemplate specifies the launch template and version to use when an instance is launched.</p>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
<p>MixedInstancesPolicy describes how multiple instance types will be used by the ASG.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the identification IDs of machine instances provided by the provider.
This field must match the provider IDs as seen on the node objects corresponding to a machine pool&rsquo;s machine instances.</p>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>refreshPreferences</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.RefreshPreferences">
RefreshPreferences
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RefreshPreferences describes set of preferences associated with the instance refresh request.</p>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enable or disable the capacity rebalance autoscaling group feature</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolStatus">AWSMachinePoolStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePool">AWSMachinePool</a>)
</p>
<p>
<p>AWSMachinePoolStatus defines the observed state of AWSMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is true when the provider resource is ready.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the most recently observed number of replicas</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSMachinePool.</p>
</td>
</tr>
<tr>
<td>
<code>instances</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolInstanceStatus">
[]AWSMachinePoolInstanceStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Instances contains the status for each instance in the pool</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateID</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The version of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the Machine and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the Machine and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>asgStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ASGStatus">
ASGStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePool">AWSManagedMachinePool
</h3>
<p>
<p>AWSManagedMachinePool is the Schema for the awsmanagedmachinepools API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">
AWSManagedMachinePoolSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>eksNodegroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSNodegroupName specifies the name of the nodegroup in AWS
corresponding to this MachinePool. If you don&rsquo;t specify a name
then a default name will be created based on the namespace and
name of the managed machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the node group role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for the node group.
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>amiVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIVersion defines the desired AMI release version. If no version number
is supplied then the latest version for the Kubernetes version
will be used</p>
</td>
</tr>
<tr>
<td>
<code>amiType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachineAMIType">
ManagedMachineAMIType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIType defines the AMI type</p>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Taints">
Taints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>diskSize</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSize specifies the root disk size</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>scaling</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolScaling">
ManagedMachinePoolScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scaling specifies scaling for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>remoteAccess</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedRemoteAccess">
ManagedRemoteAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RemoteAccess specifies how machines can be accessed remotely</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the provider IDs of instances in the
autoscaling group corresponding to the nodegroup represented by this
machine pool</p>
</td>
</tr>
<tr>
<td>
<code>capacityType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolCapacityType">
ManagedMachinePoolCapacityType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityType specifies the capacity type for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.UpdateConfig">
UpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig holds the optional config to control the behaviour of the update
to the nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLaunchTemplate specifies the launch template to use to create the managed node group.
If AWSLaunchTemplate is specified, certain node group configuraions outside of launch template
are prohibited (<a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html">https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html</a>).</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolStatus">
AWSManagedMachinePoolStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePool">AWSManagedMachinePool</a>)
</p>
<p>
<p>AWSManagedMachinePoolSpec defines the desired state of AWSManagedMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>eksNodegroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSNodegroupName specifies the name of the nodegroup in AWS
corresponding to this MachinePool. If you don&rsquo;t specify a name
then a default name will be created based on the namespace and
name of the managed machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the node group role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for the node group.
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>amiVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIVersion defines the desired AMI release version. If no version number
is supplied then the latest version for the Kubernetes version
will be used</p>
</td>
</tr>
<tr>
<td>
<code>amiType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachineAMIType">
ManagedMachineAMIType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIType defines the AMI type</p>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Taints">
Taints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>diskSize</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSize specifies the root disk size</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>scaling</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolScaling">
ManagedMachinePoolScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scaling specifies scaling for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>remoteAccess</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedRemoteAccess">
ManagedRemoteAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RemoteAccess specifies how machines can be accessed remotely</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the provider IDs of instances in the
autoscaling group corresponding to the nodegroup represented by this
machine pool</p>
</td>
</tr>
<tr>
<td>
<code>capacityType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolCapacityType">
ManagedMachinePoolCapacityType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityType specifies the capacity type for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.UpdateConfig">
UpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig holds the optional config to control the behaviour of the update
to the nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLaunchTemplate specifies the launch template to use to create the managed node group.
If AWSLaunchTemplate is specified, certain node group configuraions outside of launch template
are prohibited (<a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html">https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html</a>).</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolStatus">AWSManagedMachinePoolStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePool">AWSManagedMachinePool</a>)
</p>
<p>
<p>AWSManagedMachinePoolStatus defines the observed state of AWSManagedMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the AWSManagedMachinePool nodegroup has joined
the cluster</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the most recently observed number of replicas.</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ID of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The version of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the MachinePool and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of MachinePools
can be added as events to the MachinePool object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the MachinePool and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the MachinePool&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of MachinePools
can be added as events to the MachinePool object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the managed machine pool</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.AutoScalingGroup">AutoScalingGroup
</h3>
<p>
<p>AutoScalingGroup describes an AWS autoscaling group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>The tags associated with the instance.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>desiredCapacity</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>placementGroup</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>Status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.ASGStatus">
ASGStatus
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instances</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">
[]Instance
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.BlockDeviceMapping">BlockDeviceMapping
</h3>
<p>
<p>BlockDeviceMapping specifies the block devices for the instance.
You can specify virtual devices and EBS volumes.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The device name exposed to the EC2 instance (for example, /dev/sdh or xvdh).</p>
</td>
</tr>
<tr>
<td>
<code>ebs</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.EBS">
EBS
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>You can specify either VirtualName or Ebs, but not both.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.EBS">EBS
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.BlockDeviceMapping">BlockDeviceMapping</a>)
</p>
<p>
<p>EBS can be used to automatically set up EBS volumes when an instance is launched.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>encrypted</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSize</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The size of the volume, in GiB.
This can be a number from 1-1,024 for standard, 4-16,384 for io1, 1-16,384
for gp2, and 500-16,384 for st1 and sc1. If you specify a snapshot, the volume
size must be equal to or larger than the snapshot size.</p>
</td>
</tr>
<tr>
<td>
<code>volumeType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The volume type
For more information, see Amazon EBS Volume Types (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>)</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileSpec">FargateProfileSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSFargateProfile">AWSFargateProfile</a>)
</p>
<p>
<p>FargateProfileSpec defines the desired state of FargateProfile.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the Cluster this object belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>profileName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProfileName specifies the profile name.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for this fargate pool
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>selectors</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateSelector">
[]FargateSelector
</a>
</em>
</td>
<td>
<p>Selectors specify fargate pod selectors.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileStatus">FargateProfileStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSFargateProfile">AWSFargateProfile</a>)
</p>
<p>
<p>FargateProfileStatus defines the observed state of FargateProfile.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the FargateProfile is available.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the FargateProfile and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the FargateProfile&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of
FargateProfiles can be added as events to the FargateProfile object
and/or logged in the controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the FargateProfile and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the FargateProfile&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of
FargateProfiles can be added as events to the FargateProfile
object and/or logged in the controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current state of the Fargate profile.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.FargateSelector">FargateSelector
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileSpec">FargateProfileSpec</a>)
</p>
<p>
<p>FargateSelector specifies a selector for pods that should run on this fargate pool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>Labels specifies which pod labels this selector should match.</p>
</td>
</tr>
<tr>
<td>
<code>namespace</code><br/>
<em>
string
</em>
</td>
<td>
<p>Namespace specifies which namespace this selector should match.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.InstancesDistribution">InstancesDistribution
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">MixedInstancesPolicy</a>)
</p>
<p>
<p>InstancesDistribution to configure distribution of On-Demand Instances and Spot Instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>onDemandAllocationStrategy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.OnDemandAllocationStrategy">
OnDemandAllocationStrategy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>spotAllocationStrategy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.SpotAllocationStrategy">
SpotAllocationStrategy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>onDemandBaseCapacity</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>onDemandPercentageAboveBaseCapacity</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachineAMIType">ManagedMachineAMIType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachineAMIType specifies which AWS AMI to use for a managed MachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AL2023_ARM_64_STANDARD&#34;</p></td>
<td><p>Al2023Arm64 is the AL2023 Arm AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2023_x86_64_STANDARD&#34;</p></td>
<td><p>Al2023x86_64 is the AL2023 x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_ARM_64&#34;</p></td>
<td><p>Al2Arm64 is the Arm AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_x86_64&#34;</p></td>
<td><p>Al2x86_64 is the default AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_x86_64_GPU&#34;</p></td>
<td><p>Al2x86_64GPU is the x86-64 GPU AMI type.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolCapacityType">ManagedMachinePoolCapacityType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachinePoolCapacityType specifies the capacity type to be used for the managed MachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;onDemand&#34;</p></td>
<td><p>ManagedMachinePoolCapacityTypeOnDemand is the default capacity type, to launch on-demand instances.</p>
</td>
</tr><tr><td><p>&#34;spot&#34;</p></td>
<td><p>ManagedMachinePoolCapacityTypeSpot is the spot instance capacity type to launch spot instances.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ManagedMachinePoolScaling">ManagedMachinePoolScaling
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachinePoolScaling specifies scaling options.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.ManagedRemoteAccess">ManagedRemoteAccess
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedRemoteAccess specifies remote access settings for EC2 instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<p>SSHKeyName specifies which EC2 SSH key can be used to access machines.
If left empty, the key from the control plane is used.</p>
</td>
</tr>
<tr>
<td>
<code>sourceSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SourceSecurityGroups specifies which security groups are allowed access</p>
</td>
</tr>
<tr>
<td>
<code>public</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Public specifies whether to open port 22 to the public internet</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">MixedInstancesPolicy
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AutoScalingGroup">AutoScalingGroup</a>)
</p>
<p>
<p>MixedInstancesPolicy for an Auto Scaling group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instancesDistribution</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.InstancesDistribution">
InstancesDistribution
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>overrides</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Overrides">
[]Overrides
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.OnDemandAllocationStrategy">OnDemandAllocationStrategy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.InstancesDistribution">InstancesDistribution</a>)
</p>
<p>
<p>OnDemandAllocationStrategy indicates how to allocate instance types to fulfill On-Demand capacity.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Overrides">Overrides
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.MixedInstancesPolicy">MixedInstancesPolicy</a>)
</p>
<p>
<p>Overrides are used to override the instance type specified by the launch template with multiple
instance types that can be used to launch On-Demand Instances and Spot Instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.RefreshPreferences">RefreshPreferences
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec</a>)
</p>
<p>
<p>RefreshPreferences defines the specs for instance refreshing.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>strategy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The strategy to use for the instance refresh. The only valid value is Rolling.
A rolling update is an update that is applied to all instances in an Auto
Scaling group until all instances have been updated.</p>
</td>
</tr>
<tr>
<td>
<code>instanceWarmup</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of seconds until a newly launched instance is configured and ready
to use. During this time, the next replacement will not be initiated.
The default is to use the value for the health check grace period defined for the group.</p>
</td>
</tr>
<tr>
<td>
<code>minHealthyPercentage</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of capacity as a percentage in ASG that must remain healthy
during an instance refresh. The default is 90.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.SpotAllocationStrategy">SpotAllocationStrategy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.InstancesDistribution">InstancesDistribution</a>)
</p>
<p>
<p>SpotAllocationStrategy indicates how to allocate instances across Spot Instance pools.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Tags">Tags
(<code>map[string]string</code> alias)</p></h3>
<p>
<p>Tags is a mapping for tags.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Taint">Taint
</h3>
<p>
<p>Taint defines the specs for a Kubernetes taint.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>effect</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.TaintEffect">
TaintEffect
</a>
</em>
</td>
<td>
<p>Effect specifies the effect for the taint</p>
</td>
</tr>
<tr>
<td>
<code>key</code><br/>
<em>
string
</em>
</td>
<td>
<p>Key is the key of the taint</p>
</td>
</tr>
<tr>
<td>
<code>value</code><br/>
<em>
string
</em>
</td>
<td>
<p>Value is the value of the taint</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.TaintEffect">TaintEffect
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.Taint">Taint</a>)
</p>
<p>
<p>TaintEffect is the effect for a Kubernetes taint.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.Taints">Taints
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/exp/api/v1beta1.Taint</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>Taints is an array of Taints.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta1.UpdateConfig">UpdateConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>UpdateConfig is the configuration options for updating a nodegroup. Only one of MaxUnavailable
and MaxUnavailablePercentage should be specified.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailable is the maximum number of nodes unavailable at once during a version update.
Nodes will be updated in parallel. The maximum number is 100.</p>
</td>
</tr>
<tr>
<td>
<code>maxUnavailablePrecentage</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailablePercentage is the maximum percentage of nodes unavailable during a version update. This
percentage of nodes will be updated in parallel, up to 100 nodes at once.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="infrastructure.cluster.x-k8s.io/v1beta2">infrastructure.cluster.x-k8s.io/v1beta2</h2>
<p>
<p>Package v1beta2 contains the v1beta2 API implementation.</p>
</p>
Resource Types:
<ul></ul>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">AMIReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">AWSLaunchTemplate</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>AMIReference is a reference to a specific AWS resource by ID, ARN, or filters.
Only one of ID, ARN or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>eksLookupType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.EKSAMILookupType">
EKSAMILookupType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSOptimizedLookupType If specified, will look up an EKS Optimized image in SSM Parameter store</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSCluster">AWSCluster
</h3>
<p>
<p>AWSCluster is the schema for Amazon EC2 based Kubernetes Cluster API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">
AWSClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryControlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryControlPlaneLoadBalancer is an additional load balancer that can be used for the control plane.</p>
<p>An example use case is to have a separate internal load balancer for internal traffic,
and a separate external load balancer for external traffic.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStatus">
AWSClusterStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterControllerIdentity">AWSClusterControllerIdentity
</h3>
<p>
<p>AWSClusterControllerIdentity is the Schema for the awsclustercontrolleridentities API
It is used to grant access to use Cluster API Provider AWS Controller credentials.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterControllerIdentitySpec">
AWSClusterControllerIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterControllerIdentity.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterControllerIdentitySpec">AWSClusterControllerIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterControllerIdentity">AWSClusterControllerIdentity</a>)
</p>
<p>
<p>AWSClusterControllerIdentitySpec defines the specifications for AWSClusterControllerIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">AWSClusterIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterControllerIdentitySpec">AWSClusterControllerIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStaticIdentitySpec">AWSClusterStaticIdentitySpec</a>)
</p>
<p>
<p>AWSClusterIdentitySpec defines the Spec struct for AWSClusterIdentity types.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>allowedNamespaces</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AllowedNamespaces">
AllowedNamespaces
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedNamespaces is used to identify which namespaces are allowed to use the identity from.
Namespaces can be selected either using an array of namespaces or with label selector.
An empty allowedNamespaces object indicates that AWSClusters can use this identity from any namespace.
If this object is nil, no namespaces will be allowed (default behaviour, if this field is not provided)
A namespace should be either in the NamespaceList or match with Selector to use the identity.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentity">AWSClusterRoleIdentity
</h3>
<p>
<p>AWSClusterRoleIdentity is the Schema for the awsclusterroleidentities API
It is used to assume a role using the provided sourceRef.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentitySpec">
AWSClusterRoleIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterRoleIdentity.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>AWSRoleSpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSRoleSpec">
AWSRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>externalID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A unique identifier that might be required when you assume a role in another account.
If the administrator of the account to which the role belongs provided you with an
external ID, then provide that value in the ExternalId parameter. This value can be
any string, such as a passphrase or account number. A cross-account role is usually
set up to trust everyone in an account. Therefore, the administrator of the trusting
account might send an external ID to the administrator of the trusted account. That
way, only someone with the ID can assume the role, rather than everyone in the
account. For more information about the external ID, see How to Use an External ID
When Granting Access to Your AWS Resources to a Third Party in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>sourceIdentityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>SourceIdentityRef is a reference to another identity which will be chained to do
role assumption. All identity types are accepted.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentity">AWSClusterRoleIdentity</a>)
</p>
<p>
<p>AWSClusterRoleIdentitySpec defines the specifications for AWSClusterRoleIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>AWSRoleSpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSRoleSpec">
AWSRoleSpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSRoleSpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>externalID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>A unique identifier that might be required when you assume a role in another account.
If the administrator of the account to which the role belongs provided you with an
external ID, then provide that value in the ExternalId parameter. This value can be
any string, such as a passphrase or account number. A cross-account role is usually
set up to trust everyone in an account. Therefore, the administrator of the trusting
account might send an external ID to the administrator of the trusted account. That
way, only someone with the ID can assume the role, rather than everyone in the
account. For more information about the external ID, see How to Use an External ID
When Granting Access to Your AWS Resources to a Third Party in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>sourceIdentityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>SourceIdentityRef is a reference to another identity which will be chained to do
role assumption. All identity types are accepted.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSCluster">AWSCluster</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateResource">AWSClusterTemplateResource</a>)
</p>
<p>
<p>AWSClusterSpec defines the desired state of an EC2-based Kubernetes cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryControlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryControlPlaneLoadBalancer is an additional load balancer that can be used for the control plane.</p>
<p>An example use case is to have a separate internal load balancer for internal traffic,
and a separate external load balancer for external traffic.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStaticIdentity">AWSClusterStaticIdentity
</h3>
<p>
<p>AWSClusterStaticIdentity is the Schema for the awsclusterstaticidentities API
It represents a reference to an AWS access key ID and secret access key, stored in a secret.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStaticIdentitySpec">
AWSClusterStaticIdentitySpec
</a>
</em>
</td>
<td>
<p>Spec for this AWSClusterStaticIdentity</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>secretRef</code><br/>
<em>
string
</em>
</td>
<td>
<p>Reference to a secret containing the credentials. The secret should
contain the following data keys:
AccessKeyID: AKIAIOSFODNN7EXAMPLE
SecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
SessionToken: Optional</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStaticIdentitySpec">AWSClusterStaticIdentitySpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStaticIdentity">AWSClusterStaticIdentity</a>)
</p>
<p>
<p>AWSClusterStaticIdentitySpec defines the specifications for AWSClusterStaticIdentity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>AWSClusterIdentitySpec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">
AWSClusterIdentitySpec
</a>
</em>
</td>
<td>
<p>
(Members of <code>AWSClusterIdentitySpec</code> are embedded into this type.)
</p>
</td>
</tr>
<tr>
<td>
<code>secretRef</code><br/>
<em>
string
</em>
</td>
<td>
<p>Reference to a secret containing the credentials. The secret should
contain the following data keys:
AccessKeyID: AKIAIOSFODNN7EXAMPLE
SecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
SessionToken: Optional</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStatus">AWSClusterStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSCluster">AWSCluster</a>)
</p>
<p>
<p>AWSClusterStatus defines the observed state of AWSCluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>networkStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">
NetworkStatus
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">
Instance
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplate">AWSClusterTemplate
</h3>
<p>
<p>AWSClusterTemplate is the schema for Amazon EC2 based Kubernetes Cluster Templates.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateSpec">
AWSClusterTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateResource">
AWSClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateResource">AWSClusterTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateSpec">AWSClusterTemplateSpec</a>)
</p>
<p>
<p>AWSClusterTemplateResource defines the desired state of AWSClusterTemplateResource.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
Cluster API api/core/v1beta1.ObjectMeta
</em>
</td>
<td>
<em>(Optional)</em>
<p>Standard object&rsquo;s metadata.
More info: <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">
AWSClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>network</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">
NetworkSpec
</a>
</em>
</td>
<td>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS Region the cluster lives in.</p>
</td>
</tr>
<tr>
<td>
<code>partition</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Partition is the AWS security partition being used. Defaults to &ldquo;aws&rdquo;</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the bastion host. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>controlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneLoadBalancer is optional configuration for customizing control plane behavior.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryControlPlaneLoadBalancer</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">
AWSLoadBalancerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryControlPlaneLoadBalancer is an additional load balancer that can be used for the control plane.</p>
<p>An example use case is to have a separate internal load balancer for internal traffic,
and a separate external load balancer for external traffic.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up machine images when
a machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.
Supports substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base
OS and kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupOrg is the AWS Organization ID to look up machine images when a
machine does not specify an AMI. When set, this will be used for all
cluster machines unless a machine specifies a different ImageLookupOrg.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system used to look
up machine images when a machine does not specify an AMI. When set, this
will be used for all cluster machines unless a machine specifies a
different ImageLookupBaseOS.</p>
</td>
</tr>
<tr>
<td>
<code>bastion</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Bastion">
Bastion
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Bastion contains options to configure the bastion host.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<p>IdentityRef is a reference to an identity to be used when reconciling the managed control plane.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>s3Bucket</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.S3Bucket">
S3Bucket
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>S3Bucket contains options to configure a supporting S3 bucket for this
cluster - currently used for nodes requiring Ignition
(<a href="https://coreos.github.io/ignition/">https://coreos.github.io/ignition/</a>) for bootstrapping (requires
BootstrapFormatIgnition feature flag to be enabled).</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateSpec">AWSClusterTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplate">AWSClusterTemplate</a>)
</p>
<p>
<p>AWSClusterTemplateSpec defines the desired state of AWSClusterTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterTemplateResource">
AWSClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSConfidentialComputePolicy">AWSConfidentialComputePolicy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">CPUOptions</a>)
</p>
<p>
<p>AWSConfidentialComputePolicy represents the confidential compute configuration for the instance.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityKind">AWSIdentityKind
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">AWSIdentityReference</a>)
</p>
<p>
<p>AWSIdentityKind defines allowed AWS identity types.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">AWSIdentityReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSpec">ROSANetworkSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">ROSARoleConfigSpec</a>)
</p>
<p>
<p>AWSIdentityReference specifies a identity.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the identity.</p>
</td>
</tr>
<tr>
<td>
<code>kind</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityKind">
AWSIdentityKind
</a>
</em>
</td>
<td>
<p>Kind of the identity.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>AWSLoadBalancerSpec defines the desired state of an AWS load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name sets the name of the classic ELB load balancer. As per AWS, the name must be unique
within your set of load balancers for the region, must have a maximum of 32 characters, must
contain only alphanumeric characters or hyphens, and cannot begin or end with a hyphen. Once
set, the value cannot be changed.</p>
</td>
</tr>
<tr>
<td>
<code>scheme</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBScheme">
ELBScheme
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scheme sets the scheme of the load balancer (defaults to internet-facing)</p>
</td>
</tr>
<tr>
<td>
<code>crossZoneLoadBalancing</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>CrossZoneLoadBalancing enables the classic ELB cross availability zone balancing.</p>
<p>With cross-zone load balancing, each load balancer node for your Classic Load Balancer
distributes requests evenly across the registered instances in all enabled Availability Zones.
If cross-zone load balancing is disabled, each load balancer node distributes requests evenly across
the registered instances in its Availability Zone only.</p>
<p>Defaults to false.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets sets the subnets that should be applied to the control plane load balancer (defaults to discovered subnets for managed VPCs or an empty set for unmanaged VPCs)</p>
</td>
</tr>
<tr>
<td>
<code>healthCheckProtocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>HealthCheckProtocol sets the protocol type for ELB health check target
default value is ELBProtocolSSL</p>
</td>
</tr>
<tr>
<td>
<code>healthCheck</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheckAPISpec">
TargetGroupHealthCheckAPISpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>HealthCheck sets custom health check configuration to the API target group.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups sets the security groups used by the load balancer. Expected to be security group IDs
This is optional - if not provided new security groups will be created for the load balancer</p>
</td>
</tr>
<tr>
<td>
<code>additionalListeners</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AdditionalListenerSpec">
[]AdditionalListenerSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalListeners sets the additional listeners for the control plane load balancer.
This is only applicable to Network Load Balancer (NLB) types for the time being.</p>
</td>
</tr>
<tr>
<td>
<code>ingressRules</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">
[]IngressRule
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IngressRules sets the ingress rules for the control plane load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>loadBalancerType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancerType">
LoadBalancerType
</a>
</em>
</td>
<td>
<p>LoadBalancerType sets the type for a load balancer. The default type is classic.</p>
</td>
</tr>
<tr>
<td>
<code>disableHostsRewrite</code><br/>
<em>
bool
</em>
</td>
<td>
<p>DisableHostsRewrite disabled the hair pinning issue solution that adds the NLB&rsquo;s address as 127.0.0.1 to the hosts
file of each instance. This is by default, false.</p>
</td>
</tr>
<tr>
<td>
<code>preserveClientIP</code><br/>
<em>
bool
</em>
</td>
<td>
<p>PreserveClientIP lets the user control if preservation of client ips must be retained or not.
If this is enabled 6443 will be opened to 0.0.0.0/0.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachine">AWSMachine
</h3>
<p>
<p>AWSMachine is the schema for Amazon EC2 machines.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">
AWSMachineSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">
InstanceMetadataOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceMetadataOptions is the metadata options for the EC2 instance.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>cpuOptions,omitempty,omitzero</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">
CPUOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CPUOptions defines CPU-related settings for the instance, including the confidential computing policy.
When omitted, this means no opinion and the AWS platform is left to choose a reasonable default.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>elasticIpPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">
ElasticIPPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ElasticIPPool is the configuration to allocate Public IPv4 address (Elastic IP/EIP) from user-defined pool.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupOverrides</code><br/>
<em>
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroupRole]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroupOverrides is an optional set of security groups to use for the node.
This is optional - if not provided security groups from the cluster will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkInterfaceType">
NetworkInterfaceType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaceType is the interface type of the primary network Interface.
If not specified, AWS applies a default value.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupName specifies the name of the placement group in which to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupPartition</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupPartition is the partition number within the placement group in which to launch the instance.
This value is only valid if the placement group, referred in <code>PlacementGroupName</code>, was created with
strategy set to partition.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.
When Tenancy=host, AWS will attempt to find a suitable host from:
- Preexisting allocated hosts that have auto-placement enabled
- A specific host ID, if configured
- Allocating a new dedicated host if DynamicHostAllocation is configured</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsName</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">
PrivateDNSName
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSName is the options for the instance hostname.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the target Capacity Reservation into which the instance should be launched.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MarketType specifies the type of market for the EC2 instance. Valid values include:
&ldquo;OnDemand&rdquo; (default): The instance runs as a standard OnDemand instance.
&ldquo;Spot&rdquo;: The instance runs as a Spot instance. When SpotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.
&ldquo;CapacityBlock&rdquo;: The instance utilizes pre-purchased compute capacity (capacity blocks) with AWS Capacity Reservations.
If this value is selected, CapacityReservationID must be specified to identify the target reservation.
If marketType is not specified and spotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>hostID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostID specifies the Dedicated Host on which the instance must be started.
This field is mutually exclusive with DynamicHostAllocation.</p>
</td>
</tr>
<tr>
<td>
<code>hostAffinity</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostAffinity specifies the dedicated host affinity setting for the instance.
When HostAffinity is set to &ldquo;host&rdquo;, an instance started onto a specific host always restarts on the same host if stopped:
- If HostID is set, the instance launches on the specific host and must return to that same host after any stop/start (Targeted &amp; Pinned).
- If HostID is not set, the instance gets launched on any available and must returns to the same host after any stop/start (Auto-placed &amp; Pinned).
When HostAffinity is set to &ldquo;default&rdquo; (the default value), the instance (when restarted) can return on any available host:
- If HostID is set, the instance launches on the specified host now, but (when restarted) can return to any available hosts (Targeted &amp; Flexible).
- If HostID is not set, the instance launches on any available host now, and (when restarted) can return to any available hosts (Auto-placed &amp; Flexible).
If HostAffinity is not specified, it defaults to &ldquo;default&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>dynamicHostAllocation</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.DynamicHostAllocationSpec">
DynamicHostAllocationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DynamicHostAllocation enables automatic allocation of a single dedicated host.
Cost effectiveness of allocating a single instance on a dedicated host may vary
depending on the instance type and the region.
This field is mutually exclusive with HostID and always allocates exactly one host.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationPreference</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationPreference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
&ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
&ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
&ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation. Note that this is incompatible with a MarketType of <code>Spot</code></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineStatus">
AWSMachineStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineProviderConditionType">AWSMachineProviderConditionType
(<code>string</code> alias)</p></h3>
<p>
<p>AWSMachineProviderConditionType is a valid value for AWSMachineProviderCondition.Type.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachine">AWSMachine</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateResource">AWSMachineTemplateResource</a>)
</p>
<p>
<p>AWSMachineSpec defines the desired state of an Amazon EC2 instance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">
InstanceMetadataOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceMetadataOptions is the metadata options for the EC2 instance.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>cpuOptions,omitempty,omitzero</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">
CPUOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CPUOptions defines CPU-related settings for the instance, including the confidential computing policy.
When omitted, this means no opinion and the AWS platform is left to choose a reasonable default.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>elasticIpPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">
ElasticIPPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ElasticIPPool is the configuration to allocate Public IPv4 address (Elastic IP/EIP) from user-defined pool.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupOverrides</code><br/>
<em>
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroupRole]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroupOverrides is an optional set of security groups to use for the node.
This is optional - if not provided security groups from the cluster will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkInterfaceType">
NetworkInterfaceType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaceType is the interface type of the primary network Interface.
If not specified, AWS applies a default value.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupName specifies the name of the placement group in which to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupPartition</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupPartition is the partition number within the placement group in which to launch the instance.
This value is only valid if the placement group, referred in <code>PlacementGroupName</code>, was created with
strategy set to partition.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.
When Tenancy=host, AWS will attempt to find a suitable host from:
- Preexisting allocated hosts that have auto-placement enabled
- A specific host ID, if configured
- Allocating a new dedicated host if DynamicHostAllocation is configured</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsName</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">
PrivateDNSName
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSName is the options for the instance hostname.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the target Capacity Reservation into which the instance should be launched.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MarketType specifies the type of market for the EC2 instance. Valid values include:
&ldquo;OnDemand&rdquo; (default): The instance runs as a standard OnDemand instance.
&ldquo;Spot&rdquo;: The instance runs as a Spot instance. When SpotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.
&ldquo;CapacityBlock&rdquo;: The instance utilizes pre-purchased compute capacity (capacity blocks) with AWS Capacity Reservations.
If this value is selected, CapacityReservationID must be specified to identify the target reservation.
If marketType is not specified and spotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>hostID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostID specifies the Dedicated Host on which the instance must be started.
This field is mutually exclusive with DynamicHostAllocation.</p>
</td>
</tr>
<tr>
<td>
<code>hostAffinity</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostAffinity specifies the dedicated host affinity setting for the instance.
When HostAffinity is set to &ldquo;host&rdquo;, an instance started onto a specific host always restarts on the same host if stopped:
- If HostID is set, the instance launches on the specific host and must return to that same host after any stop/start (Targeted &amp; Pinned).
- If HostID is not set, the instance gets launched on any available and must returns to the same host after any stop/start (Auto-placed &amp; Pinned).
When HostAffinity is set to &ldquo;default&rdquo; (the default value), the instance (when restarted) can return on any available host:
- If HostID is set, the instance launches on the specified host now, but (when restarted) can return to any available hosts (Targeted &amp; Flexible).
- If HostID is not set, the instance launches on any available host now, and (when restarted) can return to any available hosts (Auto-placed &amp; Flexible).
If HostAffinity is not specified, it defaults to &ldquo;default&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>dynamicHostAllocation</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.DynamicHostAllocationSpec">
DynamicHostAllocationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DynamicHostAllocation enables automatic allocation of a single dedicated host.
Cost effectiveness of allocating a single instance on a dedicated host may vary
depending on the instance type and the region.
This field is mutually exclusive with HostID and always allocates exactly one host.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationPreference</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationPreference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
&ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
&ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
&ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation. Note that this is incompatible with a MarketType of <code>Spot</code></p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineStatus">AWSMachineStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachine">AWSMachine</a>)
</p>
<p>
<p>AWSMachineStatus defines the observed state of AWSMachine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is true when the provider resource is ready.</p>
</td>
</tr>
<tr>
<td>
<code>interruptible</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Interruptible reports that this machine is using spot instances and can therefore be interrupted by CAPI when it receives a notice that the spot instance is to be terminated by AWS.
This will be set to true when SpotMarketOptions is not nil (i.e. this machine is using a spot instance).</p>
</td>
</tr>
<tr>
<td>
<code>addresses</code><br/>
<em>
[]Cluster API api/core/v1beta1.MachineAddress
</em>
</td>
<td>
<p>Addresses contains the AWS instance associated addresses.</p>
</td>
</tr>
<tr>
<td>
<code>instanceState</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceState">
InstanceState
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceState is the state of the AWS instance for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the Machine and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the Machine and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSMachine.</p>
</td>
</tr>
<tr>
<td>
<code>dedicatedHost</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.DedicatedHostStatus">
DedicatedHostStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DedicatedHost tracks the dynamically allocated dedicated host.
This field is populated when DynamicHostAllocation is used.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplate">AWSMachineTemplate
</h3>
<p>
<p>AWSMachineTemplate is the schema for the Amazon EC2 Machine Templates API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateSpec">
AWSMachineTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateResource">
AWSMachineTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateStatus">
AWSMachineTemplateStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateResource">AWSMachineTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateSpec">AWSMachineTemplateSpec</a>)
</p>
<p>
<p>AWSMachineTemplateResource describes the data needed to create am AWSMachine from a template.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
Cluster API api/core/v1beta1.ObjectMeta
</em>
</td>
<td>
<em>(Optional)</em>
<p>Standard object&rsquo;s metadata.
More info: <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">
AWSMachineSpec
</a>
</em>
</td>
<td>
<p>Spec is the specification of the desired behavior of the machine.</p>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProviderID is the unique identifier as specified by the cloud provider.</p>
</td>
</tr>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceID is the EC2 instance ID for this machine.</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">
InstanceMetadataOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceMetadataOptions is the metadata options for the EC2 instance.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>cpuOptions,omitempty,omitzero</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">
CPUOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CPUOptions defines CPU-related settings for the instance, including the confidential computing policy.
When omitted, this means no opinion and the AWS platform is left to choose a reasonable default.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider. If both the AWSCluster and the AWSMachine specify the same tag name with different values, the
AWSMachine&rsquo;s value takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IAMInstanceProfile is a name of an IAM instance profile to assign to the instance</p>
</td>
</tr>
<tr>
<td>
<code>publicIP</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIP specifies whether the instance should get a public IP.
Precedence for this setting is as follows:
1. This field if set
2. Cluster/flavor setting
3. Subnet default</p>
</td>
</tr>
<tr>
<td>
<code>elasticIpPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">
ElasticIPPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ElasticIPPool is the configuration to allocate Public IPv4 address (Elastic IP/EIP) from user-defined pool.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instance. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator. It is possible to specify either IDs of Filters. Using Filters
will cause additional requests to AWS API and if tags change the attached security groups might change too.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnet is a reference to the subnet to use for this instance. If not specified,
the cluster subnet will be used.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupOverrides</code><br/>
<em>
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroupRole]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroupOverrides is an optional set of security groups to use for the node.
This is optional - if not provided security groups from the cluster will be used.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string (do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaces is a list of ENIs to associate with the instance.
A maximum of 2 may be specified.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkInterfaceType">
NetworkInterfaceType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NetworkInterfaceType is the interface type of the primary network Interface.
If not specified, AWS applies a default value.</p>
</td>
</tr>
<tr>
<td>
<code>uncompressedUserData</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>UncompressedUserData specify whether the user data is gzip-compressed before it is sent to ec2 instance.
cloud-init has built-in support for gzip-compressed user data
user data stored in aws secret manager is always gzip-compressed.</p>
</td>
</tr>
<tr>
<td>
<code>cloudInit</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CloudInit">
CloudInit
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SpotMarketOptions allows users to configure instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupName specifies the name of the placement group in which to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupPartition</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupPartition is the partition number within the placement group in which to launch the instance.
This value is only valid if the placement group, referred in <code>PlacementGroupName</code>, was created with
strategy set to partition.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.
When Tenancy=host, AWS will attempt to find a suitable host from:
- Preexisting allocated hosts that have auto-placement enabled
- A specific host ID, if configured
- Allocating a new dedicated host if DynamicHostAllocation is configured</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsName</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">
PrivateDNSName
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSName is the options for the instance hostname.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the target Capacity Reservation into which the instance should be launched.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MarketType specifies the type of market for the EC2 instance. Valid values include:
&ldquo;OnDemand&rdquo; (default): The instance runs as a standard OnDemand instance.
&ldquo;Spot&rdquo;: The instance runs as a Spot instance. When SpotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.
&ldquo;CapacityBlock&rdquo;: The instance utilizes pre-purchased compute capacity (capacity blocks) with AWS Capacity Reservations.
If this value is selected, CapacityReservationID must be specified to identify the target reservation.
If marketType is not specified and spotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>hostID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostID specifies the Dedicated Host on which the instance must be started.
This field is mutually exclusive with DynamicHostAllocation.</p>
</td>
</tr>
<tr>
<td>
<code>hostAffinity</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostAffinity specifies the dedicated host affinity setting for the instance.
When HostAffinity is set to &ldquo;host&rdquo;, an instance started onto a specific host always restarts on the same host if stopped:
- If HostID is set, the instance launches on the specific host and must return to that same host after any stop/start (Targeted &amp; Pinned).
- If HostID is not set, the instance gets launched on any available and must returns to the same host after any stop/start (Auto-placed &amp; Pinned).
When HostAffinity is set to &ldquo;default&rdquo; (the default value), the instance (when restarted) can return on any available host:
- If HostID is set, the instance launches on the specified host now, but (when restarted) can return to any available hosts (Targeted &amp; Flexible).
- If HostID is not set, the instance launches on any available host now, and (when restarted) can return to any available hosts (Auto-placed &amp; Flexible).
If HostAffinity is not specified, it defaults to &ldquo;default&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>dynamicHostAllocation</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.DynamicHostAllocationSpec">
DynamicHostAllocationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DynamicHostAllocation enables automatic allocation of a single dedicated host.
Cost effectiveness of allocating a single instance on a dedicated host may vary
depending on the instance type and the region.
This field is mutually exclusive with HostID and always allocates exactly one host.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationPreference</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationPreference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
&ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
&ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
&ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation. Note that this is incompatible with a MarketType of <code>Spot</code></p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateSpec">AWSMachineTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplate">AWSMachineTemplate</a>)
</p>
<p>
<p>AWSMachineTemplateSpec defines the desired state of AWSMachineTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateResource">
AWSMachineTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateStatus">AWSMachineTemplateStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplate">AWSMachineTemplate</a>)
</p>
<p>
<p>AWSMachineTemplateStatus defines a status for an AWSMachineTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>capacity</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#resourcelist-v1-core">
Kubernetes core/v1.ResourceList
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Capacity defines the resource capacity for this machine.
This value is used for autoscaling from zero operations as defined in:
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md">https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md</a></p>
</td>
</tr>
<tr>
<td>
<code>nodeInfo</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NodeInfo">
NodeInfo
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeInfo contains information about the node&rsquo;s architecture and operating system.
This value is used for autoscaling from zero operations as defined in:
<a href="https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md">https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20210310-opt-in-autoscaling-from-zero.md</a></p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSMachineTemplate.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateWebhook">AWSMachineTemplateWebhook
</h3>
<p>
<p>AWSMachineTemplateWebhook implements a custom validation webhook for AWSMachineTemplate.
Note: we use a custom validator to access the request context for SSA of AWSMachineTemplate.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedCluster">AWSManagedCluster
</h3>
<p>
<p>AWSManagedCluster is the Schema for the awsmanagedclusters API</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterSpec">
AWSManagedClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterStatus">
AWSManagedClusterStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterSpec">AWSManagedClusterSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedCluster">AWSManagedCluster</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateResource">AWSManagedClusterTemplateResource</a>)
</p>
<p>
<p>AWSManagedClusterSpec defines the desired state of AWSManagedCluster</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterStatus">AWSManagedClusterStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedCluster">AWSManagedCluster</a>)
</p>
<p>
<p>AWSManagedClusterStatus defines the observed state of AWSManagedCluster</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is when the AWSManagedControlPlane has a API server URL.</p>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureDomains specifies a list fo available availability zones that can be used</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSManagedCluster.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplate">AWSManagedClusterTemplate
</h3>
<p>
<p>AWSManagedClusterTemplate is the Schema for the AWSManagedClusterTemplates API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateSpec">
AWSManagedClusterTemplateSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateResource">
AWSManagedClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateResource">AWSManagedClusterTemplateResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateSpec">AWSManagedClusterTemplateSpec</a>)
</p>
<p>
<p>AWSManagedClusterTemplateResource describes the data needed to create an AWSManagedCluster from a template.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterSpec">
AWSManagedClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateSpec">AWSManagedClusterTemplateSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplate">AWSManagedClusterTemplate</a>)
</p>
<p>
<p>AWSManagedClusterTemplateSpec defines the desired state of AWSManagedClusterTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedClusterTemplateResource">
AWSManagedClusterTemplateResource
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">AWSResourceReference
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">AWSLaunchTemplate</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>)
</p>
<p>
<p>AWSResourceReference is a reference to a specific AWS resource by ID or filters.
Only one of ID or Filters may be specified. Specifying more than one will result in
a validation error.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID of resource</p>
</td>
</tr>
<tr>
<td>
<code>filters</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Filter">
[]Filter
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Filters is a set of key/value pairs used to identify a resource
They are applied according to the rules defined by the AWS API:
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</a></p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSRoleSpec">AWSRoleSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterRoleIdentitySpec">AWSClusterRoleIdentitySpec</a>)
</p>
<p>
<p>AWSRoleSpec defines the specifications for all identities based around AWS roles.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>roleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>The Amazon Resource Name (ARN) of the role to assume.</p>
</td>
</tr>
<tr>
<td>
<code>sessionName</code><br/>
<em>
string
</em>
</td>
<td>
<p>An identifier for the assumed role session</p>
</td>
</tr>
<tr>
<td>
<code>durationSeconds</code><br/>
<em>
int32
</em>
</td>
<td>
<p>The duration, in seconds, of the role session before it is renewed.</p>
</td>
</tr>
<tr>
<td>
<code>inlinePolicy</code><br/>
<em>
string
</em>
</td>
<td>
<p>An IAM policy as a JSON-encoded string that you want to use as an inline session policy.</p>
</td>
</tr>
<tr>
<td>
<code>policyARNs</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>The Amazon Resource Names (ARNs) of the IAM managed policies that you want
to use as managed session policies.
The policies must exist in the same account as the role.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AZSelectionScheme">AZSelectionScheme
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>AZSelectionScheme defines the scheme of selecting AZs.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AdditionalListenerSpec">AdditionalListenerSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>)
</p>
<p>
<p>AdditionalListenerSpec defines the desired state of an
additional listener on an AWS load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>port</code><br/>
<em>
int64
</em>
</td>
<td>
<p>Port sets the port for the additional listener.</p>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
<p>Protocol sets the protocol for the additional listener.
Currently only TCP is supported.</p>
</td>
</tr>
<tr>
<td>
<code>healthCheck</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheckAdditionalSpec">
TargetGroupHealthCheckAdditionalSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>HealthCheck sets the optional custom health check configuration to the API target group.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AllowedNamespaces">AllowedNamespaces
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterIdentitySpec">AWSClusterIdentitySpec</a>)
</p>
<p>
<p>AllowedNamespaces is a selector of namespaces that AWSClusters can
use this ClusterPrincipal from. This is a standard Kubernetes LabelSelector,
a label query over a set of resources. The result of matchLabels and
matchExpressions are ANDed.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>list</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>An nil or empty list indicates that AWSClusters cannot use the identity from any namespace.</p>
</td>
</tr>
<tr>
<td>
<code>selector</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>An empty selector indicates that AWSClusters cannot use this
AWSClusterIdentity from any namespace.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Architecture">Architecture
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NodeInfo">NodeInfo</a>)
</p>
<p>
<p>Architecture represents the CPU architecture of the node.
Its underlying type is a string and its value can be any of amd64, arm64.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Bastion">Bastion
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>Bastion defines a bastion host.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enabled</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enabled allows this provider to create a bastion host instance
with a public ip to access the VPC private network.</p>
</td>
</tr>
<tr>
<td>
<code>disableIngressRules</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>DisableIngressRules will ensure there are no Ingress rules in the bastion host&rsquo;s security group.
Requires AllowedCIDRBlocks to be empty.</p>
</td>
</tr>
<tr>
<td>
<code>allowedCIDRBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AllowedCIDRBlocks is a list of CIDR blocks allowed to access the bastion host.
They are set as ingress rules for the Bastion host&rsquo;s Security Group (defaults to 0.0.0.0/0).</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType will use the specified instance type for the bastion. If not specified,
Cluster API Provider AWS will use t3.micro for all regions except us-east-1, where t2.micro
will be the default.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI will use the specified AMI to boot the bastion. If not specified,
the AMI will default to one picked out in public space.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.BuildParams">BuildParams
</h3>
<p>
<p>BuildParams is used to build tags around an aws resource.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>Lifecycle</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ResourceLifecycle">
ResourceLifecycle
</a>
</em>
</td>
<td>
<p>Lifecycle determines the resource lifecycle.</p>
</td>
</tr>
<tr>
<td>
<code>ClusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the cluster associated with the resource.</p>
</td>
</tr>
<tr>
<td>
<code>ResourceID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ResourceID is the unique identifier of the resource to be tagged.</p>
</td>
</tr>
<tr>
<td>
<code>Name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Name is the name of the resource, it&rsquo;s applied as the tag &ldquo;Name&rdquo; on AWS.</p>
</td>
</tr>
<tr>
<td>
<code>Role</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Role is the role associated to the resource.</p>
</td>
</tr>
<tr>
<td>
<code>Additional</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Any additional tags to be added to the resource.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CNIIngressRule">CNIIngressRule
</h3>
<p>
<p>CNIIngressRule defines an AWS ingress rule for CNI requirements.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>description</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroupProtocol">
SecurityGroupProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>fromPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>toPort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CNIIngressRules">CNIIngressRules
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.CNIIngressRule</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CNISpec">CNISpec</a>)
</p>
<p>
<p>CNIIngressRules is a slice of CNIIngressRule.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CNISpec">CNISpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>CNISpec defines configuration for CNI.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cniIngressRules</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CNIIngressRules">
CNIIngressRules
</a>
</em>
</td>
<td>
<p>CNIIngressRules specify rules to apply to control plane and worker node security groups.
The source for the rule will be set to control plane and worker security group IDs.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">CPUOptions
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>)
</p>
<p>
<p>CPUOptions defines CPU-related settings for the instance, including the confidential computing policy.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>confidentialCompute</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSConfidentialComputePolicy">
AWSConfidentialComputePolicy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ConfidentialCompute specifies whether confidential computing should be enabled for the instance,
and, if so, which confidential computing technology to use.
Valid values are: Disabled, AMDEncryptedVirtualizationNestedPaging
When set to Disabled, confidential computing will be disabled for the instance.
When set to AMDEncryptedVirtualizationNestedPaging, AMD SEV-SNP will be used as the confidential computing technology for the instance.
In this case, ensure the following conditions are met:
1) The selected instance type supports AMD SEV-SNP.
2) The selected AWS region supports AMD SEV-SNP.
3) The selected AMI supports AMD SEV-SNP.
More details can be checked at <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html</a>
When omitted, this means no opinion and the AWS platform is left to choose a reasonable default,
which is subject to change without notice. The current default is Disabled.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">CapacityReservationPreference
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>CapacityReservationPreference describes the preferred use of capacity reservations
of an instance</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBAttributes">ClassicELBAttributes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>ClassicELBAttributes defines extra attributes associated with a classic load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>idleTimeout</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
<p>IdleTimeout is time that the connection is allowed to be idle (no data
has been sent over the connection) before it is closed by the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>crossZoneLoadBalancing</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>CrossZoneLoadBalancing enables the classic load balancer load balancing.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBHealthCheck">ClassicELBHealthCheck
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>ClassicELBHealthCheck defines an AWS classic load balancer health check.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>target</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>interval</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>timeout</code><br/>
<em>
<a href="https://golang.org/pkg/time/#Duration">
time.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>healthyThreshold</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>unhealthyThreshold</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBListener">ClassicELBListener
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>ClassicELBListener defines an AWS classic load balancer listener.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instanceProtocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instancePort</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CloudInit">CloudInit
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>)
</p>
<p>
<p>CloudInit defines options related to the bootstrapping systems where
CloudInit is used.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>insecureSkipSecretsManager</code><br/>
<em>
bool
</em>
</td>
<td>
<p>InsecureSkipSecretsManager, when set to true will not use AWS Secrets Manager
or AWS Systems Manager Parameter Store to ensure privacy of userdata.
By default, a cloud-init boothook shell script is prepended to download
the userdata from Secrets Manager and additionally delete the secret.</p>
</td>
</tr>
<tr>
<td>
<code>secretCount</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretCount is the number of secrets used to form the complete secret</p>
</td>
</tr>
<tr>
<td>
<code>secretPrefix</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecretPrefix is the prefix for the secret name. This is stored
temporarily, and deleted when the machine registers as a node against
the workload cluster.</p>
</td>
</tr>
<tr>
<td>
<code>secureSecretsBackend</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">
SecretBackend
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecureSecretsBackend, when set to parameter-store will utilize the AWS Systems Manager
Parameter Storage to distribute secrets. By default or with the value of secrets-manager,
will use AWS Secrets Manager instead.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.DedicatedHostInfo">DedicatedHostInfo
</h3>
<p>
<p>DedicatedHostInfo contains information about a dedicated host.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>hostID</code><br/>
<em>
string
</em>
</td>
<td>
<p>HostID is the ID of the dedicated host.</p>
</td>
</tr>
<tr>
<td>
<code>instanceFamily</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceFamily is the instance family supported by the host.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the instance type supported by the host.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>AvailabilityZone is the AZ where the host is located.</p>
</td>
</tr>
<tr>
<td>
<code>state</code><br/>
<em>
string
</em>
</td>
<td>
<p>State is the current state of the dedicated host.</p>
</td>
</tr>
<tr>
<td>
<code>totalCapacity</code><br/>
<em>
int32
</em>
</td>
<td>
<p>TotalCapacity is the total number of instances that can be launched on the host.</p>
</td>
</tr>
<tr>
<td>
<code>availableCapacity</code><br/>
<em>
int32
</em>
</td>
<td>
<p>AvailableCapacity is the number of instances that can still be launched on the host.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>Tags associated with the dedicated host.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.DedicatedHostStatus">DedicatedHostStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineStatus">AWSMachineStatus</a>)
</p>
<p>
<p>DedicatedHostStatus defines the observed state of a dynamically allocated dedicated host
associated with an AWSMachine. This struct is used to track the ID of the dedicated host.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ID tracks the dynamically allocated dedicated host ID.
This field is populated when DynamicHostAllocation is used.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.DynamicHostAllocationSpec">DynamicHostAllocationSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>)
</p>
<p>
<p>DynamicHostAllocationSpec defines the configuration for dynamic dedicated host allocation.
This specification always allocates exactly one dedicated host per machine.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tags to apply to the allocated dedicated host.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.EKSAMILookupType">EKSAMILookupType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">AMIReference</a>)
</p>
<p>
<p>EKSAMILookupType specifies which AWS AMI to use for a AWSMachine and AWSMachinePool.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">ELBProtocol
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AdditionalListenerSpec">AdditionalListenerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBListener">ClassicELBListener</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Listener">Listener</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupSpec">TargetGroupSpec</a>)
</p>
<p>
<p>ELBProtocol defines listener protocols for a load balancer.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ELBScheme">ELBScheme
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>ELBScheme defines the scheme of a load balancer.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">ElasticIPPool
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>ElasticIPPool allows configuring a Elastic IP pool for resources allocating
public IPv4 addresses on public subnets.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>publicIpv4Pool</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIpv4Pool sets a custom Public IPv4 Pool used to create Elastic IP address for resources
created in public IPv4 subnets. Every IPv4 address, Elastic IP, will be allocated from the custom
Public IPv4 pool that you brought to AWS, instead of Amazon-provided pool. The public IPv4 pool
resource ID starts with &lsquo;ipv4pool-ec2&rsquo;.</p>
</td>
</tr>
<tr>
<td>
<code>publicIpv4PoolFallbackOrder</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PublicIpv4PoolFallbackOrder">
PublicIpv4PoolFallbackOrder
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIpv4PoolFallBackOrder defines the fallback action when the Public IPv4 Pool has been exhausted,
no more IPv4 address available in the pool.</p>
<p>When set to &lsquo;amazon-pool&rsquo;, the controller check if the pool has available IPv4 address, when pool has reached the
IPv4 limit, the address will be claimed from Amazon-pool (default).</p>
<p>When set to &lsquo;none&rsquo;, the controller will fail the Elastic IP allocation when the publicIpv4Pool is exhausted.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Filter">Filter
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">AWSResourceReference</a>)
</p>
<p>
<p>Filter is a filter used to identify an AWS resource.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the filter. Filter names are case-sensitive.</p>
</td>
</tr>
<tr>
<td>
<code>values</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Values includes one or more filter values. Filter values are case-sensitive.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.GCTask">GCTask
(<code>string</code> alias)</p></h3>
<p>
<p>GCTask defines a task to be executed by the garbage collector.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.HTTPTokensState">HTTPTokensState
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">InstanceMetadataOptions</a>)
</p>
<p>
<p>HTTPTokensState describes the state of InstanceMetadataOptions.HTTPTokensState</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IPAMPool">IPAMPool
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IPv6">IPv6</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>IPAMPool defines the IPAM pool to be used for VPC.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is the ID of the IPAM pool this provider should use to create VPC.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the name of the IPAM pool this provider should use to create VPC.</p>
</td>
</tr>
<tr>
<td>
<code>netmaskLength</code><br/>
<em>
int64
</em>
</td>
<td>
<p>The netmask length of the IPv4 CIDR you want to allocate to VPC from
an Amazon VPC IP Address Manager (IPAM) pool.
Defaults to /16 for IPv4 if not specified.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IPv6">IPv6
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>IPv6 contains ipv6 specific settings for the network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CidrBlock is the CIDR block provided by Amazon when VPC has enabled IPv6.
Mutually exclusive with IPAMPool.</p>
</td>
</tr>
<tr>
<td>
<code>poolId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PoolID is the IP pool which must be defined in case of BYO IP is defined.
Must be specified if CidrBlock is set.
Mutually exclusive with IPAMPool.</p>
</td>
</tr>
<tr>
<td>
<code>egressOnlyInternetGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EgressOnlyInternetGatewayID is the id of the egress only internet gateway associated with an IPv6 enabled VPC.</p>
</td>
</tr>
<tr>
<td>
<code>ipamPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IPAMPool">
IPAMPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPAMPool defines the IPAMv6 pool to be used for VPC.
Mutually exclusive with CidrBlock.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Ignition">Ignition
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>)
</p>
<p>
<p>Ignition defines options related to the bootstrapping systems where Ignition is used.
For more information on Ignition configuration, see <a href="https://coreos.github.io/butane/specs/">https://coreos.github.io/butane/specs/</a></p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines which version of Ignition will be used to generate bootstrap data.
Defaults to <code>2.3</code> if storageType is set to <code>ClusterObjectStore</code>.
It will be ignored if storageType is set to <code>UnencryptedUserData</code>, as the userdata defines its own version.</p>
</td>
</tr>
<tr>
<td>
<code>storageType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionStorageTypeOption">
IgnitionStorageTypeOption
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StorageType defines how to store the boostrap user data for Ignition.
This can be used to instruct Ignition from where to fetch the user data to bootstrap an instance.</p>
<p>When omitted, the storage option will default to ClusterObjectStore.</p>
<p>When set to &ldquo;ClusterObjectStore&rdquo;, if the capability is available and a Cluster ObjectStore configuration
is correctly provided in the Cluster object (under .spec.s3Bucket),
an object store will be used to store bootstrap user data.</p>
<p>When set to &ldquo;UnencryptedUserData&rdquo;, EC2 Instance User Data will be used to store the machine bootstrap user data, unencrypted.
This option is considered less secure than others as user data may contain sensitive informations (keys, certificates, etc.)
and users with ec2:DescribeInstances permission or users running pods
that can access the ec2 metadata service have access to this sensitive information.
So this is only to be used at ones own risk, and only when other more secure options are not viable.</p>
</td>
</tr>
<tr>
<td>
<code>proxy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionProxy">
IgnitionProxy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Proxy defines proxy settings for Ignition.
Only valid for Ignition versions 3.1 and above.</p>
</td>
</tr>
<tr>
<td>
<code>tls</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionTLS">
IgnitionTLS
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>TLS defines TLS settings for Ignition.
Only valid for Ignition versions 3.1 and above.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IgnitionCASource">IgnitionCASource
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionTLS">IgnitionTLS</a>)
</p>
<p>
<p>IgnitionCASource defines the source of the certificate authority to use for Ignition.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IgnitionNoProxy">IgnitionNoProxy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionProxy">IgnitionProxy</a>)
</p>
<p>
<p>IgnitionNoProxy defines the list of domains to not proxy for Ignition.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IgnitionProxy">IgnitionProxy
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">Ignition</a>)
</p>
<p>
<p>IgnitionProxy defines proxy settings for Ignition.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>httpProxy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HTTPProxy is the HTTP proxy to use for Ignition.
A single URL that specifies the proxy server to use for HTTP and HTTPS requests,
unless overridden by the HTTPSProxy or NoProxy options.</p>
</td>
</tr>
<tr>
<td>
<code>httpsProxy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HTTPSProxy is the HTTPS proxy to use for Ignition.
A single URL that specifies the proxy server to use for HTTPS requests,
unless overridden by the NoProxy option.</p>
</td>
</tr>
<tr>
<td>
<code>noProxy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionNoProxy">
[]IgnitionNoProxy
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NoProxy is the list of domains to not proxy for Ignition.
Specifies a list of strings to hosts that should be excluded from proxying.</p>
<p>Each value is represented by:
- An IP address prefix (1.2.3.4)
- An IP address prefix in CIDR notation (1.2.3.<sup>4</sup>&frasl;<sub>8</sub>)
- A domain name
- A domain name matches that name and all subdomains
- A domain name with a leading . matches subdomains only
- A special DNS label (*), indicates that no proxying should be done</p>
<p>An IP address prefix and domain name can also include a literal port number (1.2.3.4:80).</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IgnitionStorageTypeOption">IgnitionStorageTypeOption
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">Ignition</a>)
</p>
<p>
<p>IgnitionStorageTypeOption defines the different storage types for Ignition.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IgnitionTLS">IgnitionTLS
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">Ignition</a>)
</p>
<p>
<p>IgnitionTLS defines TLS settings for Ignition.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>certificateAuthorities</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IgnitionCASource">
[]IgnitionCASource
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CASources defines the list of certificate authorities to use for Ignition.
The value is the certificate bundle (in PEM format). The bundle can contain multiple concatenated certificates.
Supported schemes are http, https, tftp, s3, arn, gs, and <code>data</code> (RFC 2397) URL scheme.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">IngressRule
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>IngressRule defines an AWS ingress rule for security groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>description</code><br/>
<em>
string
</em>
</td>
<td>
<p>Description provides extended information about the ingress rule.</p>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroupProtocol">
SecurityGroupProtocol
</a>
</em>
</td>
<td>
<p>Protocol is the protocol for the ingress rule. Accepted values are &ldquo;-1&rdquo; (all), &ldquo;4&rdquo; (IP in IP),&ldquo;tcp&rdquo;, &ldquo;udp&rdquo;, &ldquo;icmp&rdquo;, and &ldquo;58&rdquo; (ICMPv6), &ldquo;50&rdquo; (ESP).</p>
</td>
</tr>
<tr>
<td>
<code>fromPort</code><br/>
<em>
int64
</em>
</td>
<td>
<p>FromPort is the start of port range.</p>
</td>
</tr>
<tr>
<td>
<code>toPort</code><br/>
<em>
int64
</em>
</td>
<td>
<p>ToPort is the end of port range.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of CIDR blocks to allow access from. Cannot be specified with SourceSecurityGroupID.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6CidrBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of IPv6 CIDR blocks to allow access from. Cannot be specified with SourceSecurityGroupID.</p>
</td>
</tr>
<tr>
<td>
<code>sourceSecurityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The security group id to allow access from. Cannot be specified with CidrBlocks.</p>
</td>
</tr>
<tr>
<td>
<code>sourceSecurityGroupRoles</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroupRole">
[]SecurityGroupRole
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The security group role to allow access from. Cannot be specified with CidrBlocks.
The field will be combined with source security group IDs if specified.</p>
</td>
</tr>
<tr>
<td>
<code>natGatewaysIPsSource</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>NatGatewaysIPsSource use the NAT gateways IPs as the source for the ingress rule.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.IngressRules">IngressRules
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.IngressRule</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroup">SecurityGroup</a>)
</p>
<p>
<p>IngressRules is a slice of AWS ingress rules for security groups.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStatus">AWSClusterStatus</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AutoScalingGroup">AutoScalingGroup</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AutoScalingGroup">AutoScalingGroup</a>)
</p>
<p>
<p>Instance describes an AWS instance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instanceState</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceState">
InstanceState
</a>
</em>
</td>
<td>
<p>The current state of the instance.</p>
</td>
</tr>
<tr>
<td>
<code>type</code><br/>
<em>
string
</em>
</td>
<td>
<p>The instance type.</p>
</td>
</tr>
<tr>
<td>
<code>subnetId</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the subnet of the instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageId</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the AMI used to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the SSH key pair.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SecurityGroupIDs are one or more security group IDs this instance belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>userData</code><br/>
<em>
string
</em>
</td>
<td>
<p>UserData is the raw data script passed to the instance which is run upon bootstrap.
This field must not be base64 encoded and should only be used when running a new instance.</p>
</td>
</tr>
<tr>
<td>
<code>iamProfile</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the IAM instance profile associated with the instance, if applicable.</p>
</td>
</tr>
<tr>
<td>
<code>addresses</code><br/>
<em>
[]Cluster API api/core/v1beta1.MachineAddress
</em>
</td>
<td>
<p>Addresses contains the AWS instance associated addresses.</p>
</td>
</tr>
<tr>
<td>
<code>privateIp</code><br/>
<em>
string
</em>
</td>
<td>
<p>The private IPv4 address assigned to the instance.</p>
</td>
</tr>
<tr>
<td>
<code>publicIp</code><br/>
<em>
string
</em>
</td>
<td>
<p>The public IPv4 address assigned to the instance, if applicable.</p>
</td>
</tr>
<tr>
<td>
<code>enaSupport</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Specifies whether enhanced networking with ENA is enabled.</p>
</td>
</tr>
<tr>
<td>
<code>ebsOptimized</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Indicates whether the instance is optimized for Amazon EBS I/O.</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the root storage volume.</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaces</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Specifies ENIs attached to instance</p>
</td>
</tr>
<tr>
<td>
<code>networkInterfaceType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkInterfaceType">
NetworkInterfaceType
</a>
</em>
</td>
<td>
<p>NetworkInterfaceType is the interface type of the primary network Interface.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>The tags associated with the instance.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>Availability zone of instance</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<p>SpotMarketOptions option for configuring instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupName specifies the name of the placement group in which to launch the instance.</p>
</td>
</tr>
<tr>
<td>
<code>placementGroupPartition</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>PlacementGroupPartition is the partition number within the placement group in which to launch the instance.
This value is only valid if the placement group, referred in <code>PlacementGroupName</code>, was created with
strategy set to partition.</p>
</td>
</tr>
<tr>
<td>
<code>tenancy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Tenancy indicates if instance should run on shared or single-tenant hardware.</p>
</td>
</tr>
<tr>
<td>
<code>volumeIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IDs of the instance&rsquo;s volumes</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">
InstanceMetadataOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceMetadataOptions is the metadata options for the EC2 instance.</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsName</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">
PrivateDNSName
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSName is the options for the instance hostname.</p>
</td>
</tr>
<tr>
<td>
<code>publicIPOnLaunch</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>PublicIPOnLaunch is the option to associate a public IP on instance launch</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the target Capacity Reservation into which the instance should be launched.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MarketType specifies the type of market for the EC2 instance. Valid values include:
&ldquo;OnDemand&rdquo; (default): The instance runs as a standard OnDemand instance.
&ldquo;Spot&rdquo;: The instance runs as a Spot instance. When SpotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.
&ldquo;CapacityBlock&rdquo;: The instance utilizes pre-purchased compute capacity (capacity blocks) with AWS Capacity Reservations.
If this value is selected, CapacityReservationID must be specified to identify the target reservation.
If marketType is not specified and spotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>hostAffinity</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostAffinity specifies the dedicated host affinity setting for the instance.
When HostAffinity is set to &ldquo;host&rdquo;, an instance started onto a specific host always restarts on the same host if stopped:
- If HostID is set, the instance launches on the specific host and must return to that same host after any stop/start (Targeted &amp; Pinned).
- If HostID is not set, the instance gets launched on any available and must returns to the same host after any stop/start (Auto-placed &amp; Pinned).
When HostAffinity is set to &ldquo;default&rdquo; (the default value), the instance (when restarted) can return on any available host:
- If HostID is set, the instance launches on the specified host now, but (when restarted) can return to any available hosts (Targeted &amp; Flexible).
- If HostID is not set, the instance launches on any available host now, and (when restarted) can return to any available hosts (Auto-placed &amp; Flexible).</p>
</td>
</tr>
<tr>
<td>
<code>hostID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostID specifies the dedicated host on which the instance should be started.</p>
</td>
</tr>
<tr>
<td>
<code>dynamicHostAllocation</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.DynamicHostAllocationSpec">
DynamicHostAllocationSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>DynamicHostAllocation enables automatic allocation of dedicated hosts.
This field is mutually exclusive with HostID.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationPreference</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationPreference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
&ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
&ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
&ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation. Note that this is incompatible with a MarketType of <code>Spot</code></p>
</td>
</tr>
<tr>
<td>
<code>cpuOptions,omitempty,omitzero</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CPUOptions">
CPUOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CPUOptions defines CPU-related settings for the instance, including the confidential computing policy.
When omitted, this means no opinion and the AWS platform is left to choose a reasonable default.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">InstanceMetadataOptions
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>InstanceMetadataOptions describes metadata options for the EC2 instance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>httpEndpoint</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataState">
InstanceMetadataState
</a>
</em>
</td>
<td>
<p>Enables or disables the HTTP metadata endpoint on your instances.</p>
<p>If you specify a value of disabled, you cannot access your instance metadata.</p>
<p>Default: enabled</p>
</td>
</tr>
<tr>
<td>
<code>httpPutResponseHopLimit</code><br/>
<em>
int64
</em>
</td>
<td>
<p>The desired HTTP PUT response hop limit for instance metadata requests. The
larger the number, the further instance metadata requests can travel.</p>
<p>Default: 1</p>
</td>
</tr>
<tr>
<td>
<code>httpTokens</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.HTTPTokensState">
HTTPTokensState
</a>
</em>
</td>
<td>
<p>The state of token usage for your instance metadata requests.</p>
<p>If the state is optional, you can choose to retrieve instance metadata with
or without a session token on your request. If you retrieve the IAM role
credentials without a token, the version 1.0 role credentials are returned.
If you retrieve the IAM role credentials using a valid session token, the
version 2.0 role credentials are returned.</p>
<p>If the state is required, you must send a session token with any instance
metadata retrieval requests. In this state, retrieving the IAM role credentials
always returns the version 2.0 credentials; the version 1.0 credentials are
not available.</p>
<p>Default: optional</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataState">
InstanceMetadataState
</a>
</em>
</td>
<td>
<p>Set to enabled to allow access to instance tags from the instance metadata.
Set to disabled to turn off access to instance tags from the instance metadata.
For more information, see Work with instance tags using the instance metadata
(<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#work-with-tags-in-IMDS">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#work-with-tags-in-IMDS</a>).</p>
<p>Default: disabled</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataState">InstanceMetadataState
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">InstanceMetadataOptions</a>)
</p>
<p>
<p>InstanceMetadataState describes the state of InstanceMetadataOptions.HttpEndpoint and InstanceMetadataOptions.InstanceMetadataTags</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.InstanceState">InstanceState
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineStatus">AWSMachineStatus</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>)
</p>
<p>
<p>InstanceState describes the state of an AWS instance.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Listener">Listener
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>Listener defines an AWS network load balancer listener.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>targetGroup</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupSpec">
TargetGroupSpec
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">NetworkStatus</a>)
</p>
<p>
<p>LoadBalancer defines an AWS load balancer.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>arn</code><br/>
<em>
string
</em>
</td>
<td>
<p>ARN of the load balancer. Unlike the ClassicLB, ARN is used mostly
to define and get it.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The name of the load balancer. It must be unique within the set of load balancers
defined in the region. It also serves as identifier.</p>
</td>
</tr>
<tr>
<td>
<code>dnsName</code><br/>
<em>
string
</em>
</td>
<td>
<p>DNSName is the dns name of the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>scheme</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBScheme">
ELBScheme
</a>
</em>
</td>
<td>
<p>Scheme is the load balancer scheme, either internet-facing or private.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones in the VPC attached to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SubnetIDs is an array of subnets in the VPC attached to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupIds</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SecurityGroupIDs is an array of security groups assigned to the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>listeners</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBListener">
[]ClassicELBListener
</a>
</em>
</td>
<td>
<p>ClassicELBListeners is an array of classic elb listeners associated with the load balancer. There must be at least one.</p>
</td>
</tr>
<tr>
<td>
<code>healthChecks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBHealthCheck">
ClassicELBHealthCheck
</a>
</em>
</td>
<td>
<p>HealthCheck is the classic elb health check associated with the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>attributes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ClassicELBAttributes">
ClassicELBAttributes
</a>
</em>
</td>
<td>
<p>ClassicElbAttributes defines extra attributes associated with the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>Tags is a map of tags associated with the load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>elbListeners</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Listener">
[]Listener
</a>
</em>
</td>
<td>
<p>ELBListeners is an array of listeners associated with the load balancer. There must be at least one.</p>
</td>
</tr>
<tr>
<td>
<code>elbAttributes</code><br/>
<em>
map[string]*string
</em>
</td>
<td>
<p>ELBAttributes defines extra attributes associated with v2 load balancers.</p>
</td>
</tr>
<tr>
<td>
<code>loadBalancerType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancerType">
LoadBalancerType
</a>
</em>
</td>
<td>
<p>LoadBalancerType sets the type for a load balancer. The default type is classic.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancerAttribute">LoadBalancerAttribute
(<code>string</code> alias)</p></h3>
<p>
<p>LoadBalancerAttribute defines a set of attributes for a V2 load balancer.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancerType">LoadBalancerType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">LoadBalancer</a>)
</p>
<p>
<p>LoadBalancerType defines the type of load balancer to use.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.MarketType">MarketType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>MarketType describes the market type of an Instance</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.NetworkInterfaceType">NetworkInterfaceType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>)
</p>
<p>
<p>NetworkInterfaceType is the type of network interface.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>)
</p>
<p>
<p>NetworkSpec encapsulates all things related to AWS network.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>vpc</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">
VPCSpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>VPC configuration.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Subnets">
Subnets
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets configuration.</p>
</td>
</tr>
<tr>
<td>
<code>cni</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CNISpec">
CNISpec
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CNI configuration</p>
</td>
</tr>
<tr>
<td>
<code>securityGroupOverrides</code><br/>
<em>
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroupRole]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityGroupOverrides is an optional set of security groups to use for cluster instances
This is optional - if not provided new security groups will be created for the cluster</p>
</td>
</tr>
<tr>
<td>
<code>additionalControlPlaneIngressRules</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">
[]IngressRule
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalControlPlaneIngressRules is an optional set of ingress rules to add to the control plane</p>
</td>
</tr>
<tr>
<td>
<code>additionalNodeIngressRules</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">
[]IngressRule
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalNodeIngressRules is an optional set of ingress rules to add to every node</p>
</td>
</tr>
<tr>
<td>
<code>nodePortIngressRuleCidrBlocks</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodePortIngressRuleCidrBlocks is an optional set of CIDR blocks to allow traffic to nodes&rsquo; NodePort services.
If none are specified here, all IPs are allowed to connect.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">NetworkStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterStatus">AWSClusterStatus</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneStatus">AWSManagedControlPlaneStatus</a>)
</p>
<p>
<p>NetworkStatus encapsulates AWS networking resources.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>securityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroup">
map[sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroupRole]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SecurityGroup
</a>
</em>
</td>
<td>
<p>SecurityGroups is a map from the role/kind of the security group to its unique name, if any.</p>
</td>
</tr>
<tr>
<td>
<code>apiServerElb</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">
LoadBalancer
</a>
</em>
</td>
<td>
<p>APIServerELB is the Kubernetes api server load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryAPIServerELB</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LoadBalancer">
LoadBalancer
</a>
</em>
</td>
<td>
<p>SecondaryAPIServerELB is the secondary Kubernetes api server load balancer.</p>
</td>
</tr>
<tr>
<td>
<code>natGatewaysIPs</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>NatGatewaysIPs contains the public IPs of the NAT Gateways</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.NodeInfo">NodeInfo
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineTemplateStatus">AWSMachineTemplateStatus</a>)
</p>
<p>
<p>NodeInfo contains information about the node&rsquo;s architecture and operating system.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>architecture</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Architecture">
Architecture
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Architecture is the CPU architecture of the node.
Its underlying type is a string and its value can be any of amd64, arm64.</p>
</td>
</tr>
<tr>
<td>
<code>operatingSystem</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OperatingSystem">
OperatingSystem
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>OperatingSystem is the operating system of the node.
Its underlying type is a string and its value can be any of linux, windows.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.OperatingSystem">OperatingSystem
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NodeInfo">NodeInfo</a>)
</p>
<p>
<p>OperatingSystem represents the operating system of the node.
Its underlying type is a string and its value can be any of linux, windows.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">PrivateDNSName
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>PrivateDNSName is the options for the instance hostname.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enableResourceNameDnsAAAARecord</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableResourceNameDNSAAAARecord indicates whether to respond to DNS queries for instance hostnames with DNS AAAA records.</p>
</td>
</tr>
<tr>
<td>
<code>enableResourceNameDnsARecord</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableResourceNameDNSARecord indicates whether to respond to DNS queries for instance hostnames with DNS A records.</p>
</td>
</tr>
<tr>
<td>
<code>hostnameType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The type of hostname to assign to an instance.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.PublicIpv4PoolFallbackOrder">PublicIpv4PoolFallbackOrder
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">ElasticIPPool</a>)
</p>
<p>
<p>PublicIpv4PoolFallbackOrder defines the list of available fallback action when the PublicIpv4Pool is exhausted.
&lsquo;none&rsquo; let the controllers return failures when the PublicIpv4Pool is exhausted - no more IPv4 available.
&lsquo;amazon-pool&rsquo; let the controllers to skip the PublicIpv4Pool and use the Amazon pool, the default.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ResourceLifecycle">ResourceLifecycle
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.BuildParams">BuildParams</a>)
</p>
<p>
<p>ResourceLifecycle configures the lifecycle of a resource.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RouteTable">RouteTable
</h3>
<p>
<p>RouteTable defines an AWS routing table.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.S3Bucket">S3Bucket
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>)
</p>
<p>
<p>S3Bucket defines a supporting S3 bucket for the cluster, currently can be optionally used for Ignition.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlaneIAMInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneIAMInstanceProfile is a name of the IAMInstanceProfile, which will be allowed
to read control-plane node bootstrap data from S3 Bucket.</p>
</td>
</tr>
<tr>
<td>
<code>nodesIAMInstanceProfiles</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodesIAMInstanceProfiles is a list of IAM instance profiles, which will be allowed to read
worker nodes bootstrap data from S3 Bucket.</p>
</td>
</tr>
<tr>
<td>
<code>presignedURLDuration</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PresignedURLDuration defines the duration for which presigned URLs are valid.</p>
<p>This is used to generate presigned URLs for S3 Bucket objects, which are used by
control-plane and worker nodes to fetch bootstrap data.</p>
<p>When enabled, the IAM instance profiles specified are not used.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name defines name of S3 Bucket to be created.</p>
</td>
</tr>
<tr>
<td>
<code>bestEffortDeleteObjects</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>BestEffortDeleteObjects defines whether access/permission errors during object deletion should be ignored.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SecretBackend">SecretBackend
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CloudInit">CloudInit</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMConfigurationSpec">AWSIAMConfigurationSpec</a>)
</p>
<p>
<p>SecretBackend defines variants for backend secret storage.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroup">SecurityGroup
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkStatus">NetworkStatus</a>)
</p>
<p>
<p>SecurityGroup defines an AWS security group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is a unique identifier.</p>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name is the security group name.</p>
</td>
</tr>
<tr>
<td>
<code>ingressRule</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRules">
IngressRules
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IngressRules is the inbound rules associated with the security group.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a map of tags associated with the security group.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroupProtocol">SecurityGroupProtocol
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CNIIngressRule">CNIIngressRule</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">IngressRule</a>)
</p>
<p>
<p>SecurityGroupProtocol defines the protocol type for a security group rule.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroupRole">SecurityGroupRole
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IngressRule">IngressRule</a>)
</p>
<p>
<p>SecurityGroupRole defines the unique role of a security group.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">SpotMarketOptions
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">AWSLaunchTemplate</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>SpotMarketOptions defines the options available to a user when configuring
Machines to run on Spot instances.
Most users should provide an empty struct.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxPrice</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxPrice defines the maximum price the user is willing to pay for Spot VM instances</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SubnetSchemaType">SubnetSchemaType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>SubnetSchemaType specifies how given network should be divided on subnets
in the VPC depending on the number of AZs.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SubnetSpec">SubnetSpec
</h3>
<p>
<p>SubnetSpec configures an AWS Subnet.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID defines a unique identifier to reference this resource.
If you&rsquo;re bringing your subnet, set the AWS subnet-id here, it must start with <code>subnet-</code>.</p>
<p>When the VPC is managed by CAPA, and you&rsquo;d like the provider to create a subnet for you,
the id can be set to any placeholder value that does not start with <code>subnet-</code>;
upon creation, the subnet AWS identifier will be populated in the <code>ResourceID</code> field and
the <code>id</code> field is going to be used as the subnet name. If you specify a tag
called <code>Name</code>, it takes precedence.</p>
</td>
</tr>
<tr>
<td>
<code>resourceID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ResourceID is the subnet identifier from AWS, READ ONLY.
This field is populated when the provider manages the subnet.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CidrBlock is the CIDR block to be used when the provider creates a managed VPC.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6CidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6CidrBlock is the IPv6 CIDR block to be used when the provider creates a managed VPC.
A subnet can have an IPv4 and an IPv6 address.
IPv6 is only supported in managed clusters, this field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>AvailabilityZone defines the availability zone to use for this subnet in the cluster&rsquo;s region.</p>
</td>
</tr>
<tr>
<td>
<code>isPublic</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>IsPublic defines the subnet as a public subnet. A subnet is public when it is associated with a route table that has a route to an internet gateway.</p>
</td>
</tr>
<tr>
<td>
<code>isIpv6</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>IsIPv6 defines the subnet as an IPv6 subnet. A subnet is IPv6 when it is associated with a VPC that has IPv6 enabled.
IPv6 is only supported in managed clusters, this field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>routeTableId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RouteTableID is the routing table id associated with the subnet.</p>
</td>
</tr>
<tr>
<td>
<code>natGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NatGatewayID is the NAT gateway id associated with the subnet.
Ignored unless the subnet is managed by the provider, in which case this is set on the public subnet where the NAT gateway resides. It is then used to determine routes for private subnets in the same AZ as the public subnet.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a collection of tags describing the resource.</p>
</td>
</tr>
<tr>
<td>
<code>zoneType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ZoneType">
ZoneType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ZoneType defines the type of the zone where the subnet is created.</p>
<p>The valid values are availability-zone, local-zone, and wavelength-zone.</p>
<p>Subnet with zone type availability-zone (regular) is always selected to create cluster
resources, like Load Balancers, NAT Gateways, Contol Plane nodes, etc.</p>
<p>Subnet with zone type local-zone or wavelength-zone is not eligible to automatically create
regular cluster resources.</p>
<p>The public subnet in availability-zone or local-zone is associated with regular public
route table with default route entry to a Internet Gateway.</p>
<p>The public subnet in wavelength-zone is associated with a carrier public
route table with default route entry to a Carrier Gateway.</p>
<p>The private subnet in the availability-zone is associated with a private route table with
the default route entry to a NAT Gateway created in that zone.</p>
<p>The private subnet in the local-zone or wavelength-zone is associated with a private route table with
the default route entry re-using the NAT Gateway in the Region (preferred from the
parent zone, the zone type availability-zone in the region, or first table available).</p>
</td>
</tr>
<tr>
<td>
<code>parentZoneName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ParentZoneName is the zone name where the current subnet&rsquo;s zone is tied when
the zone is a Local Zone.</p>
<p>The subnets in Local Zone or Wavelength Zone locations consume the ParentZoneName
to select the correct private route table to egress traffic to the internet.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Subnets">Subnets
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/api/v1beta2.SubnetSpec</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>Subnets is a slice of Subnet.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Tags">Tags
(<code>map[string]string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSClusterSpec">AWSClusterSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.BuildParams">BuildParams</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SecurityGroup">SecurityGroup</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SubnetSpec">SubnetSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.AWSIAMRoleSpec">AWSIAMRoleSpec</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1alpha1.BootstrapUser">BootstrapUser</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.AWSIAMRoleSpec">AWSIAMRoleSpec</a>, <a href="crd/index.html#bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1.BootstrapUser">BootstrapUser</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta1.OIDCIdentityProviderConfig">OIDCIdentityProviderConfig</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSManagedControlPlaneSpec">AWSManagedControlPlaneSpec</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.OIDCIdentityProviderConfig">OIDCIdentityProviderConfig</a>, <a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.RosaControlPlaneSpec">RosaControlPlaneSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AutoScalingGroup">AutoScalingGroup</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.FargateProfileSpec">FargateProfileSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AutoScalingGroup">AutoScalingGroup</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileSpec">FargateProfileSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">RosaMachinePoolSpec</a>)
</p>
<p>
<p>Tags defines a map of tags.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupAttribute">TargetGroupAttribute
(<code>string</code> alias)</p></h3>
<p>
<p>TargetGroupAttribute defines attribute key values for V2 Load Balancer Attributes.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheck">TargetGroupHealthCheck
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupSpec">TargetGroupSpec</a>)
</p>
<p>
<p>TargetGroupHealthCheck defines health check settings for the target group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>protocol</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>path</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>intervalSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>timeoutSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>thresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>unhealthyThresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheckAPISpec">TargetGroupHealthCheckAPISpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLoadBalancerSpec">AWSLoadBalancerSpec</a>)
</p>
<p>
<p>TargetGroupHealthCheckAPISpec defines the optional health check settings for the API target group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>intervalSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The approximate amount of time, in seconds, between health checks of an individual
target.</p>
</td>
</tr>
<tr>
<td>
<code>timeoutSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, during which no response from a target means
a failed health check.</p>
</td>
</tr>
<tr>
<td>
<code>thresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of consecutive health check successes required before considering
a target healthy.</p>
</td>
</tr>
<tr>
<td>
<code>unhealthyThresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of consecutive health check failures required before considering
a target unhealthy.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheckAdditionalSpec">TargetGroupHealthCheckAdditionalSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AdditionalListenerSpec">AdditionalListenerSpec</a>)
</p>
<p>
<p>TargetGroupHealthCheckAdditionalSpec defines the optional health check settings for the additional target groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>protocol</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The protocol to use to health check connect with the target. When not specified the Protocol
will be the same of the listener.</p>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The port the load balancer uses when performing health checks for additional target groups. When
not specified this value will be set for the same of listener port.</p>
</td>
</tr>
<tr>
<td>
<code>path</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The destination for health checks on the targets when using the protocol HTTP or HTTPS,
otherwise the path will be ignored.</p>
</td>
</tr>
<tr>
<td>
<code>intervalSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The approximate amount of time, in seconds, between health checks of an individual
target.</p>
</td>
</tr>
<tr>
<td>
<code>timeoutSeconds</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, during which no response from a target means
a failed health check.</p>
</td>
</tr>
<tr>
<td>
<code>thresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of consecutive health check successes required before considering
a target healthy.</p>
</td>
</tr>
<tr>
<td>
<code>unhealthyThresholdCount</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of consecutive health check failures required before considering
a target unhealthy.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupSpec">TargetGroupSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Listener">Listener</a>)
</p>
<p>
<p>TargetGroupSpec specifies target group settings for a given listener.
This is created first, and the ARN is then passed to the listener.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>Name of the TargetGroup. Must be unique over the same group of listeners.</p>
</td>
</tr>
<tr>
<td>
<code>port</code><br/>
<em>
int64
</em>
</td>
<td>
<p>Port is the exposed port</p>
</td>
</tr>
<tr>
<td>
<code>protocol</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ELBProtocol">
ELBProtocol
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>vpcId</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>targetGroupHealthCheck</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TargetGroupHealthCheck">
TargetGroupHealthCheck
</a>
</em>
</td>
<td>
<p>HealthCheck is the elb health check associated with the load balancer.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NetworkSpec">NetworkSpec</a>)
</p>
<p>
<p>VPCSpec configures an AWS VPC.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is the vpc-id of the VPC this provider should use to create resources.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CidrBlock is the CIDR block to be used when the provider creates a managed VPC.
Defaults to 10.0.0.0/16.
Mutually exclusive with IPAMPool.</p>
</td>
</tr>
<tr>
<td>
<code>secondaryCidrBlocks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VpcCidrBlock">
[]VpcCidrBlock
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecondaryCidrBlocks are additional CIDR blocks to be associated when the provider creates a managed VPC.
Defaults to none. Mutually exclusive with IPAMPool. This makes sense to use if, for example, you want to use
a separate IP range for pods (e.g. Cilium ENI mode).</p>
</td>
</tr>
<tr>
<td>
<code>ipamPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IPAMPool">
IPAMPool
</a>
</em>
</td>
<td>
<p>IPAMPool defines the IPAMv4 pool to be used for VPC.
Mutually exclusive with CidrBlock.</p>
</td>
</tr>
<tr>
<td>
<code>ipv6</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.IPv6">
IPv6
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IPv6 contains ipv6 specific settings for the network. Supported only in managed clusters.
This field cannot be set on AWSCluster object.</p>
</td>
</tr>
<tr>
<td>
<code>internetGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InternetGatewayID is the id of the internet gateway associated with the VPC.</p>
</td>
</tr>
<tr>
<td>
<code>carrierGatewayId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CarrierGatewayID is the id of the internet gateway associated with the VPC,
for carrier network (Wavelength Zones).</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<p>Tags is a collection of tags describing the resource.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneUsageLimit</code><br/>
<em>
int
</em>
</td>
<td>
<p>AvailabilityZoneUsageLimit specifies the maximum number of availability zones (AZ) that
should be used in a region when automatically creating subnets. If a region has more
than this number of AZs then this number of AZs will be picked randomly when creating
default subnets. Defaults to 3</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSelection</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AZSelectionScheme">
AZSelectionScheme
</a>
</em>
</td>
<td>
<p>AvailabilityZoneSelection specifies how AZs should be selected if there are more AZs
in a region than specified by AvailabilityZoneUsageLimit. There are 2 selection schemes:
Ordered - selects based on alphabetical order
Random - selects AZs randomly in a region
Defaults to Ordered</p>
</td>
</tr>
<tr>
<td>
<code>emptyRoutesDefaultVPCSecurityGroup</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EmptyRoutesDefaultVPCSecurityGroup specifies whether the default VPC security group ingress
and egress rules should be removed.</p>
<p>By default, when creating a VPC, AWS creates a security group called <code>default</code> with ingress and egress
rules that allow traffic from anywhere. The group could be used as a potential surface attack and
it&rsquo;s generally suggested that the group rules are removed or modified appropriately.</p>
<p>NOTE: This only applies when the VPC is managed by the Cluster API AWS controller.</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsHostnameTypeOnLaunch</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSHostnameTypeOnLaunch is the type of hostname to assign to instances in the subnet at launch.
For IPv4-only and dual-stack (IPv4 and IPv6) subnets, an instance DNS name can be based on the instance IPv4 address (ip-name)
or the instance ID (resource-name). For IPv6 only subnets, an instance DNS name must be based on the instance ID (resource-name).</p>
</td>
</tr>
<tr>
<td>
<code>elasticIpPool</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ElasticIPPool">
ElasticIPPool
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ElasticIPPool contains specific configuration to allocate Public IPv4 address (Elastic IP) from user-defined pool
brought to AWS for core infrastructure resources, like NAT Gateways and Public Network Load Balancers for
the API Server.</p>
</td>
</tr>
<tr>
<td>
<code>subnetSchema</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SubnetSchemaType">
SubnetSchemaType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetSchema specifies how CidrBlock should be divided on subnets in the VPC depending on the number of AZs.
PreferPrivate - one private subnet for each AZ plus one other subnet that will be further sub-divided for the public subnets.
PreferPublic - have the reverse logic of PreferPrivate, one public subnet for each AZ plus one other subnet
that will be further sub-divided for the private subnets.
Defaults to PreferPrivate</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Volume">Volume
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachineSpec">AWSMachineSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">Instance</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta1.AWSLaunchTemplate">AWSLaunchTemplate</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate</a>)
</p>
<p>
<p>Volume encapsulates the configuration options for the storage device.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Device name</p>
</td>
</tr>
<tr>
<td>
<code>size</code><br/>
<em>
int64
</em>
</td>
<td>
<p>Size specifies size (in Gi) of the storage device.
Must be greater than the image snapshot size or 8 (whichever is greater).</p>
</td>
</tr>
<tr>
<td>
<code>type</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VolumeType">
VolumeType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Type is the type of the volume (e.g. gp2, io1, etc&hellip;).</p>
</td>
</tr>
<tr>
<td>
<code>iops</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>IOPS is the number of IOPS requested for the disk. Not applicable to all types.</p>
</td>
</tr>
<tr>
<td>
<code>throughput</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>Throughput to provision in MiB/s supported for the volume type. Not applicable to all types.</p>
</td>
</tr>
<tr>
<td>
<code>encrypted</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>encryptionKey</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EncryptionKey is the KMS key to use to encrypt the volume. Can be either a KMS key ID or ARN.
If Encrypted is set and this is omitted, the default AWS key will be used.
The key must already exist and be accessible by the controller.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.VolumeType">VolumeType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">Volume</a>)
</p>
<p>
<p>VolumeType describes the EBS volume type.
See: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.VpcCidrBlock">VpcCidrBlock
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.VPCSpec">VPCSpec</a>)
</p>
<p>
<p>VpcCidrBlock defines the CIDR block and settings to associate with the managed VPC. Currently, only IPv4 is supported.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ipv4CidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>IPv4CidrBlock is the IPv4 CIDR block to associate with the managed VPC.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ZoneType">ZoneType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SubnetSpec">SubnetSpec</a>)
</p>
<p>
<p>ZoneType defines listener AWS Availability Zone type.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ASGStatus">ASGStatus
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolStatus">AWSMachinePoolStatus</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AutoScalingGroup">AutoScalingGroup</a>)
</p>
<p>
<p>ASGStatus is a status string returned by the autoscaling API.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSFargateProfile">AWSFargateProfile
</h3>
<p>
<p>AWSFargateProfile is the Schema for the awsfargateprofiles API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileSpec">
FargateProfileSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>clusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the Cluster this object belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>profileName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProfileName specifies the profile name.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for this fargate pool
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>selectors</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateSelector">
[]FargateSelector
</a>
</em>
</td>
<td>
<p>Selectors specify fargate pod selectors.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileStatus">
FargateProfileStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">AWSLaunchTemplate
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>AWSLaunchTemplate defines the desired state of AWSLaunchTemplate.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the launch template.</p>
</td>
</tr>
<tr>
<td>
<code>iamInstanceProfile</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name or the Amazon Resource Name (ARN) of the instance profile associated
with the IAM role for the instance. The instance profile contains the IAM
role.</p>
</td>
</tr>
<tr>
<td>
<code>ami</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AMIReference">
AMIReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMI is the reference to the AMI from which to create the machine instance.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupFormat</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImageLookupFormat is the AMI naming format to look up the image for this
machine It will be ignored if an explicit AMI is set. Supports
substitutions for {{.BaseOS}} and {{.K8sVersion}} with the base OS and
kubernetes version, respectively. The BaseOS will be the value in
ImageLookupBaseOS or ubuntu (the default), and the kubernetes version as
defined by the packages produced by kubernetes/release without v as a
prefix: 1.13.0, 1.12.5-mybuild.1, or 1.17.3. For example, the default
image format of capa-ami-{{.BaseOS}}-?{{.K8sVersion}}-* will end up
searching for AMIs that match the pattern capa-ami-ubuntu-?1.18.0-* for a
Machine that is targeting kubernetes v1.18.0 and the ubuntu base OS. See
also: <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a></p>
</td>
</tr>
<tr>
<td>
<code>imageLookupOrg</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupOrg is the AWS Organization ID to use for image lookup if AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>imageLookupBaseOS</code><br/>
<em>
string
</em>
</td>
<td>
<p>ImageLookupBaseOS is the name of the base operating system to use for
image lookup the AMI is not set.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType is the type of instance to create. Example: m4.xlarge</p>
</td>
</tr>
<tr>
<td>
<code>rootVolume</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RootVolume encapsulates the configuration options for the root volume</p>
</td>
</tr>
<tr>
<td>
<code>nonRootVolumes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Volume">
[]Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Configuration options for the non root storage volumes.</p>
</td>
</tr>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SSHKeyName is the name of the ssh key to attach to the instance. Valid values are empty string
(do not use SSH keys), a valid SSH key name, or omitted (use the default SSH key name)</p>
</td>
</tr>
<tr>
<td>
<code>versionNumber</code><br/>
<em>
int64
</em>
</td>
<td>
<p>VersionNumber is the version of the launch template that is applied.
Typically a new version is created when at least one of the following happens:
1) A new launch template spec is applied.
2) One or more parameters in an existing template is changed.
3) A new AMI is discovered.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an array of references to security groups that should be applied to the
instances. These security groups would be set in addition to any security groups defined
at the cluster level or in the actuator.</p>
</td>
</tr>
<tr>
<td>
<code>spotMarketOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotMarketOptions">
SpotMarketOptions
</a>
</em>
</td>
<td>
<p>SpotMarketOptions are options for configuring AWSMachinePool instances to be run using AWS Spot instances.</p>
</td>
</tr>
<tr>
<td>
<code>instanceMetadataOptions</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstanceMetadataOptions">
InstanceMetadataOptions
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceMetadataOptions defines the behavior for applying metadata to instances.</p>
</td>
</tr>
<tr>
<td>
<code>privateDnsName</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.PrivateDNSName">
PrivateDNSName
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>PrivateDNSName is the options for the instance hostname.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationId</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the target Capacity Reservation into which the instance should be launched.</p>
</td>
</tr>
<tr>
<td>
<code>marketType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MarketType">
MarketType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>MarketType specifies the type of market for the EC2 instance. Valid values include:
&ldquo;OnDemand&rdquo; (default): The instance runs as a standard OnDemand instance.
&ldquo;Spot&rdquo;: The instance runs as a Spot instance. When SpotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.
&ldquo;CapacityBlock&rdquo;: The instance utilizes pre-purchased compute capacity (capacity blocks) with AWS Capacity Reservations.
If this value is selected, CapacityReservationID must be specified to identify the target reservation.
If marketType is not specified and spotMarketOptions is provided, the marketType defaults to &ldquo;Spot&rdquo;.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationPreference</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CapacityReservationPreference">
CapacityReservationPreference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationPreference specifies the preference for use of Capacity Reservations by the instance. Valid values include:
&ldquo;Open&rdquo;: The instance may make use of open Capacity Reservations that match its AZ and InstanceType
&ldquo;None&rdquo;: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads
&ldquo;CapacityReservationsOnly&rdquo;: The instance will only run if matched or targeted to a Capacity Reservation</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">AWSLifecycleHook
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>AWSLifecycleHook describes an AWS lifecycle hook</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the lifecycle hook.</p>
</td>
</tr>
<tr>
<td>
<code>notificationTargetARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ARN of the notification target that Amazon EC2 Auto Scaling uses to
notify you when an instance is in the transition state for the lifecycle hook.</p>
</td>
</tr>
<tr>
<td>
<code>roleARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ARN of the IAM role that allows the Auto Scaling group to publish to the
specified notification target.</p>
</td>
</tr>
<tr>
<td>
<code>lifecycleTransition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LifecycleTransition">
LifecycleTransition
</a>
</em>
</td>
<td>
<p>The state of the EC2 instance to which to attach the lifecycle hook.</p>
</td>
</tr>
<tr>
<td>
<code>heartbeatTimeout</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The maximum time, in seconds, that an instance can remain in a Pending:Wait or
Terminating:Wait state. The maximum is 172800 seconds (48 hours) or 100 times
HeartbeatTimeout, whichever is smaller.</p>
</td>
</tr>
<tr>
<td>
<code>defaultResult</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.LifecycleHookDefaultResult">
LifecycleHookDefaultResult
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The default result for the lifecycle hook. The possible values are CONTINUE and ABANDON.</p>
</td>
</tr>
<tr>
<td>
<code>notificationMetadata</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Contains additional metadata that will be passed to the notification target.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePool">AWSMachinePool
</h3>
<p>
<p>AWSMachinePool is the Schema for the awsmachinepools API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">
AWSMachinePoolSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderID is the ARN of the associated ASG</p>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MinSize defines the minimum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MaxSize defines the maximum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSubnetType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AZSubnetType">
AZSubnetType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZoneSubnetType specifies which type of subnets to use when an availability zone is specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets is an array of subnet configurations</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<p>AWSLaunchTemplate specifies the launch template and version to use when an instance is launched.</p>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
<p>MixedInstancesPolicy describes how multiple instance types will be used by the ASG.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the identification IDs of machine instances provided by the provider.
This field must match the provider IDs as seen on the node objects corresponding to a machine pool&rsquo;s machine instances.</p>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>defaultInstanceWarmup</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, until a new instance is considered to
have finished initializing and resource consumption to become stable
after it enters the InService state.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>refreshPreferences</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RefreshPreferences">
RefreshPreferences
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RefreshPreferences describes set of preferences associated with the instance refresh request.</p>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enable or disable the capacity rebalance autoscaling group feature</p>
</td>
</tr>
<tr>
<td>
<code>suspendProcesses</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SuspendProcessesTypes">
SuspendProcessesTypes
</a>
</em>
</td>
<td>
<p>SuspendProcesses defines a list of processes to suspend for the given ASG. This is constantly reconciled.
If a process is removed from this list it will automatically be resumed.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>lifecycleHooks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">
[]AWSLifecycleHook
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLifecycleHooks specifies lifecycle hooks for the autoscaling group.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolStatus">
AWSMachinePoolStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolInstanceStatus">AWSMachinePoolInstanceStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolStatus">AWSMachinePoolStatus</a>)
</p>
<p>
<p>AWSMachinePoolInstanceStatus defines the status of the AWSMachinePoolInstance.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceID is the identification of the Machine Instance within ASG</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version defines the Kubernetes version for the Machine Instance</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePool">AWSMachinePool</a>)
</p>
<p>
<p>AWSMachinePoolSpec defines the desired state of AWSMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>providerID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderID is the ARN of the associated ASG</p>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MinSize defines the minimum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
<p>MaxSize defines the maximum size of the group.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSubnetType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AZSubnetType">
AZSubnetType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZoneSubnetType specifies which type of subnets to use when an availability zone is specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSResourceReference">
[]AWSResourceReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Subnets is an array of subnet configurations</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to an instance, in addition to the ones added by default by the
AWS provider.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<p>AWSLaunchTemplate specifies the launch template and version to use when an instance is launched.</p>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
<p>MixedInstancesPolicy describes how multiple instance types will be used by the ASG.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the identification IDs of machine instances provided by the provider.
This field must match the provider IDs as seen on the node objects corresponding to a machine pool&rsquo;s machine instances.</p>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>defaultInstanceWarmup</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of time, in seconds, until a new instance is considered to
have finished initializing and resource consumption to become stable
after it enters the InService state.
If no value is supplied by user a default value of 300 seconds is set</p>
</td>
</tr>
<tr>
<td>
<code>refreshPreferences</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RefreshPreferences">
RefreshPreferences
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RefreshPreferences describes set of preferences associated with the instance refresh request.</p>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enable or disable the capacity rebalance autoscaling group feature</p>
</td>
</tr>
<tr>
<td>
<code>suspendProcesses</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SuspendProcessesTypes">
SuspendProcessesTypes
</a>
</em>
</td>
<td>
<p>SuspendProcesses defines a list of processes to suspend for the given ASG. This is constantly reconciled.
If a process is removed from this list it will automatically be resumed.</p>
</td>
</tr>
<tr>
<td>
<code>ignition</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Ignition">
Ignition
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ignition defined options related to the bootstrapping systems where Ignition is used.</p>
</td>
</tr>
<tr>
<td>
<code>lifecycleHooks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">
[]AWSLifecycleHook
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLifecycleHooks specifies lifecycle hooks for the autoscaling group.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolStatus">AWSMachinePoolStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePool">AWSMachinePool</a>)
</p>
<p>
<p>AWSMachinePoolStatus defines the observed state of AWSMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is true when the provider resource is ready.</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the most recently observed number of replicas</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the AWSMachinePool.</p>
</td>
</tr>
<tr>
<td>
<code>instances</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolInstanceStatus">
[]AWSMachinePoolInstanceStatus
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Instances contains the status for each instance in the pool</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateID</code><br/>
<em>
string
</em>
</td>
<td>
<p>The ID of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The version of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>infrastructureMachineKind</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InfrastructureMachineKind is the kind of the infrastructure resources behind MachinePool Machines.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the Machine and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the Machine and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of Machines
can be added as events to the Machine object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>asgStatus</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ASGStatus">
ASGStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolWebhook">AWSMachinePoolWebhook
</h3>
<p>
<p>AWSMachinePoolWebhook implements a custom validation webhook for AWSMachinePool.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePool">AWSManagedMachinePool
</h3>
<p>
<p>AWSManagedMachinePool is the Schema for the awsmanagedmachinepools API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">
AWSManagedMachinePoolSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>eksNodegroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSNodegroupName specifies the name of the nodegroup in AWS
corresponding to this MachinePool. If you don&rsquo;t specify a name
then a default name will be created based on the namespace and
name of the managed machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSubnetType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AZSubnetType">
AZSubnetType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZoneSubnetType specifies which type of subnets to use when an availability zone is specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the node group role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for the node group.
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>amiVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIVersion defines the desired AMI release version. If no version number
is supplied then the latest version for the Kubernetes version
will be used</p>
</td>
</tr>
<tr>
<td>
<code>amiType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachineAMIType">
ManagedMachineAMIType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIType defines the AMI type</p>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Taints">
Taints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>diskSize</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSize specifies the root disk size</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>scaling</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolScaling">
ManagedMachinePoolScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scaling specifies scaling for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>remoteAccess</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedRemoteAccess">
ManagedRemoteAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RemoteAccess specifies how machines can be accessed remotely</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the provider IDs of instances in the
autoscaling group corresponding to the nodegroup represented by this
machine pool</p>
</td>
</tr>
<tr>
<td>
<code>capacityType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolCapacityType">
ManagedMachinePoolCapacityType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityType specifies the capacity type for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.UpdateConfig">
UpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig holds the optional config to control the behaviour of the update
to the nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLaunchTemplate specifies the launch template to use to create the managed node group.
If AWSLaunchTemplate is specified, certain node group configuraions outside of launch template
are prohibited (<a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html">https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html</a>).</p>
</td>
</tr>
<tr>
<td>
<code>lifecycleHooks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">
[]AWSLifecycleHook
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLifecycleHooks specifies lifecycle hooks for the managed node group.</p>
</td>
</tr>
<tr>
<td>
<code>nodeRepairConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NodeRepairConfig">
NodeRepairConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeRepairConfig specifies the node auto repair configuration for the managed node group.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolStatus">
AWSManagedMachinePoolStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePool">AWSManagedMachinePool</a>)
</p>
<p>
<p>AWSManagedMachinePoolSpec defines the desired state of AWSManagedMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>eksNodegroupName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>EKSNodegroupName specifies the name of the nodegroup in AWS
corresponding to this MachinePool. If you don&rsquo;t specify a name
then a default name will be created based on the namespace and
name of the managed machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>AvailabilityZones is an array of availability zones instances can run in</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneSubnetType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AZSubnetType">
AZSubnetType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZoneSubnetType specifies which type of subnets to use when an availability zone is specified.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleAdditionalPolicies</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleAdditionalPolicies allows you to attach additional polices to
the node group role. You must enable the EKSAllowAddRoles
feature flag to incorporate these into the created role.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for the node group.
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>amiVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIVersion defines the desired AMI release version. If no version number
is supplied then the latest version for the Kubernetes version
will be used</p>
</td>
</tr>
<tr>
<td>
<code>amiType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachineAMIType">
ManagedMachineAMIType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AMIType defines the AMI type</p>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Taints">
Taints
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>diskSize</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>DiskSize specifies the root disk size</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>scaling</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolScaling">
ManagedMachinePoolScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Scaling specifies scaling for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>remoteAccess</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedRemoteAccess">
ManagedRemoteAccess
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RemoteAccess specifies how machines can be accessed remotely</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList are the provider IDs of instances in the
autoscaling group corresponding to the nodegroup represented by this
machine pool</p>
</td>
</tr>
<tr>
<td>
<code>capacityType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolCapacityType">
ManagedMachinePoolCapacityType
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityType specifies the capacity type for the ASG behind this pool</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.UpdateConfig">
UpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig holds the optional config to control the behaviour of the update
to the nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>awsLaunchTemplate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLaunchTemplate">
AWSLaunchTemplate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLaunchTemplate specifies the launch template to use to create the managed node group.
If AWSLaunchTemplate is specified, certain node group configuraions outside of launch template
are prohibited (<a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html">https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html</a>).</p>
</td>
</tr>
<tr>
<td>
<code>lifecycleHooks</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">
[]AWSLifecycleHook
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AWSLifecycleHooks specifies lifecycle hooks for the managed node group.</p>
</td>
</tr>
<tr>
<td>
<code>nodeRepairConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.NodeRepairConfig">
NodeRepairConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeRepairConfig specifies the node auto repair configuration for the managed node group.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolStatus">AWSManagedMachinePoolStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePool">AWSManagedMachinePool</a>)
</p>
<p>
<p>AWSManagedMachinePoolStatus defines the observed state of AWSManagedMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the AWSManagedMachinePool nodegroup has joined
the cluster</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the most recently observed number of replicas.</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ID of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>launchTemplateVersion</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The version of the launch template</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the MachinePool and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the Machine&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of MachinePools
can be added as events to the MachinePool object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the MachinePool and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the MachinePool&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of MachinePools
can be added as events to the MachinePool object and/or logged in the
controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the managed machine pool</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AZSubnetType">AZSubnetType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>AZSubnetType is the type of subnet to use when an availability zone is specified.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;all&#34;</p></td>
<td><p>AZSubnetTypeAll is all subnets in an availability zone.</p>
</td>
</tr><tr><td><p>&#34;private&#34;</p></td>
<td><p>AZSubnetTypePrivate is a private subnet.</p>
</td>
</tr><tr><td><p>&#34;public&#34;</p></td>
<td><p>AZSubnetTypePublic is a public subnet.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AccountRoleConfig">AccountRoleConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">ROSARoleConfigSpec</a>)
</p>
<p>
<p>AccountRoleConfig defines account IAM roles before creating your ROSA cluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>prefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>User-defined prefix for all generated AWS account role</p>
</td>
</tr>
<tr>
<td>
<code>permissionsBoundaryARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ARN of the policy that is used to set the permissions boundary for the account roles.</p>
</td>
</tr>
<tr>
<td>
<code>path</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The arn path for the account/operator roles as well as their policies.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<p>Version of OpenShift that will be used to the roles tag in formate of x.y.z example; &ldquo;4.19.0&rdquo;
Setting the role OpenShift version tag does not affect the associated ROSAControlplane version.</p>
</td>
</tr>
<tr>
<td>
<code>sharedVPCConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SharedVPCConfig">
SharedVPCConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SharedVPCConfig is used to set up shared VPC.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AccountRolesRef">AccountRolesRef
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigStatus">ROSARoleConfigStatus</a>)
</p>
<p>
<p>AccountRolesRef defscribes ARNs used as Account roles.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>installerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstallerRoleARN is an AWS IAM role that OpenShift Cluster Manager will assume to create the cluster..</p>
</td>
</tr>
<tr>
<td>
<code>supportRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>SupportRoleARN is an AWS IAM role used by Red Hat SREs to enable
access to the cluster account in order to provide support.</p>
</td>
</tr>
<tr>
<td>
<code>workerRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>WorkerRoleARN is an AWS IAM role that will be attached to worker instances.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.AutoScalingGroup">AutoScalingGroup
</h3>
<p>
<p>AutoScalingGroup describes an AWS autoscaling group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>The tags associated with the instance.</p>
</td>
</tr>
<tr>
<td>
<code>tags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>name</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>desiredCapacity</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>placementGroup</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>subnets</code><br/>
<em>
[]string
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>defaultCoolDown</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>defaultInstanceWarmup</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>capacityRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>mixedInstancesPolicy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">
MixedInstancesPolicy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>Status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ASGStatus">
ASGStatus
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instances</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Instance">
[]Instance
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>currentlySuspendProcesses</code><br/>
<em>
[]string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.BlockDeviceMapping">BlockDeviceMapping
</h3>
<p>
<p>BlockDeviceMapping specifies the block devices for the instance.
You can specify virtual devices and EBS volumes.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>deviceName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The device name exposed to the EC2 instance (for example, /dev/sdh or xvdh).</p>
</td>
</tr>
<tr>
<td>
<code>ebs</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.EBS">
EBS
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>You can specify either VirtualName or Ebs, but not both.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.CFResource">CFResource
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkStatus">ROSANetworkStatus</a>)
</p>
<p>
<p>CFResource groups information pertaining to a resource created as a part of a cloudformation stack</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>resource</code><br/>
<em>
string
</em>
</td>
<td>
<p>Type of the created resource: AWS::EC2::VPC, AWS::EC2::Subnet, &hellip;</p>
</td>
</tr>
<tr>
<td>
<code>logicalId</code><br/>
<em>
string
</em>
</td>
<td>
<p>LogicalResourceID of the created resource.</p>
</td>
</tr>
<tr>
<td>
<code>physicalId</code><br/>
<em>
string
</em>
</td>
<td>
<p>PhysicalResourceID of the created resource.</p>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
string
</em>
</td>
<td>
<p>Status of the resource: CREATE_IN_PROGRESS, CREATE_COMPLETE, &hellip;</p>
</td>
</tr>
<tr>
<td>
<code>reason</code><br/>
<em>
string
</em>
</td>
<td>
<p>Message pertaining to the status of the resource</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.EBS">EBS
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.BlockDeviceMapping">BlockDeviceMapping</a>)
</p>
<p>
<p>EBS can be used to automatically set up EBS volumes when an instance is launched.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>encrypted</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Encrypted is whether the volume should be encrypted or not.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSize</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The size of the volume, in GiB.
This can be a number from 1-1,024 for standard, 4-16,384 for io1, 1-16,384
for gp2, and 500-16,384 for st1 and sc1. If you specify a snapshot, the volume
size must be equal to or larger than the snapshot size.</p>
</td>
</tr>
<tr>
<td>
<code>volumeType</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The volume type
For more information, see Amazon EBS Volume Types (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>)</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileSpec">FargateProfileSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSFargateProfile">AWSFargateProfile</a>)
</p>
<p>
<p>FargateProfileSpec defines the desired state of FargateProfile.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>clusterName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ClusterName is the name of the Cluster this object belongs to.</p>
</td>
</tr>
<tr>
<td>
<code>profileName</code><br/>
<em>
string
</em>
</td>
<td>
<p>ProfileName specifies the profile name.</p>
</td>
</tr>
<tr>
<td>
<code>subnetIDs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>SubnetIDs specifies which subnets are used for the
auto scaling group of this nodegroup.</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition to the
ones added by default.</p>
</td>
</tr>
<tr>
<td>
<code>roleName</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RoleName specifies the name of IAM role for this fargate pool
If the role is pre-existing we will treat it as unmanaged
and not delete it on deletion. If the EKSEnableIAM feature
flag is true and no name is supplied then a role is created.</p>
</td>
</tr>
<tr>
<td>
<code>rolePath</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePath sets the path to the role. For more information about paths, see IAM Identifiers
(<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/Using_Identifiers.html</a>)
in the IAM User Guide.</p>
<p>This parameter is optional. If it is not included, it defaults to a slash
(/).</p>
</td>
</tr>
<tr>
<td>
<code>rolePermissionsBoundary</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RolePermissionsBoundary sets the ARN of the managed policy that is used
to set the permissions boundary for the role.</p>
<p>A permissions boundary policy defines the maximum permissions that identity-based
policies can grant to an entity, but does not grant permissions. Permissions
boundaries do not define the maximum permissions that a resource-based policy
can grant to an entity. To learn more, see Permissions boundaries for IAM
entities (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a>)
in the IAM User Guide.</p>
<p>For more information about policy types, see Policy types (<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types</a>)
in the IAM User Guide.</p>
</td>
</tr>
<tr>
<td>
<code>selectors</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateSelector">
[]FargateSelector
</a>
</em>
</td>
<td>
<p>Selectors specify fargate pod selectors.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileStatus">FargateProfileStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSFargateProfile">AWSFargateProfile</a>)
</p>
<p>
<p>FargateProfileStatus defines the observed state of FargateProfile.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the FargateProfile is available.</p>
</td>
</tr>
<tr>
<td>
<code>failureReason</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureReason will be set in the event that there is a terminal problem
reconciling the FargateProfile and will contain a succinct value suitable
for machine interpretation.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the FargateProfile&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of
FargateProfiles can be added as events to the FargateProfile object
and/or logged in the controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the FargateProfile and will contain a more verbose string suitable
for logging and human consumption.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the FargateProfile&rsquo;s spec or the configuration of
the controller, and that manual intervention is required. Examples
of terminal errors would be invalid combinations of settings in the
spec, values that are unsupported by the controller, or the
responsible controller itself being critically misconfigured.</p>
<p>Any transient errors that occur during the reconciliation of
FargateProfiles can be added as events to the FargateProfile
object and/or logged in the controller&rsquo;s output.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current state of the Fargate profile.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.FargateSelector">FargateSelector
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.FargateProfileSpec">FargateProfileSpec</a>)
</p>
<p>
<p>FargateSelector specifies a selector for pods that should run on this fargate pool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<p>Labels specifies which pod labels this selector should match.</p>
</td>
</tr>
<tr>
<td>
<code>namespace</code><br/>
<em>
string
</em>
</td>
<td>
<p>Namespace specifies which namespace this selector should match.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.InstancesDistribution">InstancesDistribution
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">MixedInstancesPolicy</a>)
</p>
<p>
<p>InstancesDistribution to configure distribution of On-Demand Instances and Spot Instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>onDemandAllocationStrategy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OnDemandAllocationStrategy">
OnDemandAllocationStrategy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>spotAllocationStrategy</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SpotAllocationStrategy">
SpotAllocationStrategy
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>onDemandBaseCapacity</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>onDemandPercentageAboveBaseCapacity</code><br/>
<em>
int64
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.LifecycleHookDefaultResult">LifecycleHookDefaultResult
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">AWSLifecycleHook</a>)
</p>
<p>
<p>LifecycleHookDefaultResult is the default result for the lifecycle hook.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;ABANDON&#34;</p></td>
<td><p>LifecycleHookDefaultResultAbandon is the default result for the lifecycle hook to abandon.</p>
</td>
</tr><tr><td><p>&#34;CONTINUE&#34;</p></td>
<td><p>LifecycleHookDefaultResultContinue is the default result for the lifecycle hook to continue.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.LifecycleTransition">LifecycleTransition
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSLifecycleHook">AWSLifecycleHook</a>)
</p>
<p>
<p>LifecycleTransition is the state of the EC2 instance to which to attach the lifecycle hook.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;autoscaling:EC2_INSTANCE_LAUNCHING&#34;</p></td>
<td><p>LifecycleHookTransitionInstanceLaunching is the launching state of the EC2 instance.</p>
</td>
</tr><tr><td><p>&#34;autoscaling:EC2_INSTANCE_TERMINATING&#34;</p></td>
<td><p>LifecycleHookTransitionInstanceTerminating is the terminating state of the EC2 instance.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachineAMIType">ManagedMachineAMIType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachineAMIType specifies which AWS AMI to use for a managed MachinePool.
Source of truth can be found using the link below:
<a href="https://docs.aws.amazon.com/eks/latest/APIReference/API_CreateNodegroup.html#AmazonEKS-CreateNodegroup-request-amiType">https://docs.aws.amazon.com/eks/latest/APIReference/API_CreateNodegroup.html#AmazonEKS-CreateNodegroup-request-amiType</a></p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;AL2023_ARM_64_STANDARD&#34;</p></td>
<td><p>Al2023Arm64 is the AL2023 Arm AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2023_ARM_64_NVIDIA&#34;</p></td>
<td><p>Al2023Arm64Nvidia is the AL2023 Arm Nvidia AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2023_x86_64_STANDARD&#34;</p></td>
<td><p>Al2023x86_64 is the AL2023 x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2023_x86_64_NEURON&#34;</p></td>
<td><p>Al2023x86_64Neuron is the AL2023 x86-64 Neuron AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2023_x86_64_NVIDIA&#34;</p></td>
<td><p>Al2023x86_64Nvidia is the AL2023 x86-64 Nvidia AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_ARM_64&#34;</p></td>
<td><p>Al2Arm64 is the Arm AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_x86_64&#34;</p></td>
<td><p>Al2x86_64 is the default AMI type.</p>
</td>
</tr><tr><td><p>&#34;AL2_x86_64_GPU&#34;</p></td>
<td><p>Al2x86_64GPU is the x86-64 GPU AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_ARM_64&#34;</p></td>
<td><p>BottleRocketArm64 is the Arm AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_ARM_64_FIPS&#34;</p></td>
<td><p>BottleRocketArm64Fips is the BottleRocket Arm Fips AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_ARM_64_NVIDIA&#34;</p></td>
<td><p>BottleRocketArm64Nvidia is the BottleRocket Arm Nvidia AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_x86_64&#34;</p></td>
<td><p>BottleRocketx86_64 is the BottleRocket x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_x86_64_FIPS&#34;</p></td>
<td><p>BottleRocketx86_64Fips is the BottleRocket x86-64 Fips AMI type.</p>
</td>
</tr><tr><td><p>&#34;BOTTLEROCKET_x86_64_NVIDIA&#34;</p></td>
<td><p>BottleRocketx86_64Nvidia is the BottleRocket x86-64 Nvidia AMI type.</p>
</td>
</tr><tr><td><p>&#34;CUSTOM&#34;</p></td>
<td><p>Custom is the custom AMI type.</p>
</td>
</tr><tr><td><p>&#34;WINDOWS_CORE_2019_x86_64&#34;</p></td>
<td><p>WindowsCore2019x86_64 is the Windows Core 2019 x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;WINDOWS_CORE_2022_x86_64&#34;</p></td>
<td><p>WindowsCore2022x86_64 is the Windows Core 2022 x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;WINDOWS_FULL_2019_x86_64&#34;</p></td>
<td><p>WindowsFull2019x86_64 is the Windows Full 2019 x86-64 AMI type.</p>
</td>
</tr><tr><td><p>&#34;WINDOWS_FULL_2022_x86_64&#34;</p></td>
<td><p>WindowsFull2022x86_64 is the Windows Full 2022 x86-64 AMI type.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolCapacityType">ManagedMachinePoolCapacityType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachinePoolCapacityType specifies the capacity type to be used for the managed MachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;onDemand&#34;</p></td>
<td><p>ManagedMachinePoolCapacityTypeOnDemand is the default capacity type, to launch on-demand instances.</p>
</td>
</tr><tr><td><p>&#34;spot&#34;</p></td>
<td><p>ManagedMachinePoolCapacityTypeSpot is the spot instance capacity type to launch spot instances.</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ManagedMachinePoolScaling">ManagedMachinePoolScaling
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedMachinePoolScaling specifies scaling options.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>minSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>maxSize</code><br/>
<em>
int32
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ManagedRemoteAccess">ManagedRemoteAccess
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>ManagedRemoteAccess specifies remote access settings for EC2 instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>sshKeyName</code><br/>
<em>
string
</em>
</td>
<td>
<p>SSHKeyName specifies which EC2 SSH key can be used to access machines.
If left empty, the key from the control plane is used.</p>
</td>
</tr>
<tr>
<td>
<code>sourceSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>SourceSecurityGroups specifies which security groups are allowed access</p>
</td>
</tr>
<tr>
<td>
<code>public</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Public specifies whether to open port 22 to the public internet</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">MixedInstancesPolicy
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AutoScalingGroup">AutoScalingGroup</a>)
</p>
<p>
<p>MixedInstancesPolicy for an Auto Scaling group.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instancesDistribution</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstancesDistribution">
InstancesDistribution
</a>
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>overrides</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Overrides">
[]Overrides
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.NodeRepairConfig">NodeRepairConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>NodeRepairConfig defines the node auto repair configuration for managed node groups.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>enabled</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Enabled specifies whether node auto repair is enabled for the node group.
When enabled, EKS will automatically repair unhealthy nodes by replacing them.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.OidcProviderType">OidcProviderType
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">ROSARoleConfigSpec</a>)
</p>
<p>
<p>OidcProviderType set to Managed or UnManaged</p>
</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr><td><p>&#34;Managed&#34;</p></td>
<td><p>Managed OIDC Provider type</p>
</td>
</tr><tr><td><p>&#34;Unmanaged&#34;</p></td>
<td><p>Unmanaged OIDC Provider type</p>
</td>
</tr></tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.OnDemandAllocationStrategy">OnDemandAllocationStrategy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstancesDistribution">InstancesDistribution</a>)
</p>
<p>
<p>OnDemandAllocationStrategy indicates how to allocate instance types to fulfill On-Demand capacity.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.OperatorRoleConfig">OperatorRoleConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">ROSARoleConfigSpec</a>)
</p>
<p>
<p>OperatorRoleConfig defines cluster-specific operator IAM roles based on your cluster configuration.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>prefix</code><br/>
<em>
string
</em>
</td>
<td>
<p>User-defined prefix for generated AWS operator roles.</p>
</td>
</tr>
<tr>
<td>
<code>permissionsBoundaryARN</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The ARN of the policy that is used to set the permissions boundary for the operator roles.</p>
</td>
</tr>
<tr>
<td>
<code>sharedVPCConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SharedVPCConfig">
SharedVPCConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SharedVPCConfig is used to set up shared VPC.</p>
</td>
</tr>
<tr>
<td>
<code>oidcID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>OIDCID is the ID of the OIDC config that will be used to create the operator roles.
Cannot be set when OidcProviderType set to Managed</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Overrides">Overrides
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.MixedInstancesPolicy">MixedInstancesPolicy</a>)
</p>
<p>
<p>Overrides are used to override the instance type specified by the launch template with multiple
instance types that can be used to launch On-Demand Instances and Spot Instances.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Processes">Processes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.SuspendProcessesTypes">SuspendProcessesTypes</a>)
</p>
<p>
<p>Processes defines the processes which can be enabled or disabled individually.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>launch</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>terminate</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>addToLoadBalancer</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>alarmNotification</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>azRebalance</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>healthCheck</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>instanceRefresh</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>replaceUnhealthy</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>scheduledActions</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSACluster">ROSACluster
</h3>
<p>
<p>ROSACluster is the Schema for the ROSAClusters API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSAClusterSpec">
ROSAClusterSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSAClusterStatus">
ROSAClusterStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSAClusterSpec">ROSAClusterSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSACluster">ROSACluster</a>)
</p>
<p>
<p>ROSAClusterSpec defines the desired state of ROSACluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>controlPlaneEndpoint</code><br/>
<em>
Cluster API api/core/v1beta1.APIEndpoint
</em>
</td>
<td>
<em>(Optional)</em>
<p>ControlPlaneEndpoint represents the endpoint used to communicate with the control plane.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSAClusterStatus">ROSAClusterStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSACluster">ROSACluster</a>)
</p>
<p>
<p>ROSAClusterStatus defines the observed state of ROSACluster.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Ready is when the ROSAControlPlane has a API server URL.</p>
</td>
</tr>
<tr>
<td>
<code>failureDomains</code><br/>
<em>
Cluster API api/core/v1beta1.FailureDomains
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureDomains specifies a list fo available availability zones that can be used</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the ROSACluster.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSAMachinePool">ROSAMachinePool
</h3>
<p>
<p>ROSAMachinePool is the Schema for the rosamachinepools API.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">
RosaMachinePoolSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>nodePoolName</code><br/>
<em>
string
</em>
</td>
<td>
<p>NodePoolName specifies the name of the nodepool in Rosa
must be a valid DNS-1035 label, so it must consist of lower case alphanumeric and have a max length of 15 characters.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version specifies the OpenShift version of the nodes associated with this machinepool.
ROSAControlPlane version is used if not set.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZone is an optinal field specifying the availability zone where instances of this machine pool should run
For Multi-AZ clusters, you can create a machine pool in a Single-AZ of your choice.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaTaint">
[]RosaTaint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags are user-defined tags to be added on the underlying EC2 instances associated with this machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>autoRepair</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AutoRepair specifies whether health checks should be enabled for machines
in the NodePool. The default is true.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoScaling">
AutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto scaling behaviour for this MachinePool.
required if Replicas is not configured</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfigs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>TuningConfigs specifies the names of the tuning configs to be applied to this MachinePool.
Tuning configs must already exist.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an optional set of security groups to associate
with all node instances of the machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSize</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>VolumeSize set the disk volume size for the machine pool, in Gib. The default is 300 GiB.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList contain a ProviderID for each machine instance that&rsquo;s currently managed by this machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainGracePeriod</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeDrainGracePeriod is grace period for how long Pod Disruption Budget-protected workloads will be
respected during upgrades. After this grace period, any workloads protected by Pod Disruption
Budgets that have not been successfully drained from a node will be forcibly evicted.</p>
<p>Valid values are from 0 to 1 week(10080m|168h) .
0 or empty value means that the MachinePool can be drained without any time limitation.</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaUpdateConfig">
RosaUpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig specifies update configurations.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the ID of an AWS On-Demand Capacity Reservation and Capacity Blocks for ML.
The CapacityReservationID must be pre-created in advance, before creating a NodePool.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolStatus">
RosaMachinePoolStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSANetwork">ROSANetwork
</h3>
<p>
<p>ROSANetwork is the schema for the rosanetworks API</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSpec">
ROSANetworkSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the cloudformation stack under which the network infrastructure would be created</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS region in which the components of ROSA network infrastruture are to be crated</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneCount</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of availability zones to be used for creation of the network infrastructure.
You can specify anything between one and four, depending on the chosen AWS region.
Either AvailabilityZoneCount OR AvailabilityZones must be set.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The list of availability zones to be used for creation of the network infrastructure.
You can specify anything between one and four valid availability zones from a given region.
Either AvailabilityZones OR AvailabilityZoneCount must be set.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CIDR block to be used for the VPC</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling rosa network.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>stackTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StackTags is an optional set of tags to add to the created cloudformation stack.
The stack tags will then be automatically applied to the supported AWS resources (VPC, subnets, &hellip;).</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkStatus">
ROSANetworkStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSpec">ROSANetworkSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetwork">ROSANetwork</a>)
</p>
<p>
<p>ROSANetworkSpec defines the desired state of ROSANetwork</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>stackName</code><br/>
<em>
string
</em>
</td>
<td>
<p>The name of the cloudformation stack under which the network infrastructure would be created</p>
</td>
</tr>
<tr>
<td>
<code>region</code><br/>
<em>
string
</em>
</td>
<td>
<p>The AWS region in which the components of ROSA network infrastruture are to be crated</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZoneCount</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of availability zones to be used for creation of the network infrastructure.
You can specify anything between one and four, depending on the chosen AWS region.
Either AvailabilityZoneCount OR AvailabilityZones must be set.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZones</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The list of availability zones to be used for creation of the network infrastructure.
You can specify anything between one and four valid availability zones from a given region.
Either AvailabilityZones OR AvailabilityZoneCount must be set.</p>
</td>
</tr>
<tr>
<td>
<code>cidrBlock</code><br/>
<em>
string
</em>
</td>
<td>
<p>CIDR block to be used for the VPC</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling rosa network.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>stackTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>StackTags is an optional set of tags to add to the created cloudformation stack.
The stack tags will then be automatically applied to the supported AWS resources (VPC, subnets, &hellip;).</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkStatus">ROSANetworkStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetwork">ROSANetwork</a>)
</p>
<p>
<p>ROSANetworkStatus defines the observed state of ROSANetwork</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>subnets</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSubnet">
[]ROSANetworkSubnet
</a>
</em>
</td>
<td>
<p>Array of created private, public subnets and availability zones, grouped by availability zones</p>
</td>
</tr>
<tr>
<td>
<code>resources</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.CFResource">
[]CFResource
</a>
</em>
</td>
<td>
<p>Resources created in the cloudformation stack</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<p>Conditions specifies the conditions for ROSANetwork</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSubnet">ROSANetworkSubnet
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkStatus">ROSANetworkStatus</a>)
</p>
<p>
<p>ROSANetworkSubnet groups public and private subnet and the availability zone in which the two subnets got created</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<p>Availability zone of the subnet pair, for example us-west-2a</p>
</td>
</tr>
<tr>
<td>
<code>publicSubnet</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID of the public subnet, for example subnet-0f7e49a3ce68ff338</p>
</td>
</tr>
<tr>
<td>
<code>privateSubnet</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID of the private subnet, for example subnet-07a20d6c41af2b725</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfig">ROSARoleConfig
</h3>
<p>
<p>ROSARoleConfig is the Schema for the rosaroleconfigs API</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">
ROSARoleConfigSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>accountRoleConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AccountRoleConfig">
AccountRoleConfig
</a>
</em>
</td>
<td>
<p>AccountRoleConfig defines account-wide IAM roles before creating your ROSA cluster.</p>
</td>
</tr>
<tr>
<td>
<code>operatorRoleConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OperatorRoleConfig">
OperatorRoleConfig
</a>
</em>
</td>
<td>
<p>OperatorRoleConfig defines cluster-specific operator IAM roles based on your cluster configuration.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the ROSA Role Config.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CredentialsSecretRef references a secret with necessary credentials to connect to the OCM API.</p>
</td>
</tr>
<tr>
<td>
<code>oidcProviderType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OidcProviderType">
OidcProviderType
</a>
</em>
</td>
<td>
<p>OIDC provider type values are Managed or UnManaged. When set to Unmanged OperatorRoleConfig OIDCID field must be provided.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigStatus">
ROSARoleConfigStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigSpec">ROSARoleConfigSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfig">ROSARoleConfig</a>)
</p>
<p>
<p>ROSARoleConfigSpec defines the desired state of ROSARoleConfig</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>accountRoleConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AccountRoleConfig">
AccountRoleConfig
</a>
</em>
</td>
<td>
<p>AccountRoleConfig defines account-wide IAM roles before creating your ROSA cluster.</p>
</td>
</tr>
<tr>
<td>
<code>operatorRoleConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OperatorRoleConfig">
OperatorRoleConfig
</a>
</em>
</td>
<td>
<p>OperatorRoleConfig defines cluster-specific operator IAM roles based on your cluster configuration.</p>
</td>
</tr>
<tr>
<td>
<code>identityRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSIdentityReference">
AWSIdentityReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>IdentityRef is a reference to an identity to be used when reconciling the ROSA Role Config.
If no identity is specified, the default identity for this controller will be used.</p>
</td>
</tr>
<tr>
<td>
<code>credentialsSecretRef</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#localobjectreference-v1-core">
Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>CredentialsSecretRef references a secret with necessary credentials to connect to the OCM API.</p>
</td>
</tr>
<tr>
<td>
<code>oidcProviderType</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OidcProviderType">
OidcProviderType
</a>
</em>
</td>
<td>
<p>OIDC provider type values are Managed or UnManaged. When set to Unmanged OperatorRoleConfig OIDCID field must be provided.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfigStatus">ROSARoleConfigStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSARoleConfig">ROSARoleConfig</a>)
</p>
<p>
<p>ROSARoleConfigStatus defines the observed state of ROSARoleConfig</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>oidcID</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID of created OIDC config</p>
</td>
</tr>
<tr>
<td>
<code>oidcProviderARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>Create OIDC provider for operators to authenticate against in an STS cluster.</p>
</td>
</tr>
<tr>
<td>
<code>accountRolesRef</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AccountRolesRef">
AccountRolesRef
</a>
</em>
</td>
<td>
<p>Created Account roles that can be used to</p>
</td>
</tr>
<tr>
<td>
<code>operatorRolesRef</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AWSRolesRef">
AWSRolesRef
</a>
</em>
</td>
<td>
<p>AWS IAM roles used to perform credential requests by the openshift operators.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<p>Conditions specifies the ROSARoleConfig conditions</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RefreshPreferences">RefreshPreferences
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>)
</p>
<p>
<p>RefreshPreferences defines the specs for instance refreshing.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>disable</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Disable, if true, disables instance refresh from triggering when new launch templates are detected.
This is useful in scenarios where ASG nodes are externally managed.</p>
</td>
</tr>
<tr>
<td>
<code>strategy</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The strategy to use for the instance refresh. The only valid value is Rolling.
A rolling update is an update that is applied to all instances in an Auto
Scaling group until all instances have been updated.</p>
</td>
</tr>
<tr>
<td>
<code>instanceWarmup</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The number of seconds until a newly launched instance is configured and ready
to use. During this time, the next replacement will not be initiated.
The default is to use the value for the health check grace period defined for the group.</p>
</td>
</tr>
<tr>
<td>
<code>minHealthyPercentage</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of capacity as a percentage in ASG that must remain healthy
during an instance refresh. The default is 90.</p>
</td>
</tr>
<tr>
<td>
<code>maxHealthyPercentage</code><br/>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>The amount of capacity as a percentage in ASG that can be in service and healthy, or pending,
to support your workload when replacing instances.
The value is expressed as a percentage of the desired capacity of the ASG. Value range is 100 to 200.
If you specify MaxHealthyPercentage , you must also specify MinHealthyPercentage , and the difference between
them cannot be greater than 100.
A larger range increases the number of instances that can be replaced at the same time.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RollingUpdate">RollingUpdate
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaUpdateConfig">RosaUpdateConfig</a>)
</p>
<p>
<p>RollingUpdate specifies MaxUnavailable &amp; MaxSurge number of nodes during update.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code><br/>
<em>
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailable is the maximum number of nodes that can be unavailable during the update.
Value can be an absolute number (ex: 5) or a percentage of desired nodes (ex: 10%).
Absolute number is calculated from percentage by rounding down.</p>
<p>MaxUnavailable can not be 0 if MaxSurge is 0, default is 0.
Both MaxUnavailable &amp; MaxSurge must use the same units (absolute value or percentage).</p>
<p>Example: when MaxUnavailable is set to 30%, old nodes can be deleted down to 70% of
desired nodes immediately when the rolling update starts. Once new nodes
are ready, more old nodes be deleted, followed by provisioning new nodes,
ensuring that the total number of nodes available at all times during the
update is at least 70% of desired nodes.</p>
</td>
</tr>
<tr>
<td>
<code>maxSurge</code><br/>
<em>
k8s.io/apimachinery/pkg/util/intstr.IntOrString
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxSurge is the maximum number of nodes that can be provisioned above the desired number of nodes.
Value can be an absolute number (ex: 5) or a percentage of desired nodes (ex: 10%).
Absolute number is calculated from percentage by rounding up.</p>
<p>MaxSurge can not be 0 if MaxUnavailable is 0, default is 1.
Both MaxSurge &amp; MaxUnavailable must use the same units (absolute value or percentage).</p>
<p>Example: when MaxSurge is set to 30%, new nodes can be provisioned immediately
when the rolling update starts, such that the total number of old and new
nodes do not exceed 130% of desired nodes. Once old nodes have been
deleted, new nodes can be provisioned, ensuring that total number of nodes
running at any time during the update is at most 130% of desired nodes.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">RosaMachinePoolSpec
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSAMachinePool">ROSAMachinePool</a>)
</p>
<p>
<p>RosaMachinePoolSpec defines the desired state of RosaMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>nodePoolName</code><br/>
<em>
string
</em>
</td>
<td>
<p>NodePoolName specifies the name of the nodepool in Rosa
must be a valid DNS-1035 label, so it must consist of lower case alphanumeric and have a max length of 15 characters.</p>
</td>
</tr>
<tr>
<td>
<code>version</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Version specifies the OpenShift version of the nodes associated with this machinepool.
ROSAControlPlane version is used if not set.</p>
</td>
</tr>
<tr>
<td>
<code>availabilityZone</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AvailabilityZone is an optinal field specifying the availability zone where instances of this machine pool should run
For Multi-AZ clusters, you can create a machine pool in a Single-AZ of your choice.</p>
</td>
</tr>
<tr>
<td>
<code>subnet</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
</td>
</tr>
<tr>
<td>
<code>labels</code><br/>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Labels specifies labels for the Kubernetes node objects</p>
</td>
</tr>
<tr>
<td>
<code>taints</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaTaint">
[]RosaTaint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Taints specifies the taints to apply to the nodes of the machine pool</p>
</td>
</tr>
<tr>
<td>
<code>additionalTags</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Tags">
Tags
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalTags are user-defined tags to be added on the underlying EC2 instances associated with this machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>autoRepair</code><br/>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AutoRepair specifies whether health checks should be enabled for machines
in the NodePool. The default is true.</p>
</td>
</tr>
<tr>
<td>
<code>instanceType</code><br/>
<em>
string
</em>
</td>
<td>
<p>InstanceType specifies the AWS instance type</p>
</td>
</tr>
<tr>
<td>
<code>autoscaling</code><br/>
<em>
<a href="crd/index.html#controlplane.cluster.x-k8s.io/v1beta2.AutoScaling">
AutoScaling
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Autoscaling specifies auto scaling behaviour for this MachinePool.
required if Replicas is not configured</p>
</td>
</tr>
<tr>
<td>
<code>tuningConfigs</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>TuningConfigs specifies the names of the tuning configs to be applied to this MachinePool.
Tuning configs must already exist.</p>
</td>
</tr>
<tr>
<td>
<code>additionalSecurityGroups</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>AdditionalSecurityGroups is an optional set of security groups to associate
with all node instances of the machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>volumeSize</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>VolumeSize set the disk volume size for the machine pool, in Gib. The default is 300 GiB.</p>
</td>
</tr>
<tr>
<td>
<code>providerIDList</code><br/>
<em>
[]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ProviderIDList contain a ProviderID for each machine instance that&rsquo;s currently managed by this machine pool.</p>
</td>
</tr>
<tr>
<td>
<code>nodeDrainGracePeriod</code><br/>
<em>
<a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#Duration">
Kubernetes meta/v1.Duration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeDrainGracePeriod is grace period for how long Pod Disruption Budget-protected workloads will be
respected during upgrades. After this grace period, any workloads protected by Pod Disruption
Budgets that have not been successfully drained from a node will be forcibly evicted.</p>
<p>Valid values are from 0 to 1 week(10080m|168h) .
0 or empty value means that the MachinePool can be drained without any time limitation.</p>
</td>
</tr>
<tr>
<td>
<code>updateConfig</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaUpdateConfig">
RosaUpdateConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>UpdateConfig specifies update configurations.</p>
</td>
</tr>
<tr>
<td>
<code>capacityReservationID</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>CapacityReservationID specifies the ID of an AWS On-Demand Capacity Reservation and Capacity Blocks for ML.
The CapacityReservationID must be pre-created in advance, before creating a NodePool.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolStatus">RosaMachinePoolStatus
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSAMachinePool">ROSAMachinePool</a>)
</p>
<p>
<p>RosaMachinePoolStatus defines the observed state of RosaMachinePool.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>ready</code><br/>
<em>
bool
</em>
</td>
<td>
<p>Ready denotes that the RosaMachinePool nodepool has joined
the cluster</p>
</td>
</tr>
<tr>
<td>
<code>replicas</code><br/>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>Replicas is the most recently observed number of replicas.</p>
</td>
</tr>
<tr>
<td>
<code>conditions</code><br/>
<em>
Cluster API api/core/v1beta1.Conditions
</em>
</td>
<td>
<em>(Optional)</em>
<p>Conditions defines current service state of the managed machine pool</p>
</td>
</tr>
<tr>
<td>
<code>failureMessage</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>FailureMessage will be set in the event that there is a terminal problem
reconciling the state and will be set to a descriptive error message.</p>
<p>This field should not be set for transitive errors that a controller
faces that are expected to be fixed automatically over
time (like service outages), but instead indicate that something is
fundamentally wrong with the spec or the configuration of
the controller, and that manual intervention is required.</p>
</td>
</tr>
<tr>
<td>
<code>id</code><br/>
<em>
string
</em>
</td>
<td>
<p>ID is the ID given by ROSA.</p>
</td>
</tr>
<tr>
<td>
<code>availableUpgrades</code><br/>
<em>
[]string
</em>
</td>
<td>
<p>Available upgrades for the ROSA MachinePool.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RosaTaint">RosaTaint
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">RosaMachinePoolSpec</a>)
</p>
<p>
<p>RosaTaint represents a taint to be applied to a node.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>key</code><br/>
<em>
string
</em>
</td>
<td>
<p>The taint key to be applied to a node.</p>
</td>
</tr>
<tr>
<td>
<code>value</code><br/>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>The taint value corresponding to the taint key.</p>
</td>
</tr>
<tr>
<td>
<code>effect</code><br/>
<em>
<a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#tainteffect-v1-core">
Kubernetes core/v1.TaintEffect
</a>
</em>
</td>
<td>
<p>The effect of the taint on pods that do not tolerate the taint.
Valid effects are NoSchedule, PreferNoSchedule and NoExecute.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.RosaUpdateConfig">RosaUpdateConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RosaMachinePoolSpec">RosaMachinePoolSpec</a>)
</p>
<p>
<p>RosaUpdateConfig specifies update configuration</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>rollingUpdate</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.RollingUpdate">
RollingUpdate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>RollingUpdate specifies MaxUnavailable &amp; MaxSurge number of nodes during update.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SharedVPCConfig">SharedVPCConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AccountRoleConfig">AccountRoleConfig</a>, <a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.OperatorRoleConfig">OperatorRoleConfig</a>)
</p>
<p>
<p>SharedVPCConfig is used to set up shared VPC.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>routeRoleARN</code><br/>
<em>
string
</em>
</td>
<td>
<p>Role ARN associated with the private hosted zone used for Hosted Control Plane cluster shared VPC, this role contains policies to be used with Route 53</p>
</td>
</tr>
<tr>
<td>
<code>vpcEndpointRoleArn</code><br/>
<em>
string
</em>
</td>
<td>
<p>Role ARN associated with the shared VPC used for Hosted Control Plane clusters, this role contains policies to be used with the VPC endpoint</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SpotAllocationStrategy">SpotAllocationStrategy
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.InstancesDistribution">InstancesDistribution</a>)
</p>
<p>
<p>SpotAllocationStrategy indicates how to allocate instances across Spot Instance pools.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.SuspendProcessesTypes">SuspendProcessesTypes
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSMachinePoolSpec">AWSMachinePoolSpec</a>)
</p>
<p>
<p>SuspendProcessesTypes contains user friendly auto-completable values for suspended process names.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>all</code><br/>
<em>
bool
</em>
</td>
<td>
</td>
</tr>
<tr>
<td>
<code>processes</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Processes">
Processes
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Tags">Tags
(<code>map[string]string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.ROSANetworkSpec">ROSANetworkSpec</a>)
</p>
<p>
<p>Tags is a mapping for tags.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Taint">Taint
</h3>
<p>
<p>Taint defines the specs for a Kubernetes taint.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>effect</code><br/>
<em>
<a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.TaintEffect">
TaintEffect
</a>
</em>
</td>
<td>
<p>Effect specifies the effect for the taint</p>
</td>
</tr>
<tr>
<td>
<code>key</code><br/>
<em>
string
</em>
</td>
<td>
<p>Key is the key of the taint</p>
</td>
</tr>
<tr>
<td>
<code>value</code><br/>
<em>
string
</em>
</td>
<td>
<p>Value is the value of the taint</p>
</td>
</tr>
</tbody>
</table>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.TaintEffect">TaintEffect
(<code>string</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.Taint">Taint</a>)
</p>
<p>
<p>TaintEffect is the effect for a Kubernetes taint.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.Taints">Taints
(<code>[]sigs.k8s.io/cluster-api-provider-aws/v2/exp/api/v1beta2.Taint</code> alias)</p></h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>Taints is an array of Taints.</p>
</p>
<h3 id="infrastructure.cluster.x-k8s.io/v1beta2.UpdateConfig">UpdateConfig
</h3>
<p>
(<em>Appears on:</em><a href="crd/index.html#infrastructure.cluster.x-k8s.io/v1beta2.AWSManagedMachinePoolSpec">AWSManagedMachinePoolSpec</a>)
</p>
<p>
<p>UpdateConfig is the configuration options for updating a nodegroup. Only one of MaxUnavailable
and MaxUnavailablePercentage should be specified.</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>maxUnavailable</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailable is the maximum number of nodes unavailable at once during a version update.
Nodes will be updated in parallel. The maximum number is 100.</p>
</td>
</tr>
<tr>
<td>
<code>maxUnavailablePercentage</code><br/>
<em>
int
</em>
</td>
<td>
<em>(Optional)</em>
<p>MaxUnavailablePercentage is the maximum percentage of nodes unavailable during a version update. This
percentage of nodes will be updated in parallel, up to 100 nodes at once.</p>
</td>
</tr>
</tbody>
</table>
<hr/>
<h1><a class="header" href="#reference" id="reference">Reference</a></h1>
<h2><a class="header" href="#table-of-feature-gates-and-their-corresponding-environment-variables" id="table-of-feature-gates-and-their-corresponding-environment-variables">Table of feature gates and their corresponding environment variables</a></h2>
<table><thead><tr><th>Feature Gate</th><th>Environment Variable</th><th>Default</th></tr></thead><tbody>
<tr><td>EKS</td><td>CAPA_EKS</td><td>true</td></tr>
<tr><td>EKSEnableIAM</td><td>CAPA_EKS_IAM</td><td>false</td></tr>
<tr><td>EKSAllowAddRoles</td><td>CAPA_EKS_ADD_ROLES</td><td>false</td></tr>
<tr><td>EKSFargate</td><td>EXP_EKS_FARGATE</td><td>false</td></tr>
<tr><td>MachinePool</td><td>EXP_MACHINE_POOL</td><td>false</td></tr>
<tr><td>MachinePoolMachines</td><td>EXP_MACHINE_POOL_MACHINES</td><td>false</td></tr>
<tr><td>EventBridgeInstanceState</td><td>EVENT_BRIDGE_INSTANCE_STATE</td><td>false</td></tr>
<tr><td>AutoControllerIdentityCreator</td><td>AUTO_CONTROLLER_IDENTITY_CREATOR</td><td>true</td></tr>
<tr><td>BootstrapFormatIgnition</td><td>EXP_BOOTSTRAP_FORMAT_IGNITION</td><td>false</td></tr>
<tr><td>ExternalResourceGC</td><td>EXP_EXTERNAL_RESOURCE_GC</td><td>false</td></tr>
<tr><td>AlternativeGCStrategy</td><td>EXP_ALTERNATIVE_GC_STRATEGY</td><td>false</td></tr>
<tr><td>TagUnmanagedNetworkResources</td><td>TAG_UNMANAGED_NETWORK_RESOURCES</td><td>true</td></tr>
<tr><td>ROSA</td><td>EXP_ROSA</td><td>false</td></tr>
</tbody></table>
<h1><a class="header" href="#glossary" id="glossary">Glossary</a></h1>
<h1><a class="header" href="#table-of-contents" id="table-of-contents">Table of Contents</a></h1>
<p><a href="topics/reference/glossary.html#a">A</a> | <a href="topics/reference/glossary.html#b">B</a> | <a href="topics/reference/glossary.html#c">C</a> | <a href="topics/reference/glossary.html#d">D</a> | <a href="topics/reference/glossary.html#e">E</a> | <a href="topics/reference/glossary.html#h">H</a> | <a href="topics/reference/glossary.html#i">I</a> | <a href="topics/reference/glossary.html#k">K</a> | <a href="topics/reference/glossary.html#l">L</a>| <a href="topics/reference/glossary.html#m">M</a> | <a href="topics/reference/glossary.html#n">N</a> | <a href="topics/reference/glossary.html#o">O</a> | <a href="topics/reference/glossary.html#p">P</a> | <a href="topics/reference/glossary.html#r">R</a> | <a href="topics/reference/glossary.html#s">S</a> | <a href="topics/reference/glossary.html#t">T</a> | <a href="topics/reference/glossary.html#u">U</a> |<a href="topics/reference/glossary.html#w">W</a></p>
<h1><a class="header" href="#a" id="a">A</a></h1>
<hr />
<h3><a class="header" href="#add-ons" id="add-ons">Add-ons</a></h3>
<p>Services beyond the fundamental components of Kubernetes.</p>
<ul>
<li><strong>Core Add-ons</strong>: Addons that are required to deploy a Kubernetes-conformant cluster: DNS, kube-proxy, CNI.</li>
<li><strong>Additional Add-ons</strong>: Addons that are not required for a Kubernetes-conformant cluster (e.g. metrics/Heapster, Dashboard).</li>
</ul>
<h1><a class="header" href="#b" id="b">B</a></h1>
<hr />
<h3><a class="header" href="#bootstrap" id="bootstrap">Bootstrap</a></h3>
<p>The process of turning a server into a Kubernetes node. This may involve assembling data to provide when creating the server that backs the Machine, as well as runtime configuration of the software running on that server.</p>
<h3><a class="header" href="#bootstrap-cluster" id="bootstrap-cluster">Bootstrap cluster</a></h3>
<p>A temporary cluster that is used to provision a Target Management cluster.</p>
<h3><a class="header" href="#bootstrap-provider" id="bootstrap-provider">Bootstrap provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that implements a solution for the <a href="topics/reference/glossary.html#bootstrap">bootstrap</a> process.
Bootstrap provider’s interaction with Cluster API is based on what is defined in the <a href="topics/reference/glossary.html#contract">Cluster API contract</a>.</p>
<p>See <a href="topics/reference/glossary.html#cabpk">CABPK</a>.</p>
<h1><a class="header" href="#c" id="c">C</a></h1>
<hr />
<h3><a class="header" href="#caep" id="caep">CAEP</a></h3>
<p>Cluster API Enhancement Proposal - patterned after <a href="https://git.k8s.io/enhancements/keps/README.md">KEP</a>. See <a href="https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/YYYYMMDD-template.md">template</a></p>
<h3><a class="header" href="#capi" id="capi">CAPI</a></h3>
<p><a href="topics/reference/glossary.html#core-cluster-api">Core Cluster API</a></p>
<h3><a class="header" href="#capa" id="capa">CAPA</a></h3>
<p>Cluster API Provider AWS</p>
<h3><a class="header" href="#cabpk" id="cabpk">CABPK</a></h3>
<p>Cluster API Bootstrap Provider Kubeadm</p>
<h3><a class="header" href="#capc" id="capc">CAPC</a></h3>
<p>Cluster API Provider CloudStack</p>
<h3><a class="header" href="#capd" id="capd">CAPD</a></h3>
<p>Cluster API Provider Docker</p>
<h3><a class="header" href="#capdo" id="capdo">CAPDO</a></h3>
<p>Cluster API Provider DigitalOcean</p>
<h3><a class="header" href="#capg" id="capg">CAPG</a></h3>
<p>Cluster API Google Cloud Provider</p>
<h3><a class="header" href="#caph" id="caph">CAPH</a></h3>
<p>Cluster API Provider Hetzner</p>
<h3><a class="header" href="#caphv" id="caphv">CAPHV</a></h3>
<p>Cluster API Provider Hivelocity</p>
<h3><a class="header" href="#caphw" id="caphw">CAPHW</a></h3>
<p>Cluster API Provider Huawei</p>
<h3><a class="header" href="#capibm" id="capibm">CAPIBM</a></h3>
<p>Cluster API Provider IBM Cloud</p>
<h3><a class="header" href="#capio" id="capio">CAPIO</a></h3>
<p>Cluster API Operator</p>
<h3><a class="header" href="#capl" id="capl">CAPL</a></h3>
<p>Cluster API Provider Akamai (Linode)</p>
<h3><a class="header" href="#capm3" id="capm3">CAPM3</a></h3>
<p>Cluster API Provider Metal3</p>
<h3><a class="header" href="#capms" id="capms">CAPMS</a></h3>
<p>Cluster API Provider metal-stack</p>
<h3><a class="header" href="#capn" id="capn">CAPN</a></h3>
<p>Cluster API Provider Nested</p>
<h3><a class="header" href="#capx" id="capx">CAPX</a></h3>
<p>Cluster API Provider Nutanix</p>
<h3><a class="header" href="#capkk" id="capkk">CAPKK</a></h3>
<p>Cluster API Provider KubeKey</p>
<h3><a class="header" href="#capk" id="capk">CAPK</a></h3>
<p>Cluster API Provider Kubevirt</p>
<h3><a class="header" href="#capo" id="capo">CAPO</a></h3>
<p>Cluster API Provider OpenStack</p>
<h3><a class="header" href="#caposc" id="caposc">CAPOSC</a></h3>
<p>Cluster API Provider Outscale</p>
<h3><a class="header" href="#capoci" id="capoci">CAPOCI</a></h3>
<p>Cluster API Provider Oracle Cloud Infrastructure (OCI)</p>
<h3><a class="header" href="#caps" id="caps">CAPS</a></h3>
<p>Cluster API Provider Scaleway</p>
<h3><a class="header" href="#capt" id="capt">CAPT</a></h3>
<p>Cluster API Provider Tinkerbell</p>
<h3><a class="header" href="#capv" id="capv">CAPV</a></h3>
<p>Cluster API Provider vSphere</p>
<h3><a class="header" href="#capvc" id="capvc">CAPVC</a></h3>
<p>Cluster API Provider vcluster</p>
<h3><a class="header" href="#capvcd" id="capvcd">CAPVCD</a></h3>
<p>Cluster API Provider VMware Cloud Director</p>
<h3><a class="header" href="#capz" id="capz">CAPZ</a></h3>
<p>Cluster API Provider Azure</p>
<h3><a class="header" href="#caipamic" id="caipamic">CAIPAMIC</a></h3>
<p>Cluster API IPAM Provider In Cluster</p>
<h3><a class="header" href="#caipamx" id="caipamx">CAIPAMX</a></h3>
<p>Cluster API IPAM Provider Nutanix</p>
<h3><a class="header" href="#caipam3" id="caipam3">CAIPAM3</a></h3>
<p>Cluster API IPAM Provider Metal3</p>
<h3><a class="header" href="#carex" id="carex">CAREX</a></h3>
<p>Cluster API Runtime Extensions Provider Nutanix</p>
<h3><a class="header" href="#chained-upgrade" id="chained-upgrade">Chained upgrade</a></h3>
<p>An upgrade sequence that goes from one Kubernetes version to another by passing through a set of intermediate versions.
E.g. upgrading from v1.31.0 (current state) to v1.34.0 (target version) requires
a chained upgrade with the following steps: v1.32.0 (first intermediate version) -&gt; v1.33.0 (second intermediate version) -&gt; v1.34.0 (target version).</p>
<p>The sequence of versions in a chained upgrade is also called <a href="topics/reference/glossary.html#upgrade-plan">upgrade plan</a>.</p>
<p>See also <a href="topics/reference/glossary.html#efficient-upgrade">efficient upgrade</a>.</p>
<h3><a class="header" href="#cloud-provider" id="cloud-provider">Cloud provider</a></h3>
<p>Or <strong>Cloud service provider</strong></p>
<p>Refers to an information technology (IT) company that provides computing resources (e.g. AWS, Azure, Google, etc.).</p>
<h3><a class="header" href="#cluster" id="cluster">Cluster</a></h3>
<p>A full Kubernetes deployment. See Management Cluster and Workload Cluster.</p>
<h3><a class="header" href="#clusterclass" id="clusterclass">ClusterClass</a></h3>
<p>A collection of templates that define a topology (control plane and workers) to be used to continuously reconcile one or more Clusters.
See <a href="topics/reference/../tasks/experimental-features/cluster-class/index.html">ClusterClass</a></p>
<h3><a class="header" href="#cluster-api" id="cluster-api">Cluster API</a></h3>
<p>Or <strong>Cluster API project</strong></p>
<p>The Cluster API sub-project of the SIG-cluster-lifecycle. It is also used to refer to the software components, APIs, and community that produce them.</p>
<p>See <a href="topics/reference/glossary.html#core-cluster-api">Core Cluster API</a>, <a href="topics/reference/glossary.html#capi">CAPI</a></p>
<h3><a class="header" href="#cluster-api-runtime" id="cluster-api-runtime">Cluster API Runtime</a></h3>
<p>The Cluster API execution model, a set of controllers cooperating in managing the Kubernetes cluster lifecycle.</p>
<h3><a class="header" href="#cluster-infrastructure" id="cluster-infrastructure">Cluster Infrastructure</a></h3>
<p>or <strong>Kubernetes Cluster Infrastructure</strong></p>
<p>Defines the <strong>infrastructure that supports a Kubernetes cluster</strong>, like e.g. VPC, security groups, load balancers, etc. Please note that in the context of managed Kubernetes some of those components are going to be provided by the corresponding abstraction for a specific Cloud provider (EKS, OKE, AKS etc), and thus Cluster API should not take care of managing a subset or all those components.</p>
<h3><a class="header" href="#contract" id="contract">Contract</a></h3>
<p>Or <strong>Cluster API contract</strong></p>
<p>Defines a set of rules a <a href="topics/reference/glossary.html#provider">provider</a> is expected to comply with in order to interact with Cluster API.
Those rules can be in the form of CustomResourceDefinition (CRD) fields and/or expected behaviors to be implemented.</p>
<h3><a class="header" href="#control-plane" id="control-plane">Control plane</a></h3>
<p>The set of Kubernetes services that form the basis of a cluster. See also <a href="https://kubernetes.io/docs/concepts/#kubernetes-control-plane">https://kubernetes.io/docs/concepts/#kubernetes-control-plane</a> There are two variants:</p>
<ul>
<li><strong>Self-provisioned</strong>: A Kubernetes control plane consisting of pods or machines wholly managed by a single Cluster API deployment.</li>
<li><strong>External</strong> or <strong>Managed</strong>: A control plane offered and controlled by some system other than Cluster API (e.g., GKE, AKS, EKS, IKS).</li>
</ul>
<h3><a class="header" href="#control-plane-provider" id="control-plane-provider">Control plane provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that implements a solution for the management of a Kubernetes <a href="topics/reference/glossary.html#control-plane">control plane</a>.
Control plane provider’s interaction with Cluster API is based on what is defined in the <a href="topics/reference/glossary.html#contract">Cluster API contract</a>.</p>
<p>See <a href="topics/reference/glossary.html#kcp">KCP</a>.</p>
<h3><a class="header" href="#core-cluster-api" id="core-cluster-api">Core Cluster API</a></h3>
<p>With “core” Cluster API we refer to the common set of API and controllers that are required to run
any Cluster API provider.</p>
<p>Please note that in the Cluster API code base, side by side of “core” Cluster API components there
is also a limited number of in-tree providers: <a href="topics/reference/glossary.html#cabpk">CABPK</a>, <a href="topics/reference/glossary.html#kcp">KCP</a>, <a href="topics/reference/glossary.html#capd">CAPD</a>.</p>
<p>See <a href="topics/reference/glossary.html#cluster-api">Cluster API</a>, <a href="topics/reference/glossary.html#capi">CAPI</a>.</p>
<h3><a class="header" href="#core-provider" id="core-provider">Core provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that implements Cluster API <a href="topics/reference/glossary.html#core-controllers">core controllers</a></p>
<p>See <a href="topics/reference/glossary.html#cluster-api">Cluster API</a>, <a href="topics/reference/glossary.html#capi">CAPI</a>.</p>
<h3><a class="header" href="#core-controllers" id="core-controllers">Core controllers</a></h3>
<p>The set of controllers in <a href="topics/reference/glossary.html#core-cluster-api">Core Cluster API</a>.</p>
<p>See <a href="topics/reference/glossary.html#cluster-api">Cluster API</a>, <a href="topics/reference/glossary.html#capi">CAPI</a>.</p>
<h1><a class="header" href="#d" id="d">D</a></h1>
<hr />
<h3><a class="header" href="#default-implementation" id="default-implementation">Default implementation</a></h3>
<p>A feature implementation offered as part of the Cluster API project and maintained by the CAPI core team; For example
<a href="topics/reference/glossary.html#kcp">KCP</a> is a default implementation for a <a href="topics/reference/glossary.html#control-plane-provider">control plane provider</a>.</p>
<h1><a class="header" href="#e" id="e">E</a></h1>
<hr />
<h3><a class="header" href="#efficient-upgrade" id="efficient-upgrade">Efficient upgrade</a></h3>
<p>A <a href="topics/reference/glossary.html#chained-upgrade">chained upgrade</a> where worker nodes skip some of the intermediate versions,
when allowed by the <a href="https://kubernetes.io/releases/version-skew-policy/">Kubernetes version skew policy</a>.</p>
<p>When the chained upgrade is also an efficient upgrade, the <a href="topics/reference/glossary.html#upgrade-plan">upgrade plan</a> for worker machines is a subset 
of the <a href="topics/reference/glossary.html#upgrade-plan">upgrade plan</a> for control plane machines.</p>
<h3><a class="header" href="#external-patch" id="external-patch">External patch</a></h3>
<p><a href="topics/reference/glossary.html#patch">Patch</a> generated by an external component using <a href="topics/reference/glossary.html#runtime-sdk">Runtime SDK</a>. Alternative to <a href="topics/reference/glossary.html#inline-patch">inline patch</a>.</p>
<h3><a class="header" href="#external-patch-extension" id="external-patch-extension">External patch extension</a></h3>
<p>A <a href="topics/reference/glossary.html#runtime-extension">runtime extension</a> that implements a <a href="topics/reference/glossary.html#topology-mutation-hook">topology mutation hook</a>.</p>
<h1><a class="header" href="#h" id="h">H</a></h1>
<hr />
<h3><a class="header" href="#horizontal-scaling" id="horizontal-scaling">Horizontal Scaling</a></h3>
<p>The ability to add more machines based on policy and well-defined metrics. For example, add a machine to a cluster when CPU load average &gt; (X) for a period of time (Y).</p>
<h3><a class="header" href="#host" id="host">Host</a></h3>
<p>see <a href="topics/reference/glossary.html#server">Server</a></p>
<h1><a class="header" href="#i" id="i">I</a></h1>
<hr />
<h3><a class="header" href="#infrastructure-provider" id="infrastructure-provider">Infrastructure provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that implements provisioning of infrastructure/computational resources required by
the Cluster or by Machines (e.g. VMs, networking, etc.).
Infrastructure provider’s interaction with Cluster API is based on what is defined in the <a href="topics/reference/glossary.html#contract">Cluster API contract</a>.</p>
<p>Clouds infrastructure providers include AWS, Azure, or Google; while VMware, MAAS, or metal3.io can be defined as bare metal providers.
When there is more than one way to obtain resources from the same infrastructure provider (e.g. EC2 vs. EKS in AWS) each way is referred to as a variant.</p>
<p>For a complete list of providers see <a href="topics/reference/providers.html">Provider Implementations</a>.</p>
<h3><a class="header" href="#inline-patch" id="inline-patch">Inline patch</a></h3>
<p>A <a href="topics/reference/glossary.html#patch">patch</a> defined inline in a <a href="topics/reference/glossary.html#clusterclass">ClusterClass</a>. An alternative to an <a href="topics/reference/glossary.html#external-patch">external patch</a>.</p>
<h3><a class="header" href="#in-place-mutable-fields" id="in-place-mutable-fields">In-place mutable fields</a></h3>
<p>Fields which changes would only impact Kubernetes objects or/and controller behaviour
but they won’t mutate in any way provider infrastructure nor the software running on it. In-place mutable fields
are propagated in place by CAPI controllers to avoid the more elaborated mechanics of a replace rollout.
They include metadata, MinReadySeconds, NodeDrainTimeout, NodeVolumeDetachTimeout and NodeDeletionTimeout but are
not limited to be expanded in the future.</p>
<h3><a class="header" href="#in-place-update" id="in-place-update">In-place update</a></h3>
<p>Any change to a Machine spec that is performed without deleting the Machine and creating a new one.</p>
<p>Note: changing <a href="topics/reference/glossary.html#in-place-mutable-fields">in-place mutable fields</a> is not considered an in-place update.</p>
<h3><a class="header" href="#instance" id="instance">Instance</a></h3>
<p>see <a href="topics/reference/glossary.html#server">Server</a></p>
<h3><a class="header" href="#immutability" id="immutability">Immutability</a></h3>
<p>A resource that does not mutate.  In Kubernetes we often state the instance of a running pod is immutable or does not change once it is run.  In order to make a change, a new pod is run.  In the context of <a href="topics/reference/glossary.html#cluster-api">Cluster API</a> we often refer to a running instance of a <a href="topics/reference/glossary.html#machine">Machine</a> as being immutable, from a <a href="topics/reference/glossary.html#cluster-api">Cluster API</a> perspective.</p>
<p>Note: Cluster API also has extensibility points that make it possible to perform <a href="topics/reference/glossary.html#in-place-update">in-place updates</a> of machines.</p>
<h3><a class="header" href="#ipam-provider" id="ipam-provider">IPAM provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that allows Cluster API to interact with IPAM solutions.
IPAM provider’s interaction with Cluster API is based on the <code>IPAddressClaim</code> and <code>IPAddress</code> API types.</p>
<h1><a class="header" href="#k" id="k">K</a></h1>
<hr />
<h3><a class="header" href="#kubernetes-conformant" id="kubernetes-conformant">Kubernetes-conformant</a></h3>
<p>Or <strong>Kubernetes-compliant</strong></p>
<p>A cluster that passes the Kubernetes conformance tests.</p>
<h3><a class="header" href="#kk" id="kk">k/k</a></h3>
<p>Refers to the <a href="https://github.com/kubernetes/kubernetes">main Kubernetes git repository</a> or the main Kubernetes project.</p>
<h3><a class="header" href="#kcp" id="kcp">KCP</a></h3>
<p>Kubeadm Control plane Provider</p>
<h1><a class="header" href="#l" id="l">L</a></h1>
<hr />
<h3><a class="header" href="#lifecycle-hook" id="lifecycle-hook">Lifecycle hook</a></h3>
<p>A <a href="topics/reference/glossary.html#runtime-hook">Runtime Hook</a> that allows external components to interact with the lifecycle of a Cluster.</p>
<p>See <a href="topics/reference/../tasks/experimental-features/runtime-sdk/implement-lifecycle-hooks.html">Implementing Lifecycle Hooks</a></p>
<h1><a class="header" href="#m" id="m">M</a></h1>
<hr />
<h3><a class="header" href="#machine" id="machine">Machine</a></h3>
<p>Or <strong>Machine Resource</strong></p>
<p>The Custom Resource for Kubernetes that represents a request to have a place to run kubelet.</p>
<p>See also: <a href="topics/reference/glossary.html#server">Server</a></p>
<h3><a class="header" href="#manage-a-cluster" id="manage-a-cluster">Manage a cluster</a></h3>
<p>Perform create, scale, upgrade, or destroy operations on the cluster.</p>
<h3><a class="header" href="#managed-kubernetes" id="managed-kubernetes">Managed Kubernetes</a></h3>
<p>Managed Kubernetes refers to any Kubernetes cluster provisioning and maintenance abstraction, usually exposed as an API, that is natively available in a Cloud provider. For example: <a href="https://aws.amazon.com/eks/">EKS</a>, <a href="https://www.oracle.com/cloud/cloud-native/container-engine-kubernetes/">OKE</a>, <a href="https://azure.microsoft.com/en-us/products/kubernetes-service">AKS</a>, <a href="https://cloud.google.com/kubernetes-engine">GKE</a>, <a href="https://www.ibm.com/cloud/kubernetes-service">IBM Cloud Kubernetes Service</a>, <a href="https://www.digitalocean.com/products/kubernetes">DOKS</a>, and many more throughout the Kubernetes Cloud Native ecosystem.</p>
<h3><a class="header" href="#managed-topology" id="managed-topology">Managed Topology</a></h3>
<p>See <a href="topics/reference/glossary.html#topology">Topology</a></p>
<h3><a class="header" href="#management-cluster" id="management-cluster">Management cluster</a></h3>
<p>The cluster where one or more Infrastructure Providers run, and where resources (e.g. Machines) are stored. Typically referred to when you are provisioning multiple workload clusters.</p>
<h3><a class="header" href="#multi-tenancy-1" id="multi-tenancy-1">Multi-tenancy</a></h3>
<p>Multi tenancy in Cluster API defines the capability of an infrastructure provider to manage different credentials, each
one of them corresponding to an infrastructure tenant.</p>
<p>Please note that up until v1alpha3 this concept had a different meaning, referring to the capability to run multiple
instances of the same provider, each one with its own credentials; starting from v1alpha4 we are disambiguating the two concepts.</p>
<p>See also <a href="topics/reference/../developer/core/support-multiple-instances.html">Support multiple instances</a>.</p>
<h1><a class="header" href="#n" id="n">N</a></h1>
<hr />
<h3><a class="header" href="#node-pools" id="node-pools">Node pools</a></h3>
<p>A node pool is a group of nodes within a cluster that all have the same configuration.</p>
<h1><a class="header" href="#o" id="o">O</a></h1>
<hr />
<h3><a class="header" href="#operating-system" id="operating-system">Operating system</a></h3>
<p>Or <strong>OS</strong></p>
<p>A generically understood combination of a kernel and system-level userspace interface, such as Linux or Windows, as opposed to a particular distribution.</p>
<h1><a class="header" href="#p" id="p">P</a></h1>
<hr />
<h3><a class="header" href="#patch" id="patch">Patch</a></h3>
<p>A set of instructions describing modifications to a Kubernetes object. Examples include JSON Patch and JSON Merge Patch.</p>
<h3><a class="header" href="#pivot" id="pivot">Pivot</a></h3>
<p>Pivot is a process for moving the provider components and declared cluster-api resources from a Source Management cluster to a Target Management cluster.</p>
<p>The pivot process is also used for deleting a management cluster and could also be used during an upgrade of the management cluster.</p>
<h3><a class="header" href="#provider" id="provider">Provider</a></h3>
<p>Or <strong>Cluster API provider</strong></p>
<p>This term was originally used as abbreviation for <a href="topics/reference/glossary.html#infrastructure-provider">Infrastructure provider</a>, but currently it is used
to refer to any project that can be deployed and provides functionality to the Cluster API management Cluster.</p>
<p>See <a href="topics/reference/glossary.html#bootstrap-provider">Bootstrap provider</a>, <a href="topics/reference/glossary.html#control-plane-provider">Control plane provider</a>, <a href="topics/reference/glossary.html#core-provider">Core provider</a>,
<a href="topics/reference/glossary.html#infrastructure-provider">Infrastructure provider</a>, <a href="topics/reference/glossary.html#ipam-provider">IPAM provider</a>  <a href="topics/reference/glossary.html#runtime-extension-provider">Runtime extension provider</a>.</p>
<h3><a class="header" href="#providerid" id="providerid">ProviderID</a></h3>
<p>ProviderID is the provider-specific identifier used to correlate Cluster API objects with the
underlying cloud instance. It appears in three resource types: InfrastructureMachine, Machine
(Cluster API core), and Node (workload cluster). CAPI copies the ProviderID from the
InfrastructureMachine to the Machine. The Node’s ProviderID is set by the <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager
(CCM)</a> or the kubelet.</p>
<p>The format is a URI-like string: <code>&lt;provider&gt;://&lt;provider-specific-identifier&gt;</code>.</p>
<h3><a class="header" href="#provider-components" id="provider-components">Provider components</a></h3>
<p>Refers to the YAML artifact published as part of the release process for <a href="topics/reference/glossary.html#provider">providers</a>;
it usually includes Custom Resource Definitions (CRDs), Deployments (to run the controller manager), RBAC, etc.</p>
<p>In some cases, the same expression is used to refer to the instances of above components deployed in a management cluster.</p>
<p>See <a href="topics/reference/glossary.html#provider-repository">Provider repository</a></p>
<h3><a class="header" href="#provider-repository" id="provider-repository">Provider repository</a></h3>
<p>Refers to the location where the YAML for <a href="topics/reference/glossary.html#provider-components">provider components</a> are hosted; usually a provider repository hosts
many version of provider components, one for each released version.</p>
<h1><a class="header" href="#r" id="r">R</a></h1>
<hr />
<h3><a class="header" href="#runtime-extension" id="runtime-extension">Runtime Extension</a></h3>
<p>An external component which is part of a system built on top of Cluster API that can handle requests for a specific Runtime Hook.</p>
<p>See <a href="topics/reference/glossary.html#runtime-sdk">Runtime SDK</a></p>
<h3><a class="header" href="#runtime-extension-provider" id="runtime-extension-provider">Runtime Extension provider</a></h3>
<p>Refers to a <a href="topics/reference/glossary.html#provider">provider</a> that implements one or more <a href="topics/reference/glossary.html#runtime-extension">runtime extensions</a>.
Runtime Extension provider’s interaction with Cluster API are based on the Open API spec for <a href="topics/reference/glossary.html#runtime-hook">runtime hooks</a>.</p>
<h3><a class="header" href="#runtime-hook" id="runtime-hook">Runtime Hook</a></h3>
<p>A single, well identified, extension point allowing applications built on top of Cluster API to hook into specific moments of the <a href="topics/reference/glossary.html#cluster-api-runtime">Cluster API Runtime</a>, e.g. <a href="topics/reference/../tasks/experimental-features/runtime-sdk/implement-lifecycle-hooks.html#beforeclusterupgrade">BeforeClusterUpgrade</a>, <a href="topics/reference/glossary.html#topology-mutation-hook">TopologyMutationHook</a>.</p>
<p>See <a href="topics/reference/glossary.html#runtime-sdk">Runtime SDK</a></p>
<h3><a class="header" href="#runtime-sdk" id="runtime-sdk">Runtime SDK</a></h3>
<p>A developer toolkit required to build Runtime Hooks and Runtime Extensions.</p>
<p>See <a href="topics/reference/../tasks/experimental-features/runtime-sdk/index.html">Runtime SDK</a></p>
<h1><a class="header" href="#s" id="s">S</a></h1>
<hr />
<h3><a class="header" href="#scaling" id="scaling">Scaling</a></h3>
<p>Unless otherwise specified, this refers to horizontal scaling.</p>
<h3><a class="header" href="#stacked-control-plane" id="stacked-control-plane">Stacked control plane</a></h3>
<p>A control plane node where etcd is colocated with the Kubernetes API server, and
is running as a static pod.</p>
<h3><a class="header" href="#server" id="server">Server</a></h3>
<p>The infrastructure that backs a <a href="topics/reference/glossary.html#machine">Machine Resource</a>, typically either a cloud instance, virtual machine, or physical host.</p>
<h1><a class="header" href="#t" id="t">T</a></h1>
<hr />
<h3><a class="header" href="#topology" id="topology">Topology</a></h3>
<p>A field in the Cluster object spec that allows defining and managing the shape of the Cluster’s control plane and worker machines from a single point of control. The Cluster’s topology is based on a <a href="topics/reference/glossary.html#clusterclass">ClusterClass</a>.
Sometimes it is also referred as a managed topology.</p>
<p>See <a href="topics/reference/glossary.html#clusterclass">ClusterClass</a></p>
<h3><a class="header" href="#topology-mutation-hook" id="topology-mutation-hook">Topology Mutation Hook</a></h3>
<p>A <a href="topics/reference/glossary.html#runtime-hook">Runtime Hook</a> that allows external components to generate <a href="topics/reference/glossary.html#patch">patches</a> for customizing Kubernetes objects that are part of a <a href="topics/reference/glossary.html#topology">Cluster topology</a>.</p>
<p>See <a href="topics/reference/../tasks/experimental-features/runtime-sdk/implement-topology-mutation-hook.html">Topology Mutation</a></p>
<h1><a class="header" href="#u" id="u">U</a></h1>
<hr />
<h3><a class="header" href="#update-extension" id="update-extension">Update Extension</a></h3>
<p>A <a href="topics/reference/glossary.html#runtime-extension-provider">runtime extension provider</a> that implements <a href="topics/reference/glossary.html#update-hooks">Update Lifecycle Hooks</a>.</p>
<h3><a class="header" href="#update-hooks" id="update-hooks">Update Hooks</a></h3>
<p>Is a set of Cluster API <a href="topics/reference/glossary.html#runtime-hook">Runtime Hooks</a> called when performing the “can update in-place” decision or
when performing an <a href="topics/reference/glossary.html#in-place-update">in-place update</a>.</p>
<h3><a class="header" href="#upgrade-plan" id="upgrade-plan">Upgrade plan</a></h3>
<p>The sequence of intermediate versions ... target version that a Cluster must upgrade to when
performing a <a href="topics/reference/glossary.html#chained-upgrade">chained upgrade</a>.</p>
<p>Notably, the upgrade plan for control plane machines might be a superset of the upgrade plan for
workers machines.</p>
<h1><a class="header" href="#w" id="w">W</a></h1>
<hr />
<h3><a class="header" href="#workload-cluster" id="workload-cluster">Workload Cluster</a></h3>
<p>A cluster created by a ClusterAPI controller, which is <em>not</em> a bootstrap cluster, and is meant to be used by end-users, as opposed to by CAPI tooling.</p>
<h3><a class="header" href="#workerclass" id="workerclass">WorkerClass</a></h3>
<p>A collection of templates that define a set of worker nodes in the cluster. A ClusterClass contains zero or more WorkerClass definitions.</p>
<p>See <a href="topics/reference/glossary.html#clusterclass">ClusterClass</a></p>
<h1><a class="header" href="#ports-used-by-capa" id="ports-used-by-capa">Ports used by CAPA</a></h1>
<table><thead><tr><th>Name</th><th>Port Number</th><th>Description</th></tr></thead><tbody>
<tr><td><code>metrics</code></td><td></td><td>Port that exposes the metrics. This can be customized by setting the <code>--metrics-bind-addr</code> flag when starting the manager. The default is to only listen on <code>localhost:8080</code></td></tr>
<tr><td><code>webhook</code></td><td><code>9443</code></td><td>Webhook server port. To disable this set <code>--webhook-port</code> flag to <code>0</code>.</td></tr>
<tr><td><code>health</code></td><td><code>9440</code></td><td>Port that exposes the health endpoint. This can be customized by setting the <code>--health-addr</code> flag when starting the manager.</td></tr>
<tr><td><code>profiler</code></td><td></td><td>Expose the pprof profiler. By default is not configured. Can set the <code>--profiler-address</code> flag. e.g. <code>--profiler-address 6060</code></td></tr>
</tbody></table>
<h1><a class="header" href="#jobs" id="jobs">Jobs</a></h1>
<p>This document intends to provide an overview over our jobs running via Prow, GitHub actions and Google Cloud Build.</p>
<h2><a class="header" href="#builds-and-tests-running-on-the-main-branch" id="builds-and-tests-running-on-the-main-branch">Builds and Tests running on the main branch</a></h2>
<blockquote>
<p>NOTE: To see which test jobs execute which tests or e2e tests, you can click on the links which lead to the respective test overviews in [test-grid].</p>
</blockquote>
<h3><a class="header" href="#presubmits" id="presubmits">Presubmits</a></h3>
<p>Prow Presubmits:</p>
<ul>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-test&amp;show-stale-tests=">pull-cluster-api-provider-aws-test</a> <code>./scripts/ci-test.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-build&amp;show-stale-tests=">pull-cluster-api-provider-aws-build</a> <code>./scripts/ci-build.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-verify&amp;show-stale-tests=">pull-cluster-api-provider-aws-verify</a> <code>make verify</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-conformance&amp;show-stale-tests=">pull-cluster-api-provider-aws-e2e-conformance</a> <code>./scripts/ci-conformance.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-conformance-main-k8s-main&amp;show-stale-tests=">pull-cluster-api-provider-aws-e2e-conformance-with-ci-artifacts</a> <code>./scripts/ci-conformance.sh</code>
<ul>
<li>E2E_ARGS: <code>-kubetest.use-ci-artifacts</code></li>
</ul>
</li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-quick-e2e-main&amp;show-stale-tests=">pull-cluster-api-provider-aws-e2e-blocking</a> <code>./scripts/ci-e2e.sh</code>
<ul>
<li>GINKGO_FOCUS: <code>[PR-Blocking]</code></li>
</ul>
</li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-e2e-main&amp;show-stale-tests=">pull-cluster-api-provider-aws-e2e</a> <code>./scripts/ci-e2e.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#pr-e2e-eks-main&amp;show-stale-tests=">pull-cluster-api-provider-aws-e2e-eks</a> <code>./scripts/ci-e2e-eks.sh</code></li>
</ul>
<h3><a class="header" href="#postsubmits" id="postsubmits">Postsubmits</a></h3>
<p>Prow Postsubmits:</p>
<ul>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#postsubmit-e2e-main&amp;show-stale-tests=">ci-cluster-api-provider-aws-e2e</a> <code>./scripts/ci-e2e.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#postsubmit-eks-e2e-main&amp;show-stale-tests=">ci-cluster-api-provider-aws-eks-e2e</a> <code>./scripts/ci-e2e-eks.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#postsubmit-conformance-main&amp;show-stale-tests=">ci-cluster-api-provider-aws-e2e-conformance</a> <code>./scripts/ci-conformance.sh</code></li>
</ul>
<ul>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-image-pushes#post-cluster-api-provider-aws-push-images">post-cluster-api-provider-aws-push-images</a> Google Cloud Build: <code>make release-staging</code></li>
</ul>
<h3><a class="header" href="#periodics" id="periodics">Periodics</a></h3>
<p>Prow Periodics:</p>
<ul>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#periodic-e2e-main">periodic-cluster-api-provider-aws-e2e</a> <code>./scripts/ci-e2e.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#periodic-eks-e2e-main">periodic-cluster-api-provider-aws-eks-e2e</a> <code>/scripts/ci-e2e-eks.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#periodic-conformance-main">periodic-cluster-api-provider-aws-e2e-conformance</a> <code>./scripts/ci-conformance.sh</code></li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#periodic-conformance-main-k8s-main">periodic-cluster-api-provider-aws-e2e-conformance-with-k8s-ci-artifacts</a> <code>./scripts/ci-conformance.sh</code>
<ul>
<li>E2E_ARGS: <code>-kubetest.use-ci-artifacts</code></li>
</ul>
</li>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api-provider-aws#periodic-test-coverage">periodic-cluster-api-provider-aws-coverage</a> <code>./scripts/ci-test-coverage.sh</code></li>
</ul>
<ul>
<li><a href="https://testgrid.k8s.io/sig-cluster-lifecycle-image-pushes#cluster-api-provider-aws-push-images-nightly">cluster-api-provider-aws-push-images-nightly</a> Google Cloud Build: <code>make release-staging-nightly</code></li>
</ul>
<h1><a class="header" href="#capa-version-support" id="capa-version-support">CAPA Version Support</a></h1>
<h2><a class="header" href="#release-versioning" id="release-versioning">Release Versioning</a></h2>
<p>CAPA follows the <a href="https://semver.org/#semantic-versioning-200">semantic versionining</a> specification:</p>
<p>MAJOR version release for incompatible API changes, 
MINOR version release for backwards compatible feature additions, 
and PATCH version release for only bug fixes.</p>
<p><strong>Example versions:</strong></p>
<ul>
<li>Minor release: <code>v0.1.0</code></li>
<li>Patch release: <code>v0.1.1</code></li>
<li>Major release: <code>v1.0.0</code></li>
</ul>
<h2><a class="header" href="#compatibility-with-cluster-api-versions" id="compatibility-with-cluster-api-versions">Compatibility with Cluster API Versions</a></h2>
<p>CAPA’s versions are compatible with the following versions of Cluster API</p>
<table><thead><tr><th>API Version</th><th>Cluster API v1alpha3 (v0.3)</th><th>Cluster API v1alpha4 (v0.4)</th></tr></thead><tbody>
<tr><td>AWS Provider v1alpha3 (v0.6)</td><td>✓</td><td></td></tr>
<tr><td>AWS Provider v1alpha4 (v0.7)</td><td></td><td>✓</td></tr>
</tbody></table>
<p>CAPA v1beta1 versions are not released in lock-step with Cluster API releases.
Multiple CAPA minor releases can use the same Cluster API minor release.</p>
<p>For compatibility, check the release notes <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/">here</a> to see which v1beta1 Cluster API version each CAPA version is compatible with.</p>
<p>For example:</p>
<ul>
<li>CAPA v1.0.x, v1.1.x, v1.2.x is compatible with Cluster API v1.0.x</li>
<li>CAPA v1.3.x is compatible with Cluster API v1.1.x</li>
</ul>
<h2><a class="header" href="#end-of-life-timeline" id="end-of-life-timeline">End-of-Life Timeline</a></h2>
<p>CAPA team maintains branches for <strong>v1.x (v1beta1)</strong>, <strong>v0.7 (v1alpha4)</strong>, and <strong>v0.6 (v1alpha3)</strong>.</p>
<p>CAPA branches follow their compatible Cluster API branch EOL date.</p>
<table><thead><tr><th>API Version</th><th>Branch</th><th>Supported Until</th></tr></thead><tbody>
<tr><td><strong>v1alpha4</strong></td><td>release-0.7</td><td>2022-04-06</td></tr>
<tr><td><strong>v1alpha3</strong></td><td>release-0.6</td><td>2022-02-23</td></tr>
</tbody></table>
<h2><a class="header" href="#compatibility-with-kubernetes-versions" id="compatibility-with-kubernetes-versions">Compatibility with Kubernetes Versions</a></h2>
<p>CAPA API versions support all Kubernetes versions that is supported by its compatible Cluster API version:</p>
<table><thead><tr><th>API Versions</th><th>CAPI v1alpha3 (v0.3)</th><th>CAPI v1alpha4 (v0.4)</th><th>CAPI v1beta1 (v1.x)</th></tr></thead><tbody>
<tr><td>CAPA v1alpha3 (v0.6)</td><td>✓</td><td></td><td></td></tr>
<tr><td>CAPA v1alpha4 (v0.7)</td><td></td><td>✓</td><td></td></tr>
<tr><td>CAPA v1beta1 (v1.x)</td><td></td><td></td><td>✓</td></tr>
</tbody></table>
<p>(See <a href="https://cluster-api.sigs.k8s.io/reference/versions.html">Kubernetes support matrix</a> of Cluster API versions).</p>
<h1><a class="header" href="#contributing-guidelines" id="contributing-guidelines">Contributing guidelines</a></h1>
<h2><a class="header" href="#sign-the-cla" id="sign-the-cla">Sign the CLA</a></h2>
<p>Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests.  Please see https://git.k8s.io/community/CLA.md for more info</p>
<h3><a class="header" href="#contributing-a-patch" id="contributing-a-patch">Contributing A Patch</a></h3>
<ol>
<li>Submit an issue describing your proposed change to the repo in question.</li>
<li>The <a href="topics/reference/OWNERS">repo owners</a> will respond to your issue promptly.</li>
<li>If your proposed change is accepted, and you haven’t already done so, sign a Contributor License Agreement (see details above).</li>
<li>Fork the desired repo, develop and test your code changes. </li>
</ol>
<blockquote>
<p>See the <a href="https://cluster-api-aws.sigs.k8s.io/development/development.html">developer guide</a> on how to setup your development environment.
5. Submit a pull request.</p>
</blockquote>
<h3><a class="header" href="#contributer-ladder" id="contributer-ladder">Contributer Ladder</a></h3>
<p>We broadly follow the requirements from the <a href="https://github.com/kubernetes/community/blob/master/community-membership.md">Kubernetes Community Membership</a>.</p>
<blockquote>
<p>When making changes to <strong>OWNER_ALIASES</strong> please check that the <strong>sig-cluster-lifecycle-leads</strong>, <strong>cluster-api-admins</strong> and <strong>cluster-api-maintainers</strong> are correct.</p>
</blockquote>
<h4><a class="header" href="#becoming-a-reviewer" id="becoming-a-reviewer">Becoming a reviewer</a></h4>
<p>If you would like to become a reviewer, then please ask one of the current maintainers.</p>
<p>We generally try to follow the <a href="https://github.com/kubernetes/community/blob/master/community-membership.md#reviewer">requirements for a reviewer</a> from upstream Kubernetes. But if you feel that you don’t full meet the requirements then reach out to us, they are not set in stone.</p>
<p>A reviewer can get PRs automatically assigned for review, and can <code>/lgtm</code> PRs.</p>
<p>To become a reviewer, ensure you are a member of the <strong>kubernetes-sigs</strong> Github organisation
following https://github.com/kubernetes/org/issues/new/choose.</p>
<p>The steps to add someone as a reviewer are:</p>
<ul>
<li>Add the GitHub alias to the <strong>cluster-api-aws-reviewers</strong> section of <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/OWNERS_ALIASES">OWNERS_ALIASES</a></li>
<li>Create a PR with the change that is held (i.e. by using <code>/hold</code>)</li>
<li>Announce the change within the CAPA slack channel and as a PSA in the next CAPA office hours</li>
<li>After 7 days of lazy consensus or after the next CAPA office hours (whichever is longer) the PR can be merged</li>
</ul>
<h4><a class="header" href="#becoming-a-maintainer" id="becoming-a-maintainer">Becoming a maintainer</a></h4>
<p>If you have made significant contributions to Cluster API Provider AWS, a maintainer may nominate you to become a maintainer for the project.</p>
<p>We generally follow the <a href="https://github.com/kubernetes/community/blob/master/community-membership.md#approver">requirements for a approver</a> from upstream Kubernetes. However, if you don’t fully meet the requirements then a quorum of maintainers may still propose you if they feel you will make significant contributions.</p>
<p>Maintainers are able to approve PRs, as well as participate in release processes and have write access to the repo. <strong>As a maintainer you will be expected to run the office hours, especially if no else wants to</strong>.</p>
<p>Maintainers require membership of the <strong>Kubernetes</strong> Github organisation via
https://github.com/kubernetes/org/issues/new/choose</p>
<p>The steps to add someone as a reviewer are:</p>
<ul>
<li>Add the GitHub alias to the <strong>cluster-api-aws-maintainers</strong> and remove them from <strong>cluster-api-aws-reviewers</strong> sections of <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/OWNERS_ALIASES">OWNERS_ALIASES</a></li>
<li>Create a PR with the change that is held (i.e. by using <code>/hold</code>)</li>
<li>Announce the change within the CAPA slack channel and as a PSA in the next CAPA office hours</li>
<li>After 7 days of lazy consensus or after the next CAPA office hours (whichever is longer) the PR can be merged</li>
<li>Open PR to add Github username to <strong>cluster-api-provider-aws-maintainers</strong>
to https://github.com/kubernetes/org/blob/main/config/kubernetes-sigs/sig-cluster-lifecycle/teams.yaml</li>
<li>Open PR to add Github username to https://github.com/kubernetes/test-infra/blob/master/config/jobs/kubernetes-sigs/cluster-api-provider-aws/OWNERS</li>
<li>Open PR to add Github username to https://github.com/kubernetes/k8s.io/blob/main/k8s.gcr.io/images/k8s-staging-cluster-api-aws/OWNERS</li>
<li>Open PR to add Google ID to the k8s-infra-staging-cluster-api-aws@kubernetes.io Google group in https://github.com/kubernetes/k8s.io/blob/main/groups/groups.yaml</li>
</ul>
<h4><a class="header" href="#becoming-a-admin" id="becoming-a-admin">Becoming a admin</a></h4>
<p>After a period of time one of the existing CAPA or CAPI admins may propose you to become an admin of the CAPA project.</p>
<p>Admins have GitHub <strong>admin</strong> access to perform tasks on the repo.</p>
<p>The steps to add someone as an admin are:</p>
<ul>
<li>Add the GitHub alias to the <strong>cluster-api-aws-admins</strong> section of <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/blob/main/OWNERS_ALIASES">OWNERS_ALIASES</a></li>
<li>Create a PR with the change that is held (i.e. by using <code>/hold</code>)</li>
<li>Announce the change within the CAPA slack channel and as a PSA in the next CAPA office hours</li>
<li>After 7 days of lazy consensus or after the next CAPA office hours (whichever is longer) the PR can be merged</li>
<li>Open PR to add Github username to <strong>cluster-api-provider-aws-admins</strong>
to https://github.com/kubernetes/org/blob/main/config/kubernetes-sigs/sig-cluster-lifecycle/teams.yaml</li>
</ul>
<h1><a class="header" href="#cluster-api-provider-aws-roadmap" id="cluster-api-provider-aws-roadmap">Cluster API Provider AWS Roadmap</a></h1>
<p>This roadmap is a constant work in progress, subject to frequent revision. Dates are approximations.</p>
<h2><a class="header" href="#v15x-v1beta1---aprilmay-2022" id="v15x-v1beta1---aprilmay-2022">v1.5.x (v1beta1) - April/May 2022</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3088">Network load balancer support</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3414">Graduating EventBridge experimental feature</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3384">EFS CSI driver support</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2055">AWSManagedMachinePool - Launch Template support</a></li>
</ul>
<h2><a class="header" href="#v16x-v1beta1---junejuly-2022" id="v16x-v1beta1---junejuly-2022">v1.6.x (v1beta1) - June/July 2022</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2523">Spot instance support for AWSMachinePools</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2574">Node draining support for AWSMachinePools</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2420">IPv6 Support</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/392">Security group customization support</a></li>
</ul>
<h2><a class="header" href="#v20x-v1beta2---end-of-2022" id="v20x-v1beta2---end-of-2022">v2.0.x (v1beta2) - End of 2022</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/1484">Support for multiple topologies</a></li>
</ul>
<h2><a class="header" href="#tbd" id="tbd">TBD</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2173">AWS Fault injector integration to improve resiliency</a></li>
<li>AWSMachinePool implementation backed by Spot Fleet and EC2 Fleet</li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3381">Dual stack IPv4/IPv6 support</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3533">Windows Worker Node Support for Windows Server 2019/2022 for both CAPA-managed and EKS-managed Clusters</a></li>
<li>FIPS/NIST/STIG compliance</li>
<li>Workload identity support to CAPA-managed clusters</li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/discussions/3306">Use ACK/CrossPlane as backend for AWS SDK calls</a></li>
<li>Karpenter support</li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api/issues/3075">Draining resources created by CCM/CSI like LBs, SGs</a></li>
<li><a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/2178">OpenTelemetry integration</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
