<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Quick Start - Kubernetes Cluster API Provider AWS</title>
        
        


        <!-- Custom HTML head -->
        <link
  href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Lato:ital,wght@0,400;0,700;1,400;1,700&display=swap"
  rel="stylesheet">
<link rel="stylesheet" href="https://cdn.datatables.net/1.10.24/css/jquery.dataTables.min.css">
<script src="https://code.jquery.com/jquery-3.5.1.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/jquery.dataTables.min.js"></script>
<meta name="google-site-verification" content="FGwibyytASqneHUQBmwzN0eOO3-Nd_coIx2YlbhzzSM" />


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        
        <link rel="shortcut icon" href="../favicon.png">
        
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        
        <link rel="stylesheet" href="../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="../user/quick-start.html" class="active">Quick Start</a></li><li class="chapter-item expanded "><a href="../user/quick-start-operator.html">Quick Start Operator</a></li><li class="chapter-item expanded "><a href="../topics/images/amis.html">AMIs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/images/built-amis.html">Published AMIs</a></li><li class="chapter-item expanded "><a href="../topics/images/custom-amis.html">Custom AMIs</a></li></ol></li><li class="chapter-item expanded "><a href="../topics/index.html">Topics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/using-clusterawsadm-to-fulfill-prerequisites.html">Using clusterawsadm to fulfill prerequisites</a></li><li class="chapter-item expanded "><a href="../topics/accessing-ec2-instances.html">Accessing EC2 instances</a></li><li class="chapter-item expanded "><a href="../topics/spot-instances.html">Spot instances</a></li><li class="chapter-item expanded "><a href="../topics/machinepools.html">Machine Pools</a></li><li class="chapter-item expanded "><a href="../topics/multitenancy.html">Multi-tenancy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/full-multitenancy-implementation.html">Multi-tenancy in EKS-managed clusters</a></li></ol></li><li class="chapter-item expanded "><a href="../topics/eks/index.html">EKS Support</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/eks/prerequisites.html">Prerequisites</a></li><li class="chapter-item expanded "><a href="../topics/eks/enabling.html">Enabling EKS Support</a></li><li class="chapter-item expanded "><a href="../topics/eks/pod-networking.html">Pod Networking</a></li><li class="chapter-item expanded "><a href="../topics/eks/creating-a-cluster.html">Creating a cluster</a></li><li class="chapter-item expanded "><a href="../topics/eks/creating-a-cluster.html">Disabling</a></li><li class="chapter-item expanded "><a href="../topics/eks/eks-console.html">Using EKS Console</a></li><li class="chapter-item expanded "><a href="../topics/eks/addons.html">Using EKS Addons</a></li><li class="chapter-item expanded "><a href="../topics/eks/encryption.html">Enabling Encryption</a></li><li class="chapter-item expanded "><a href="../topics/eks/cluster-upgrades.html">Cluster Upgrades</a></li></ol></li><li class="chapter-item expanded "><a href="../topics/rosa/index.html">ROSA Support</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/rosa/enabling.html">Enabling ROSA Support</a></li><li class="chapter-item expanded "><a href="../topics/rosa/creating-a-cluster.html">Creating a cluster</a></li><li class="chapter-item expanded "><a href="../topics/rosa/creating-rosa-machinepools.html">Creating MachinePools</a></li><li class="chapter-item expanded "><a href="../topics/rosa/upgrades.html">Upgrades</a></li><li class="chapter-item expanded "><a href="../topics/rosa/external-auth.html">External Auth Providers</a></li><li class="chapter-item expanded "><a href="../topics/rosa/support.html">Support</a></li></ol></li><li class="chapter-item expanded "><a href="../topics/bring-your-own-aws-infrastructure.html">Bring Your Own AWS Infrastructure</a></li><li class="chapter-item expanded "><a href="../topics/specify-management-iam-role.html">Specifying the IAM Role to use for Management Components</a></li><li class="chapter-item expanded "><a href="../topics/external-cloud-provider-with-ebs-csi-driver.html">Using external cloud provider with EBS CSI driver</a></li><li class="chapter-item expanded "><a href="../topics/restricting-cluster-api-to-certain-namespaces.html">Restricting Cluster API to certain namespaces</a></li><li class="chapter-item expanded "><a href="../topics/using-iam-roles-in-mgmt-cluster.html">Using IAM roles in management cluster instead of credentials</a></li><li class="chapter-item expanded "><a href="../topics/failure-domains/index.html">Failure domains</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../topics/failure-domains/control-planes.html">Control planes</a></li><li class="chapter-item expanded "><a href="../topics/failure-domains/worker-nodes.html">Worker nodes</a></li></ol></li><li class="chapter-item expanded "><a href="../topics/userdata-privacy.html">Userdata Privacy</a></li><li class="chapter-item expanded "><a href="../topics/troubleshooting.html">Troubleshooting</a></li><li class="chapter-item expanded "><a href="../topics/iam-permissions.html">IAM Permissions Used</a></li><li class="chapter-item expanded "><a href="../topics/ignition-support.html">Ignition support</a></li><li class="chapter-item expanded "><a href="../topics/external-resource-gc.html">External Resource Garbage Collection</a></li><li class="chapter-item expanded "><a href="../topics/instance-metadata.html">Instance Metadata</a></li><li class="chapter-item expanded "><a href="../topics/network-load-balancer-with-awscluster.html">Network Load Balancers</a></li><li class="chapter-item expanded "><a href="../topics/secondary-load-balancer.html">Secondary Control Plane Load Balancer</a></li><li class="chapter-item expanded "><a href="../topics/provision-edge-zones.html">Provision AWS Local Zone subnets</a></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm.html">clusterawsadm command reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap.html">bootstrap</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_credentials.html">credentials</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_credentials_encode-as-profile.html">encode-as-profile</a></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam.html">iam</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam_print-policy.html">print-policy</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam_create-cloudformation-stack.html">create-cloudformation-stack</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam_delete-cloudformation-stack.html">delete-cloudformation-stack</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam_print-cloudformation-template.html">print-cloudformation-template</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_bootstrap_iam_print-config.html">print-config</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_controller.html">controller</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_controller_print-credentials.html">print-credentials</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_controller_rollout-controller.html">rollout-controller</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_controller_update-credentials.html">update-credentials</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_controller_zero-credentials.html">zero-credentials</a></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_eks.html">eks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_eks_addons.html">addons</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_eks_addons_list-available.html">list-available</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_eks_addons_list-installed.html">list-installed</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_gc.html">gc</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_gc_configure.html">configure</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_gc_disable.html">disable</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_gc_enable.html">enable</a></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_resource.html">resource</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_resource_list.html">list</a></li></ol></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_version.html">version</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_ami.html">ami</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_ami_copy.html">copy</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_ami_encrypted-copy.html">encrypted-copy</a></li><li class="chapter-item expanded "><a href="../clusterawsadm/clusterawsadm_ami_list.html">list</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../development/development.html">Developer Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../development/tilt-setup.html">Development with Tilt</a></li><li class="chapter-item expanded "><a href="../development/e2e.html">Developing E2E tests</a></li><li class="chapter-item expanded "><a href="../development/conventions.html">Coding Conventions</a></li><li class="chapter-item expanded "><a href="../development/nightlies.html">Try unreleased changes with Nightly Builds</a></li><li class="chapter-item expanded "><a href="../development/amis.html">Publishing AMIs</a></li></ol></li><li class="chapter-item expanded "><a href="../crd/index.html">CRD Reference</a></li><li class="chapter-item expanded "><a href="../topics/reference/reference.html">Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../reference/glossary.html">Glossary</a></li><li class="chapter-item expanded "><a href="../topics/reference/ports.html">Ports</a></li><li class="chapter-item expanded "><a href="../topics/reference/jobs.html">Jobs</a></li><li class="chapter-item expanded "><a href="../topics/reference/versions.html">Version Support</a></li><li class="chapter-item expanded "><a href="../topics/reference/contributing.html">Contributing</a></li></ol></li><li class="chapter-item expanded "><a href="../roadmap.html">Roadmap</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Kubernetes Cluster API Provider AWS</h1>

                    <div class="right-buttons">
                        
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://sigs.k8s.io/cluster-api-provider-aws" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#quick-start" id="quick-start">Quick Start</a></h1>
<h1><a class="header" href="#quick-start-1" id="quick-start-1">Quick Start</a></h1>
<p>In this tutorial we’ll cover the basics of how to use Cluster API to create one or more Kubernetes clusters.</p>
<h2><a class="header" href="#installation" id="installation">Installation</a></h2>
<p>There are two major quickstart paths:  Using clusterctl or the Cluster API Operator.</p>
<p>This article describes a path that uses the <code>clusterctl</code> CLI tool to handle the lifecycle of a Cluster API <a href="https://cluster-api.sigs.k8s.io/reference/glossary#management-cluster">management cluster</a>.</p>
<p>The clusterctl command line interface is specifically designed for providing a simple “day 1 experience” and a quick start with Cluster API. It automates fetching the YAML files defining <a href="https://cluster-api.sigs.k8s.io/reference/glossary#provider-components">provider components</a> and installing them.</p>
<p>Additionally it encodes a set of best practices in managing providers, that helps the user in avoiding mis-configurations or in managing day 2 operations such as upgrades.</p>
<p>The Cluster API Operator is a Kubernetes Operator built on top of clusterctl and designed to empower cluster administrators to handle the lifecycle of Cluster API providers within a management cluster using a declarative approach. It aims to improve user experience in deploying and managing Cluster API, making it easier to handle day-to-day tasks and automate workflows with GitOps. Visit the <a href="./quick-start-operator.html">CAPI Operator quickstart</a> if you want to experiment with this tool.</p>
<h3><a class="header" href="#common-prerequisites" id="common-prerequisites">Common Prerequisites</a></h3>
<ul>
<li>Install and setup <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> in your local environment</li>
<li>Install <a href="https://kind.sigs.k8s.io/">kind</a> and <a href="https://www.docker.com/">Docker</a></li>
<li>Install <a href="https://helm.sh/docs/intro/install/">Helm</a></li>
</ul>
<h3><a class="header" href="#install-andor-configure-a-kubernetes-cluster" id="install-andor-configure-a-kubernetes-cluster">Install and/or configure a Kubernetes cluster</a></h3>
<p>Cluster API requires an existing Kubernetes cluster accessible via kubectl. During the installation process the
Kubernetes cluster will be transformed into a <a href="../reference/glossary.html#management-cluster">management cluster</a> by installing the Cluster API <a href="../reference/glossary.html#provider-components">provider components</a>, so it
is recommended to keep it separated from any application workload.</p>
<p>It is a common practice to create a temporary, local bootstrap cluster which is then used to provision
a target <a href="../reference/glossary.html#management-cluster">management cluster</a> on the selected <a href="../reference/glossary.html#infrastructure-provider">infrastructure provider</a>.</p>
<p><strong>Choose one of the options below:</strong></p>
<ol>
<li>
<p><strong>Existing Management Cluster</strong></p>
<p>For production use-cases a “real” Kubernetes cluster should be used with appropriate backup and disaster recovery policies and procedures in place. The Kubernetes cluster must be at least v1.20.0.</p>
<pre><code class="language-bash">export KUBECONFIG=&lt;...&gt;
</code></pre>
</li>
</ol>
<p><strong>OR</strong></p>
<ol start="2">
<li>
<p><strong>Kind</strong></p>
<aside class="note warning">
<h1><a class="header" href="#warning" id="warning">Warning</a></h1>
<p><a href="https://kind.sigs.k8s.io/">kind</a> is not designed for production use.</p>
<p><strong>Minimum <a href="https://kind.sigs.k8s.io/">kind</a> supported version</strong>: v0.31.0</p>
<p><strong>Help with common issues can be found in the <a href="./troubleshooting.html">Troubleshooting Guide</a>.</strong></p>
<p>Note for macOS users: you may need to <a href="https://docs.docker.com/docker-for-mac/#resources">increase the memory available</a> for containers (recommend 6 GB for CAPD).</p>
<p>Note for Linux users: you may need to <a href="./troubleshooting.html#cluster-api-with-docker----too-many-open-files">increase <code>ulimit</code> and <code>inotify</code> when using Docker (CAPD)</a>.</p>
</aside>
<p><a href="https://kind.sigs.k8s.io/">kind</a> can be used for creating a local Kubernetes cluster for development environments or for
the creation of a temporary <a href="../reference/glossary.html#bootstrap-cluster">bootstrap cluster</a> used to provision a target <a href="../reference/glossary.html#management-cluster">management cluster</a> on the selected infrastructure provider.</p>
<p>The installation procedure depends on the version of kind; if you are planning to use the Docker infrastructure provider,
please follow the additional instructions in the dedicated tab:</p>
<div id="install-kind" class="tabset"><input type="radio" name="install-kind" id="install-kind-Default" aria-controls="install-kind-Default" checked><label for="install-kind-Default">Default</label><input type="radio" name="install-kind" id="install-kind-Docker" aria-controls="install-kind-Docker" ><label for="install-kind-Docker">Docker</label><input type="radio" name="install-kind" id="install-kind-KubeVirt" aria-controls="install-kind-KubeVirt" ><label for="install-kind-KubeVirt">KubeVirt</label><div class="tab-panels">
<section id="tab-Default" class="tab-panel">
<p>Create the kind cluster:</p>
<pre><code class="language-bash">kind create cluster
</code></pre>
<p>Test to ensure the local kind cluster is ready:</p>
<pre><code class="language-bash">kubectl cluster-info
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<p>Run the following command to create a kind config file for allowing the Docker provider to access Docker on the host:</p>
<pre><code class="language-bash">cat &gt; kind-cluster-with-extramounts.yaml &lt;&lt;EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  extraMounts:
    - hostPath: /var/run/docker.sock
      containerPath: /var/run/docker.sock
EOF
</code></pre>
<p>Then follow the instruction for your kind version using  <code>kind create cluster --config kind-cluster-with-extramounts.yaml</code>
to create the management cluster using the above file.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<h4><a class="header" href="#create-the-kind-cluster" id="create-the-kind-cluster">Create the Kind Cluster</a></h4>
<p><a href="https://kubevirt.io/">KubeVirt</a> is a cloud native virtualization solution. The virtual machines we’re going to create and use for
the workload cluster’s nodes, are actually running within pods in the management cluster. In order to communicate with
the workload cluster’s API server, we’ll need to expose it. We are using Kind which is a limited environment. The
easiest way to expose the workload cluster’s API server (a pod within a node running in a VM that is itself running
within a pod in the management cluster, that is running inside a Docker container), is to use a LoadBalancer service.</p>
<p>To allow using a LoadBalancer service, we can’t use the kind’s default CNI (kindnet), but we’ll need to install
another CNI, like Calico. In order to do that, we’ll need first to initiate the kind cluster with two modifications:</p>
<ol>
<li>Disable the default CNI</li>
<li>Add the Docker credentials to the cluster, to avoid the Docker Hub pull rate limit of the calico images; read more
about it in the <a href="https://docs.docker.com/docker-hub/download-rate-limit/">docker documentation</a>, and in the
<a href="https://kind.sigs.k8s.io/docs/user/private-registries/#mount-a-config-file-to-each-node">kind documentation</a>.</li>
</ol>
<p>Create a configuration file for kind. Please notice the Docker config file path, and adjust it to your local setting:</p>
<pre><code class="language-bash">cat &lt;&lt;EOF &gt; kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
# the default CNI will not be installed
  disableDefaultCNI: true
nodes:
- role: control-plane
  extraMounts:
   - containerPath: /var/lib/kubelet/config.json
     hostPath: &lt;YOUR DOCKER CONFIG FILE PATH&gt;
EOF
</code></pre>
<p>Now, create the kind cluster with the configuration file:</p>
<pre><code class="language-bash">kind create cluster --config=kind-config.yaml
</code></pre>
<p>Test to ensure the local kind cluster is ready:</p>
<pre><code class="language-bash">kubectl cluster-info
</code></pre>
<h4><a class="header" href="#install-the-calico-cni" id="install-the-calico-cni">Install the Calico CNI</a></h4>
<p>Now we’ll need to install a CNI. In this example, we’re using calico, but other CNIs should work as well. Please see
<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico">calico installation guide</a>
for more details (use the “Manifest” tab). Below is an example of how to install calico version v3.29.1.</p>
<p>Use the Calico manifest to create the required resources; e.g.:</p>
<pre><code class="language-bash">kubectl create -f  https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/calico.yaml
</code></pre>
</section>
</div></div>
</li>
</ol>
<h3><a class="header" href="#install-clusterctl" id="install-clusterctl">Install clusterctl</a></h3>
<p>The clusterctl CLI tool handles the lifecycle of a Cluster API management cluster.</p>
<div id="install-clusterctl" class="tabset"><input type="radio" name="install-clusterctl" id="install-clusterctl-Linux" aria-controls="install-clusterctl-Linux" checked><label for="install-clusterctl-Linux">Linux</label><input type="radio" name="install-clusterctl" id="install-clusterctl-macOS" aria-controls="install-clusterctl-macOS" ><label for="install-clusterctl-macOS">macOS</label><input type="radio" name="install-clusterctl" id="install-clusterctl-homebrew" aria-controls="install-clusterctl-homebrew" ><label for="install-clusterctl-homebrew">homebrew</label><input type="radio" name="install-clusterctl" id="install-clusterctl-Windows" aria-controls="install-clusterctl-Windows" ><label for="install-clusterctl-Windows">Windows</label><div class="tab-panels">
<section id="tab-Linux" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-linux" id="install-clusterctl-binary-with-curl-on-linux">Install clusterctl binary with curl on Linux</a></h4>
<p>If you are unsure you can determine your computers architecture by running <code>uname -a</code></p>
<p>Download for AMD64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-amd64 -o clusterctl
</code></pre>
<p>Download for ARM64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-arm64 -o clusterctl
</code></pre>
<p>Download for PPC64LE:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-linux-ppc64le -o clusterctl
</code></pre>
<p>Install clusterctl:</p>
<pre><code class="language-bash">sudo install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-macOS" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-macos" id="install-clusterctl-binary-with-curl-on-macos">Install clusterctl binary with curl on macOS</a></h4>
<p>If you are unsure you can determine your computers architecture by running <code>uname -a</code></p>
<p>Download for AMD64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-darwin-amd64 -o clusterctl
</code></pre>
<p>Download for M1 CPU (”Apple Silicon”) / ARM64:</p>
<pre><code class="language-bash">curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-darwin-arm64 -o clusterctl
</code></pre>
<p>Make the clusterctl binary executable.</p>
<pre><code class="language-bash">chmod +x ./clusterctl
</code></pre>
<p>Move the binary in to your PATH.</p>
<pre><code class="language-bash">sudo mv ./clusterctl /usr/local/bin/clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-homebrew" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-with-homebrew-on-macos-and-linux" id="install-clusterctl-with-homebrew-on-macos-and-linux">Install clusterctl with homebrew on macOS and Linux</a></h4>
<p>Install the latest release using homebrew:</p>
<pre><code class="language-bash">brew install clusterctl
</code></pre>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-bash">clusterctl version
</code></pre>
</section>
<section id="tab-windows" class="tab-panel">
<h4><a class="header" href="#install-clusterctl-binary-with-curl-on-windows-using-powershell" id="install-clusterctl-binary-with-curl-on-windows-using-powershell">Install clusterctl binary with curl on Windows using PowerShell</a></h4>
<p>Go to the working directory where you want clusterctl downloaded.</p>
<p>Download the latest release; on Windows, type:</p>
<pre><code class="language-powershell">curl.exe -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.12.1/clusterctl-windows-amd64.exe -o clusterctl.exe
</code></pre>
<p>Append or prepend the path of that directory to the <code>PATH</code> environment variable.</p>
<p>Test to ensure the version you installed is up-to-date:</p>
<pre><code class="language-powershell">clusterctl.exe version
</code></pre>
</section>
</div></div>
<h3><a class="header" href="#initialize-the-management-cluster" id="initialize-the-management-cluster">Initialize the management cluster</a></h3>
<p>Now that we’ve got clusterctl installed and all the prerequisites in place, let’s transform the Kubernetes cluster
into a management cluster by using <code>clusterctl init</code>.</p>
<p>The command accepts as input a list of providers to install; when executed for the first time, <code>clusterctl init</code>
automatically adds to the list the <code>cluster-api</code> core provider, and if unspecified, it also adds the <code>kubeadm</code> bootstrap
and <code>kubeadm</code> control-plane providers.</p>
<h4><a class="header" href="#enabling-feature-gates" id="enabling-feature-gates">Enabling Feature Gates</a></h4>
<p>Feature gates can be enabled by exporting environment variables before executing <code>clusterctl init</code>.
For example, the <code>ClusterTopology</code> feature, which is required to enable support for managed topologies and ClusterClass,
can be enabled via:</p>
<pre><code class="language-bash">export CLUSTER_TOPOLOGY=true
</code></pre>
<p>Additional documentation about experimental features can be found in <a href="../tasks/experimental-features/experimental-features.html">Experimental Features</a>.</p>
<h4><a class="header" href="#initialization-for-common-providers" id="initialization-for-common-providers">Initialization for common providers</a></h4>
<p>Depending on the infrastructure provider you are planning to use, some additional prerequisites should be satisfied
before getting started with Cluster API. See below for the expected settings for common providers.</p>
<div id="tab-installation-infrastructure" class="tabset"><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Akamai (Linode)" aria-controls="tab-installation-infrastructure-Akamai (Linode)" checked><label for="tab-installation-infrastructure-Akamai (Linode)">Akamai (Linode)</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-AWS" aria-controls="tab-installation-infrastructure-AWS" ><label for="tab-installation-infrastructure-AWS">AWS</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Azure" aria-controls="tab-installation-infrastructure-Azure" ><label for="tab-installation-infrastructure-Azure">Azure</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-CloudStack" aria-controls="tab-installation-infrastructure-CloudStack" ><label for="tab-installation-infrastructure-CloudStack">CloudStack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-DigitalOcean" aria-controls="tab-installation-infrastructure-DigitalOcean" ><label for="tab-installation-infrastructure-DigitalOcean">DigitalOcean</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Docker" aria-controls="tab-installation-infrastructure-Docker" ><label for="tab-installation-infrastructure-Docker">Docker</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-GCP" aria-controls="tab-installation-infrastructure-GCP" ><label for="tab-installation-infrastructure-GCP">GCP</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Harvester" aria-controls="tab-installation-infrastructure-Harvester" ><label for="tab-installation-infrastructure-Harvester">Harvester</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Hetzner" aria-controls="tab-installation-infrastructure-Hetzner" ><label for="tab-installation-infrastructure-Hetzner">Hetzner</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Hivelocity" aria-controls="tab-installation-infrastructure-Hivelocity" ><label for="tab-installation-infrastructure-Hivelocity">Hivelocity</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Huawei" aria-controls="tab-installation-infrastructure-Huawei" ><label for="tab-installation-infrastructure-Huawei">Huawei</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-IBM Cloud" aria-controls="tab-installation-infrastructure-IBM Cloud" ><label for="tab-installation-infrastructure-IBM Cloud">IBM Cloud</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-IONOS Cloud" aria-controls="tab-installation-infrastructure-IONOS Cloud" ><label for="tab-installation-infrastructure-IONOS Cloud">IONOS Cloud</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-K0smotron" aria-controls="tab-installation-infrastructure-K0smotron" ><label for="tab-installation-infrastructure-K0smotron">K0smotron</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-KubeKey" aria-controls="tab-installation-infrastructure-KubeKey" ><label for="tab-installation-infrastructure-KubeKey">KubeKey</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-KubeVirt" aria-controls="tab-installation-infrastructure-KubeVirt" ><label for="tab-installation-infrastructure-KubeVirt">KubeVirt</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Metal3" aria-controls="tab-installation-infrastructure-Metal3" ><label for="tab-installation-infrastructure-Metal3">Metal3</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-metal-stack" aria-controls="tab-installation-infrastructure-metal-stack" ><label for="tab-installation-infrastructure-metal-stack">metal-stack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Nutanix" aria-controls="tab-installation-infrastructure-Nutanix" ><label for="tab-installation-infrastructure-Nutanix">Nutanix</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OCI" aria-controls="tab-installation-infrastructure-OCI" ><label for="tab-installation-infrastructure-OCI">OCI</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OpenNebula" aria-controls="tab-installation-infrastructure-OpenNebula" ><label for="tab-installation-infrastructure-OpenNebula">OpenNebula</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-OpenStack" aria-controls="tab-installation-infrastructure-OpenStack" ><label for="tab-installation-infrastructure-OpenStack">OpenStack</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Outscale" aria-controls="tab-installation-infrastructure-Outscale" ><label for="tab-installation-infrastructure-Outscale">Outscale</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Proxmox" aria-controls="tab-installation-infrastructure-Proxmox" ><label for="tab-installation-infrastructure-Proxmox">Proxmox</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Scaleway" aria-controls="tab-installation-infrastructure-Scaleway" ><label for="tab-installation-infrastructure-Scaleway">Scaleway</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-VCD" aria-controls="tab-installation-infrastructure-VCD" ><label for="tab-installation-infrastructure-VCD">VCD</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-vcluster" aria-controls="tab-installation-infrastructure-vcluster" ><label for="tab-installation-infrastructure-vcluster">vcluster</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Virtink" aria-controls="tab-installation-infrastructure-Virtink" ><label for="tab-installation-infrastructure-Virtink">Virtink</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-vSphere" aria-controls="tab-installation-infrastructure-vSphere" ><label for="tab-installation-infrastructure-vSphere">vSphere</label><input type="radio" name="tab-installation-infrastructure" id="tab-installation-infrastructure-Vultr" aria-controls="tab-installation-infrastructure-Vultr" ><label for="tab-installation-infrastructure-Vultr">Vultr</label><div class="tab-panels">
<section id="tab-Akamai (Linode)" class="tab-panel">
<pre><code class="language-bash">export LINODE_TOKEN=&lt;your-access-token&gt;

# Initialize the management cluster
clusterctl init --infrastructure linode-linode
</code></pre>
</section>
<section id="tab-AWS" class="tab-panel">
<p>Download the latest binary of <code>clusterawsadm</code> from the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases">AWS provider releases</a>. The <a href="https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm.html">clusterawsadm</a> command line utility assists with identity and access management (IAM) for <a href="https://cluster-api-aws.sigs.k8s.io">Cluster API Provider AWS</a>.</p>
<div id="install-clusterawsadm" class="tabset"><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-Linux" aria-controls="install-clusterawsadm-Linux" checked><label for="install-clusterawsadm-Linux">Linux</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-macOS" aria-controls="install-clusterawsadm-macOS" ><label for="install-clusterawsadm-macOS">macOS</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-homebrew" aria-controls="install-clusterawsadm-homebrew" ><label for="install-clusterawsadm-homebrew">homebrew</label><input type="radio" name="install-clusterawsadm" id="install-clusterawsadm-Windows" aria-controls="install-clusterawsadm-Windows" ><label for="install-clusterawsadm-Windows">Windows</label><div class="tab-panels">
<section id="tab-Linux" class="tab-panel">
<p>Download the latest release; on Linux, type:</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-linux-amd64 -o clusterawsadm
</code></pre>
<p>Make it executable</p>
<pre><code>chmod +x clusterawsadm
</code></pre>
<p>Move the binary to a directory present in your PATH</p>
<pre><code>sudo mv clusterawsadm /usr/local/bin
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-macOS" class="tab-panel">
<p>Download the latest release; on macOs, type:</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-darwin-amd64 -o clusterawsadm
</code></pre>
<p>Or if your Mac has an M1 CPU (”Apple Silicon”):</p>
<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-darwin-arm64 -o clusterawsadm
</code></pre>
<p>Make it executable</p>
<pre><code>chmod +x clusterawsadm
</code></pre>
<p>Move the binary to a directory present in your PATH</p>
<pre><code>sudo mv clusterawsadm /usr/local/bin
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-homebrew" class="tab-panel">
<p>Install the latest release using homebrew:</p>
<pre><code>brew install clusterawsadm
</code></pre>
<p>Check version to confirm installation</p>
<pre><code>clusterawsadm version
</code></pre>
<p><strong>Example Usage</strong></p>
<pre><code class="language-bash">export AWS_REGION=us-east-1 # This is used to help encode your environment variables
export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your-secret-access-key&gt;
export AWS_SESSION_TOKEN=&lt;session-token&gt; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
<section id="tab-Windows" class="tab-panel">
<p>Download the latest release; on Windows, type:</p>
<pre><code>curl.exe -L https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.10.0/clusterawsadm-windows-amd64.exe -o clusterawsadm.exe
</code></pre>
<p>Append or prepend the path of that directory to the <code>PATH</code> environment variable.
Check version to confirm installation</p>
<pre><code>clusterawsadm.exe version
</code></pre>
<p><strong>Example Usage in Powershell</strong></p>
<pre><code class="language-bash">$Env:AWS_REGION=&quot;us-east-1&quot; # This is used to help encode your environment variables
$Env:AWS_ACCESS_KEY_ID=&quot;&lt;your-access-key&gt;&quot;
$Env:AWS_SECRET_ACCESS_KEY=&quot;&lt;your-secret-access-key&gt;&quot;
$Env:AWS_SESSION_TOKEN=&quot;&lt;session-token&gt;&quot; # If you are using Multi-Factor Auth.

# The clusterawsadm utility takes the credentials that you set as environment
# variables and uses them to create a CloudFormation stack in your AWS account
# with the correct IAM resources.
clusterawsadm bootstrap iam create-cloudformation-stack

# Create the base64 encoded credentials using clusterawsadm.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
$Env:AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)

# Finally, initialize the management cluster
clusterctl init --infrastructure aws
</code></pre>
</section>
</div></div>
<p>See the <a href="https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites.html">AWS provider prerequisites</a> document for more details.</p>
</section>
<section id="tab-Azure" class="tab-panel">
<p>For more information about authorization, AAD, or requirements for Azure, visit the <a href="https://capz.sigs.k8s.io/getting-started-with-aks.html#prerequisites">Azure provider prerequisites</a> document.</p>
<pre><code class="language-bash">export AZURE_SUBSCRIPTION_ID=&quot;&lt;SubscriptionId&gt;&quot;

# Create an Azure Service Principal and paste the output here
export AZURE_TENANT_ID=&quot;&lt;Tenant&gt;&quot;
export AZURE_CLIENT_ID=&quot;&lt;AppId&gt;&quot;
export AZURE_CLIENT_ID_USER_ASSIGNED_IDENTITY=$AZURE_CLIENT_ID # for compatibility with CAPZ v1.16 templates
export AZURE_CLIENT_SECRET=&quot;&lt;Password&gt;&quot;

# Settings needed for AzureClusterIdentity used by the AzureCluster
export AZURE_CLUSTER_IDENTITY_SECRET_NAME=&quot;cluster-identity-secret&quot;
export CLUSTER_IDENTITY_NAME=&quot;cluster-identity&quot;
export AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE=&quot;default&quot;

# Create a secret to include the password of the Service Principal identity created in Azure
# This secret will be referenced by the AzureClusterIdentity used by the AzureCluster
kubectl create secret generic &quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAME}&quot; --from-literal=clientSecret=&quot;${AZURE_CLIENT_SECRET}&quot; --namespace &quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}&quot;

# Finally, initialize the management cluster
clusterctl init --infrastructure azure
</code></pre>
</section>
<section id="tab-CloudStack" class="tab-panel">
<p>Create a file named cloud-config in the repo’s root directory, substituting in your own environment’s values</p>
<pre><code class="language-bash">[Global]
api-url = &lt;cloudstackApiUrl&gt;
api-key = &lt;cloudstackApiKey&gt;
secret-key = &lt;cloudstackSecretKey&gt;
</code></pre>
<p>Create the base64 encoded credentials by catting your credentials file.
This command uses your environment variables and encodes
them in a value to be stored in a Kubernetes Secret.</p>
<pre><code class="language-bash">export CLOUDSTACK_B64ENCODED_SECRET=`cat cloud-config | base64 | tr -d '\n'`
</code></pre>
<p>Finally, initialize the management cluster</p>
<pre><code class="language-bash">clusterctl init --infrastructure cloudstack
</code></pre>
</section>
<section id="tab-DigitalOcean" class="tab-panel">
<pre><code class="language-bash">export DIGITALOCEAN_ACCESS_TOKEN=&lt;your-access-token&gt;
export DO_B64ENCODED_CREDENTIALS=&quot;$(echo -n &quot;${DIGITALOCEAN_ACCESS_TOKEN}&quot; | base64 | tr -d '\n')&quot;

# Initialize the management cluster
clusterctl init --infrastructure digitalocean
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-1" id="warning-1">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<p>The Docker provider requires the <code>ClusterTopology</code> and <code>MachinePool</code> features to deploy ClusterClass-based clusters.
We are only supporting ClusterClass-based cluster-templates in this quickstart as ClusterClass makes it possible to
adapt configuration based on Kubernetes version. This is required to install Kubernetes clusters &lt; v1.24 and
for the upgrade from v1.23 to v1.24 as we have to use different cgroupDrivers depending on Kubernetes version.</p>
<pre><code class="language-bash"># Enable the experimental Cluster topology feature.
export CLUSTER_TOPOLOGY=true

# Initialize the management cluster
clusterctl init --infrastructure docker
</code></pre>
</section>
<section id="tab-GCP" class="tab-panel">
<pre><code class="language-bash"># Create the base64 encoded credentials by catting your credentials json.
# This command uses your environment variables and encodes
# them in a value to be stored in a Kubernetes Secret.
export GCP_B64ENCODED_CREDENTIALS=$( cat /path/to/gcp-credentials.json | base64 | tr -d '\n' )

# Finally, initialize the management cluster
clusterctl init --infrastructure gcp
</code></pre>
</section>
<section id="tab-Harvester" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure harvester-harvester
</code></pre>
<p>For more information, please visit the <a href="https://github.com/rancher-sandbox/cluster-api-provider-harvester">Harvester project</a>.</p>
</section>
<section id="tab-Hetzner" class="tab-panel">
<p>Please visit the <a href="https://github.com/syself/cluster-api-provider-hetzner">Hetzner project</a>.</p>
</section>
<section id="tab-Hivelocity" class="tab-panel">
<p>Please visit the <a href="https://github.com/hivelocity/cluster-api-provider-hivelocity">Hivelocity project</a>.</p>
</section>
<section id="tab-Huawei" class="tab-panel">
<pre><code class="language-bash"># Please ensure that the values for `CLOUD_SDK_AK` and `CLOUD_SDK_SK` are base64 encoded.
export CLOUD_SDK_AK=$( echo $AccessKey | base64 | tr -d '\n' )
export CLOUD_SDK_SK=$( echo $SecretKey | base64 | tr -d '\n' )

# Finally, initialize the management cluster
clusterctl init --infrastructure huawei
</code></pre>
</section>
<section id="tab-IBM Cloud" class="tab-panel">
<p>In order to initialize the IBM Cloud Provider you have to expose the environment
variable <code>IBMCLOUD_API_KEY</code>. This variable is used to authorize the infrastructure
provider manager against the IBM Cloud API. To create one from the UI, refer <a href="https://cloud.ibm.com/docs/account?topic=account-userapikey&amp;interface=ui#create_user_key">here</a>.</p>
<pre><code class="language-bash">export IBMCLOUD_API_KEY=&lt;you_api_key&gt;

# Finally, initialize the management cluster
clusterctl init --infrastructure ibmcloud
</code></pre>
</section>
<section id="tab-IONOS Cloud" class="tab-panel">
<p>The IONOS Cloud credentials are configured in the <code>IONOSCloudCluster</code>.
Therefore, there is no need to specify them during the provider initialization.</p>
<pre><code class="language-bash">clusterctl init --infrastructure ionoscloud-ionoscloud
</code></pre>
<p>For more information, please visit the <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud">IONOS Cloud project</a>.</p>
</section>
<section id="tab-K0smotron" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure k0sproject-k0smotron
</code></pre>
</section>
<section id="tab-KubeKey" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure kubekey
</code></pre>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt/">KubeVirt project</a> for more information.</p>
<p>As described above, we want to use a LoadBalancer service in order to expose the workload cluster’s API server. In the
example below, we will use <a href="https://metallb.universe.tf/">MetalLB</a> solution to implement load balancing to our kind
cluster. Other solution should work as well.</p>
<h4><a class="header" href="#install-metallb-for-load-balancing" id="install-metallb-for-load-balancing">Install MetalLB for load balancing</a></h4>
<p>Install MetalLB, as described <a href="https://metallb.universe.tf/installation/#installation-by-manifest">here</a>; for example:</p>
<pre><code class="language-bash">METALLB_VER=$(curl &quot;https://api.github.com/repos/metallb/metallb/releases/latest&quot; | jq -r &quot;.tag_name&quot;)
kubectl apply -f &quot;https://raw.githubusercontent.com/metallb/metallb/${METALLB_VER}/config/manifests/metallb-native.yaml&quot;
kubectl wait pods -n metallb-system -l app=metallb,component=controller --for=condition=Ready --timeout=10m
kubectl wait pods -n metallb-system -l app=metallb,component=speaker --for=condition=Ready --timeout=2m
</code></pre>
<p>Now, we’ll create the <code>IPAddressPool</code> and the <code>L2Advertisement</code> custom resources. For that, we’ll need to set the IP
range. First, we’ll read the <code>kind</code> network in order to find its subnet:</p>
<pre><code class="language-bash">SUBNET=$(docker network inspect -f '{{range .IPAM.Config}}{{if .Gateway}}{{.Subnet}}{{end}}{{end}}' kind)
PREFIX=$(echo $SUBNET | sed -E 's|^([0-9]+\.[0-9]+)\..*$|\1|g')

cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: capi-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${PREFIX}.255.200-${PREFIX}.255.250
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: empty
  namespace: metallb-system
EOF
</code></pre>
<aside class="note warning">
<h1><a class="header" href="#notice" id="notice">Notice</a></h1>
<p>The example above is based on the Docker container runtime. The output of <code>docker network inspect</code> may be different when
using another runtime. In such a case, the IPAddressPool’s <code>spec.addresses</code> field should be populated manually,
according to the specific network.</p>
</aside>
<h4><a class="header" href="#install-kubevirt-on-the-kind-cluster" id="install-kubevirt-on-the-kind-cluster">Install KubeVirt on the kind cluster</a></h4>
<pre><code class="language-bash"># get KubeVirt version
KV_VER=$(curl &quot;https://api.github.com/repos/kubevirt/kubevirt/releases/latest&quot; | jq -r &quot;.tag_name&quot;)
# deploy required CRDs
kubectl apply -f &quot;https://github.com/kubevirt/kubevirt/releases/download/${KV_VER}/kubevirt-operator.yaml&quot;
# deploy the KubeVirt custom resource
kubectl apply -f &quot;https://github.com/kubevirt/kubevirt/releases/download/${KV_VER}/kubevirt-cr.yaml&quot;
kubectl wait -n kubevirt kv kubevirt --for=condition=Available --timeout=10m
</code></pre>
<h4><a class="header" href="#initialize-the-management-cluster-with-the-kubevirt-provider" id="initialize-the-management-cluster-with-the-kubevirt-provider">Initialize the management cluster with the KubeVirt Provider</a></h4>
<pre><code class="language-bash">clusterctl init --infrastructure kubevirt
</code></pre>
</section>
<section id="tab-Metal3" class="tab-panel">
<p>Please visit the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/">Metal3 project</a>.</p>
</section>
<section id="tab-metal-stack" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure metal-stack
</code></pre>
<p>Please follow the Cluster API Provider for <a href="https://metal-stack.io/docs/references/cluster-api-provider-metal-stack#getting-started">metal-stack Getting Started Guide</a></p>
</section>
<section id="tab-Nutanix" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://opendocs.nutanix.com/capx/latest/getting_started/">Nutanix Getting Started Guide</a></p>
</section>
<section id="tab-OCI" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://oracle.github.io/cluster-api-provider-oci/#getting-started">Oracle Cloud Infrastructure (OCI) Getting Started Guide</a></p>
</section>
<section id="tab-OpenNebula" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure opennebula
</code></pre>
<p>Please visit <a href="https://github.com/OpenNebula/cluster-api-provider-opennebula/wiki">OpenNebula Cluster API Provider Wiki</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>Cluster API Provider OpenStack depends on <a href="https://k-orc.cloud/">openstack-resource-controller</a> since v0.12.</p>
<pre><code class="language-bash"># Install ORC (needed for CAPO &gt;=v0.12)
kubectl apply -f https://github.com/k-orc/openstack-resource-controller/releases/latest/download/install.yaml
# Initialize the management cluster
clusterctl init --infrastructure openstack
</code></pre>
</section>
<section id="tab-Outscale" class="tab-panel">
<pre><code class="language-bash">export OSC_SECRET_KEY=&lt;your-secret-key&gt;
export OSC_ACCESS_KEY=&lt;your-access-key&gt;
export OSC_REGION=&lt;you-region&gt;
# Create namespace
kubectl create namespace cluster-api-provider-outscale-system
# Create secret
kubectl create secret generic cluster-api-provider-outscale --from-literal=access_key=${OSC_ACCESS_KEY} --from-literal=secret_key=${OSC_SECRET_KEY} --from-literal=region=${OSC_REGION}  -n cluster-api-provider-outscale-system
# Initialize the management cluster
clusterctl init --infrastructure outscale
</code></pre>
</section>
<section id="tab-Proxmox" class="tab-panel">
<p>The Proxmox credentials are optional, when creating a cluster they can be set in the <code>ProxmoxCluster</code> resource,
if you do not set them here.</p>
<pre><code class="language-bash"># The host for the Proxmox cluster
export PROXMOX_URL=&quot;https://pve.example:8006&quot;
# The Proxmox token ID to access the remote Proxmox endpoint
export PROXMOX_TOKEN='root@pam!capi'
# The secret associated with the token ID
# You may want to set this in `$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml` so your password is not in
# bash history
export PROXMOX_SECRET=&quot;1234-1234-1234-1234&quot;


# Finally, initialize the management cluster
clusterctl init --infrastructure proxmox --ipam in-cluster
</code></pre>
<p>For more information about the CAPI provider for Proxmox, see the <a href="https://github.com/ionos-cloud/cluster-api-provider-proxmox/blob/main/docs/Usage.md">Proxmox
project</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure scaleway
</code></pre>
</section>
<section id="tab-VCD" class="tab-panel">
<p>Please follow the Cluster API Provider for <a href="https://github.com/vmware/cluster-api-provider-cloud-director/blob/main/README.md">Cloud Director Getting Started Guide</a></p>
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure vcd
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">clusterctl init --infrastructure vcluster
</code></pre>
<p>Please follow the Cluster API Provider for <a href="https://github.com/loft-sh/cluster-api-provider-vcluster/blob/main/docs/quick-start.md">vcluster Quick Start Guide</a></p>
</section>
<section id="tab-Virtink" class="tab-panel">
<pre><code class="language-bash"># Initialize the management cluster
clusterctl init --infrastructure virtink
</code></pre>
</section>
<section id="tab-vSphere" class="tab-panel">
<pre><code class="language-bash"># The username used to access the remote vSphere endpoint
export VSPHERE_USERNAME=&quot;vi-admin@vsphere.local&quot;
# The password used to access the remote vSphere endpoint
# You may want to set this in `$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml` so your password is not in
# bash history
export VSPHERE_PASSWORD=&quot;admin!23&quot;

# Finally, initialize the management cluster
clusterctl init --infrastructure vsphere
</code></pre>
<p>For more information about prerequisites, credentials management, or permissions for vSphere, see the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md">vSphere
project</a>.</p>
</section>
<section id="tab-Vultr" class="tab-panel">
<pre><code class="language-bash">export VULTR_API_KEY=&quot;$(echo -n &quot;${VULTR_API_KEY}&quot; | base64 | tr -d '\n')&quot;

# initialize the management cluster
clusterctl init --infrastructure vultr-vultr
</code></pre>
</section>
</div></div>
<p>The output of <code>clusterctl init</code> is similar to this:</p>
<pre><code class="language-bash">Fetching providers
Installing cert-manager Version=&quot;v1.11.0&quot;
Waiting for cert-manager to be available...
Installing Provider=&quot;cluster-api&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-system&quot;
Installing Provider=&quot;bootstrap-kubeadm&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-kubeadm-bootstrap-system&quot;
Installing Provider=&quot;control-plane-kubeadm&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capi-kubeadm-control-plane-system&quot;
Installing Provider=&quot;infrastructure-docker&quot; Version=&quot;v1.0.0&quot; TargetNamespace=&quot;capd-system&quot;

Your management cluster has been initialized successfully!

You can now create your first workload cluster by running the following:

  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -
</code></pre>
<aside class="note">
<h1><a class="header" href="#alternatives-to-environment-variables" id="alternatives-to-environment-variables">Alternatives to environment variables</a></h1>
<p>Throughout this quickstart guide we’ve given instructions on setting parameters using environment variables. For most
environment variables in the rest of the guide, you can also set them in <code>$XDG_CONFIG_HOME/cluster-api/clusterctl.yaml</code></p>
<p>See <a href="../clusterctl/commands/init.html"><code>clusterctl init</code></a> for more details.</p>
</aside>
<h3><a class="header" href="#create-your-first-workload-cluster" id="create-your-first-workload-cluster">Create your first workload cluster</a></h3>
<p>Once the management cluster is ready, you can create your first workload cluster.</p>
<h4><a class="header" href="#preparing-the-workload-cluster-configuration" id="preparing-the-workload-cluster-configuration">Preparing the workload cluster configuration</a></h4>
<p>The <code>clusterctl generate cluster</code> command returns a YAML template for creating a <a href="../reference/glossary.html#workload-cluster">workload cluster</a>.</p>
<aside class="note">
<h1><a class="header" href="#which-provider-will-be-used-for-my-cluster" id="which-provider-will-be-used-for-my-cluster"> Which provider will be used for my cluster? </a></h1>
<p>The <code>clusterctl generate cluster</code> command uses smart defaults in order to simplify the user experience; for example,
if only the <code>aws</code> infrastructure provider is deployed, it detects and uses that when creating the cluster.</p>
</aside>
<aside class="note">
<h1><a class="header" href="#what-topology-will-be-used-for-my-cluster" id="what-topology-will-be-used-for-my-cluster"> What topology will be used for my cluster? </a></h1>
<p>The <code>clusterctl generate cluster</code> command by default uses cluster templates which are provided by the infrastructure
providers. See the provider’s documentation for more information.</p>
<p>See the <code>clusterctl generate cluster</code> <a href="../clusterctl/commands/generate-cluster.html">command</a> documentation for
details about how to use alternative sources. for cluster templates.</p>
</aside>
<h4><a class="header" href="#required-configuration-for-common-providers" id="required-configuration-for-common-providers">Required configuration for common providers</a></h4>
<p>Depending on the infrastructure provider you are planning to use, some additional prerequisites should be satisfied
before configuring a cluster with Cluster API. Instructions are provided for common providers below.</p>
<p>Otherwise, you can look at the <code>clusterctl generate cluster</code> <a href="../clusterctl/commands/generate-cluster.html">command</a> documentation for details about how to
discover the list of variables required by a cluster templates.</p>
<div id="tab-configuration-infrastructure" class="tabset"><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Akamai (Linode)" aria-controls="tab-configuration-infrastructure-Akamai (Linode)" checked><label for="tab-configuration-infrastructure-Akamai (Linode)">Akamai (Linode)</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-AWS" aria-controls="tab-configuration-infrastructure-AWS" ><label for="tab-configuration-infrastructure-AWS">AWS</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Azure" aria-controls="tab-configuration-infrastructure-Azure" ><label for="tab-configuration-infrastructure-Azure">Azure</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-CloudStack" aria-controls="tab-configuration-infrastructure-CloudStack" ><label for="tab-configuration-infrastructure-CloudStack">CloudStack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-DigitalOcean" aria-controls="tab-configuration-infrastructure-DigitalOcean" ><label for="tab-configuration-infrastructure-DigitalOcean">DigitalOcean</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Docker" aria-controls="tab-configuration-infrastructure-Docker" ><label for="tab-configuration-infrastructure-Docker">Docker</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-GCP" aria-controls="tab-configuration-infrastructure-GCP" ><label for="tab-configuration-infrastructure-GCP">GCP</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Harvester" aria-controls="tab-configuration-infrastructure-Harvester" ><label for="tab-configuration-infrastructure-Harvester">Harvester</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Huawei" aria-controls="tab-configuration-infrastructure-Huawei" ><label for="tab-configuration-infrastructure-Huawei">Huawei</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-IBM Cloud" aria-controls="tab-configuration-infrastructure-IBM Cloud" ><label for="tab-configuration-infrastructure-IBM Cloud">IBM Cloud</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-IONOS Cloud" aria-controls="tab-configuration-infrastructure-IONOS Cloud" ><label for="tab-configuration-infrastructure-IONOS Cloud">IONOS Cloud</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-K0smotron" aria-controls="tab-configuration-infrastructure-K0smotron" ><label for="tab-configuration-infrastructure-K0smotron">K0smotron</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-KubeKey" aria-controls="tab-configuration-infrastructure-KubeKey" ><label for="tab-configuration-infrastructure-KubeKey">KubeKey</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-KubeVirt" aria-controls="tab-configuration-infrastructure-KubeVirt" ><label for="tab-configuration-infrastructure-KubeVirt">KubeVirt</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Metal3" aria-controls="tab-configuration-infrastructure-Metal3" ><label for="tab-configuration-infrastructure-Metal3">Metal3</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-metal-stack" aria-controls="tab-configuration-infrastructure-metal-stack" ><label for="tab-configuration-infrastructure-metal-stack">metal-stack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Nutanix" aria-controls="tab-configuration-infrastructure-Nutanix" ><label for="tab-configuration-infrastructure-Nutanix">Nutanix</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-OpenNebula" aria-controls="tab-configuration-infrastructure-OpenNebula" ><label for="tab-configuration-infrastructure-OpenNebula">OpenNebula</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-OpenStack" aria-controls="tab-configuration-infrastructure-OpenStack" ><label for="tab-configuration-infrastructure-OpenStack">OpenStack</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Outscale" aria-controls="tab-configuration-infrastructure-Outscale" ><label for="tab-configuration-infrastructure-Outscale">Outscale</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Proxmox" aria-controls="tab-configuration-infrastructure-Proxmox" ><label for="tab-configuration-infrastructure-Proxmox">Proxmox</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Scaleway" aria-controls="tab-configuration-infrastructure-Scaleway" ><label for="tab-configuration-infrastructure-Scaleway">Scaleway</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Tinkerbell" aria-controls="tab-configuration-infrastructure-Tinkerbell" ><label for="tab-configuration-infrastructure-Tinkerbell">Tinkerbell</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-VCD" aria-controls="tab-configuration-infrastructure-VCD" ><label for="tab-configuration-infrastructure-VCD">VCD</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-vcluster" aria-controls="tab-configuration-infrastructure-vcluster" ><label for="tab-configuration-infrastructure-vcluster">vcluster</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Virtink" aria-controls="tab-configuration-infrastructure-Virtink" ><label for="tab-configuration-infrastructure-Virtink">Virtink</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-vSphere" aria-controls="tab-configuration-infrastructure-vSphere" ><label for="tab-configuration-infrastructure-vSphere">vSphere</label><input type="radio" name="tab-configuration-infrastructure" id="tab-configuration-infrastructure-Vultr" aria-controls="tab-configuration-infrastructure-Vultr" ><label for="tab-configuration-infrastructure-Vultr">Vultr</label><div class="tab-panels">
<section id="tab-Akamai (Linode)" class="tab-panel">
<pre><code class="language-bash">export LINODE_REGION=us-ord
export LINODE_TOKEN=&lt;your linode PAT&gt;
export LINODE_CONTROL_PLANE_MACHINE_TYPE=g6-standard-2
export LINODE_MACHINE_TYPE=g6-standard-2
</code></pre>
<p>See the <a href="https://linode.github.io/cluster-api-provider-linode/introduction.html">Akamai (Linode) provider</a> for more information.</p>
</section>
<section id="tab-AWS" class="tab-panel">
<pre><code class="language-bash">export AWS_REGION=us-east-1
export AWS_SSH_KEY_NAME=default
# Select instance types
export AWS_CONTROL_PLANE_MACHINE_TYPE=t3.large
export AWS_NODE_MACHINE_TYPE=t3.large
</code></pre>
<p>See the <a href="https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites.html">AWS provider prerequisites</a> document for more details.</p>
</section>
<section id="tab-Azure" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-2" id="warning-2">Warning</a></h1>
<p>Make sure you choose a VM size which is available in the desired location for your subscription. To see available SKUs, use <code>az vm list-skus -l &lt;your_location&gt; -r virtualMachines -o table</code></p>
</aside>
<pre><code class="language-bash"># Name of the Azure datacenter location. Change this value to your desired location.
export AZURE_LOCATION=&quot;centralus&quot;

# Select VM types.
export AZURE_CONTROL_PLANE_MACHINE_TYPE=&quot;Standard_D2s_v3&quot;
export AZURE_NODE_MACHINE_TYPE=&quot;Standard_D2s_v3&quot;

# [Optional] Select resource group. The default value is ${CLUSTER_NAME}.
export AZURE_RESOURCE_GROUP=&quot;&lt;ResourceGroupName&gt;&quot;
</code></pre>
</section>
<section id="tab-CloudStack" class="tab-panel">
<p>A Cluster API compatible image must be available in your CloudStack installation. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/providers/cloudstack.html">image-builder (CloudStack)</a></p>
<p>Prebuilt images can be found <a href="http://packages.shapeblue.com/cluster-api-provider-cloudstack/images/">here</a></p>
<p>To see all required CloudStack environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure cloudstack --list-variables capi-quickstart
</code></pre>
<p>Apart from the script, the following CloudStack environment variables are required.</p>
<pre><code class="language-bash"># Set this to the name of the zone in which to deploy the cluster
export CLOUDSTACK_ZONE_NAME=&lt;zone name&gt;
# The name of the network on which the VMs will reside
export CLOUDSTACK_NETWORK_NAME=&lt;network name&gt;
# The endpoint of the workload cluster
export CLUSTER_ENDPOINT_IP=&lt;cluster endpoint address&gt;
export CLUSTER_ENDPOINT_PORT=&lt;cluster endpoint port&gt;
# The service offering of the control plane nodes
export CLOUDSTACK_CONTROL_PLANE_MACHINE_OFFERING=&lt;control plane service offering name&gt;
# The service offering of the worker nodes
export CLOUDSTACK_WORKER_MACHINE_OFFERING=&lt;worker node service offering name&gt;
# The capi compatible template to use
export CLOUDSTACK_TEMPLATE_NAME=&lt;template name&gt;
# The ssh key to use to log into the nodes
export CLOUDSTACK_SSH_KEY_NAME=&lt;ssh key name&gt;

</code></pre>
<p>A full configuration reference can be found in <a href="https://github.com/kubernetes-sigs/cluster-api-provider-cloudstack/blob/master/docs/book/src/clustercloudstack/configuration.md">configuration.md</a>.</p>
</section>
<section id="tab-DigitalOcean" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your DigitalOcean account. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash">export DO_REGION=nyc1
export DO_SSH_KEY_FINGERPRINT=&lt;your-ssh-key-fingerprint&gt;
export DO_CONTROL_PLANE_MACHINE_TYPE=s-2vcpu-2gb
export DO_CONTROL_PLANE_MACHINE_IMAGE=&lt;your-capi-image-id&gt;
export DO_NODE_MACHINE_TYPE=s-2vcpu-2gb
export DO_NODE_MACHINE_IMAGE==&lt;your-capi-image-id&gt;
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-3" id="warning-3">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<p>The Docker provider does not require additional configurations for cluster templates.</p>
<p>However, if you require special network settings you can set the following environment variables:</p>
<pre><code class="language-bash"># The list of service CIDR, default [&quot;10.128.0.0/12&quot;]
export SERVICE_CIDR=[&quot;10.96.0.0/12&quot;]

# The list of pod CIDR, default [&quot;192.168.0.0/16&quot;]
export POD_CIDR=[&quot;192.168.0.0/16&quot;]

# The service domain, default &quot;cluster.local&quot;
export SERVICE_DOMAIN=&quot;k8s.test&quot;
</code></pre>
<p>It is also possible but <strong>not recommended</strong> to disable the per-default enabled <a href="../security/pod-security-standards.html">Pod Security Standard</a>:</p>
<pre><code class="language-bash">export POD_SECURITY_STANDARD_ENABLED=&quot;false&quot;
</code></pre>
</section>
<section id="tab-GCP" class="tab-panel">
<pre><code class="language-bash"># Name of the GCP datacenter location. Change this value to your desired location
export GCP_REGION=&quot;&lt;GCP_REGION&gt;&quot;
export GCP_PROJECT=&quot;&lt;GCP_PROJECT&gt;&quot;
# Make sure to use same Kubernetes version here as building the GCE image
export KUBERNETES_VERSION=1.23.3
# This is the image you built. See https://github.com/kubernetes-sigs/image-builder
export IMAGE_ID=projects/$GCP_PROJECT/global/images/&lt;built image&gt;
export GCP_CONTROL_PLANE_MACHINE_TYPE=n1-standard-2
export GCP_NODE_MACHINE_TYPE=n1-standard-2
export GCP_NETWORK_NAME=&lt;GCP_NETWORK_NAME or default&gt;
export CLUSTER_NAME=&quot;&lt;CLUSTER_NAME&gt;&quot;
</code></pre>
<p>See the <a href="https://cluster-api-gcp.sigs.k8s.io/">GCP provider</a> for more information.</p>
</section>
<section id="tab-Harvester" class="tab-panel">
<pre><code class="language-bash"># Cloud Provider credentials, which are a Kubeconfig generated using this process: https://docs.harvesterhci.io/v1.3/rancher/cloud-provider/#deploying-to-the-rke2-custom-cluster-experimental
# Since v0.1.5, this can be left &quot;&quot;, because the controller can update it automatically
export CLOUD_CONFIG_KUBECONFIG_B64=&quot;&quot;
# Name of the CAPI Cluster
export CLUSTER_NAME=&quot;&lt;CLUSTER_NAME&gt;&quot;
# Number of Control Plane machines
export CONTROL_PLANE_MACHINE_COUNT=3
# URL to access the Harvester Cluster, this will be overridden by the controller
export HARVESTER_ENDPOINT=&quot;&quot;
# Base64-Encoded Kubeconfig to access Harvester, which can be downloaded from Harvester's UI or from a Harvester Manager Node.
export HARVESTER_KUBECONFIG_B64=&quot;&lt;HARVESTER_KUBECONFIG_ENCODED_IN_BASE64&gt;&quot;
# Namespace for all resources in the Management Cluster
export NAMESPACE=&quot;test&quot;
# Pod CIDR for the Workload Cluster, it should have the format: 192.168.0.0/16
export POD_CIDR=&quot;10.42.0.0/16&quot;
# Service CIDR for the Workload Cluster, it should have the format : 192.168.0.0/16 and be different from POD_CIDR
export SERVICE_CIDR=&quot;10.43.0.0/16&quot;
# Reference to SSH Keypair in Harvester. It should follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export SSH_KEYPAIR=&quot;default/ssk-key-pair&quot;
# Namespace in Harvester where the VMs will be created.
export TARGET_HARVESTER_NAMESPACE=&quot;default&quot;
# Disk Size to be used by the VMs
export VM_DISK_SIZE=&quot;50Gi&quot;
# Reference to OS Image in Harvester which will be used for creating VMs, It must follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export VM_IMAGE_NAME=&quot;default/jammy-server&quot;
# Reference to VM Network in Harvester. It must follow the format &lt;NAMESPACE&gt;/&lt;NAME&gt;
export VM_NETWORK=&quot;default/untagged&quot;
# Linux Username for the VMs
export VM_SSH_USER=&quot;ubuntu&quot;
# Number of Worker nodes in the target Workload cluster
export WORKER_MACHINE_COUNT=2
</code></pre>
<p>See the <a href="https://github.com/rancher-sandbox/cluster-api-provider-harvester">Harvester provider</a> for more information.</p>
</section>
<section id="tab-Huawei" class="tab-panel">
<pre><code class="language-bash"># huawei cloud region
export HC_REGION=&quot;cn-east-1&quot;
# ECS SSH key name
export HC_SSH_KEY_NAME=&quot;default&quot;
# kubernetes version
export KUBERNETES_VERSION=&quot;1.32.0&quot;
# number of control plane machines
export CONTROL_PLANE_MACHINE_COUNT=&quot;1&quot;
# number of worker machines
export WORKER_MACHINE_COUNT=&quot;1&quot;
# control plane machine type
export HC_CONTROL_PLANE_MACHINE_TYPE=&quot;x1e.2u.4g&quot;
# worker node machine type
export HC_NODE_MACHINE_TYPE=&quot;x1e.2u.4g&quot;
# ECS image ID
export ECS_IMAGE_ID=&quot;218ca5t7-bxf3-5dg0-852p-y703c9fe1a52&quot;
</code></pre>
<p>See the <a href="https://github.com/HuaweiCloudDeveloper/cluster-api-provider-huawei">Huawei Cloud provider</a> for more information.</p>
</section>
<section id="tab-IBM Cloud" class="tab-panel">
<pre><code class="language-bash"># Required environment variables for VPC
# VPC region
export IBMVPC_REGION=us-south
# VPC zone within the region
export IBMVPC_ZONE=us-south-1
# ID of the resource group in which the VPC will be created
export IBMVPC_RESOURCEGROUP=&lt;your-resource-group-id&gt;
# Name of the VPC
export IBMVPC_NAME=ibm-vpc-0
export IBMVPC_IMAGE_ID=&lt;you-image-id&gt;
# Profile for the virtual server instances
export IBMVPC_PROFILE=bx2-4x16
export IBMVPC_SSHKEY_ID=&lt;your-sshkey-id&gt;

# Required environment variables for PowerVS
export IBMPOWERVS_SSHKEY_NAME=&lt;your-ssh-key&gt;
# Internal and external IP of the network
export IBMPOWERVS_VIP=&lt;internal-ip&gt;
export IBMPOWERVS_VIP_EXTERNAL=&lt;external-ip&gt;
export IBMPOWERVS_VIP_CIDR=29
export IBMPOWERVS_IMAGE_NAME=&lt;your-capi-image-name&gt;
# ID of the PowerVS service instance
export IBMPOWERVS_SERVICE_INSTANCE_ID=&lt;service-instance-id&gt;
export IBMPOWERVS_NETWORK_NAME=&lt;your-capi-network-name&gt;
</code></pre>
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-ibmcloud">IBM Cloud provider</a> for more information.</p>
</section>
<section id="tab-IONOS Cloud" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your IONOS Cloud contract.
For instructions on how to build a compatible Image, see <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud/blob/main/docs/custom-image.md">our docs</a>.</p>
<pre><code class="language-bash"># The token which is used to authenticate against the IONOS Cloud API
export IONOS_TOKEN=&lt;your-token&gt;
# The datacenter ID where the cluster will be deployed
export IONOSCLOUD_DATACENTER_ID=&quot;&lt;your-datacenter-id&gt;&quot;
# The IP of the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=10.10.10.4
# The location of the data center where the cluster will be deployed
export CONTROL_PLANE_ENDPOINT_LOCATION=de/txl
# The image ID of the custom image that will be used for the VMs
export IONOSCLOUD_MACHINE_IMAGE_ID=&quot;&lt;your-image-id&gt;&quot;
# The SSH key that will be used to access the VMs
export IONOSCLOUD_MACHINE_SSH_KEYS=&quot;&lt;your-ssh-key&gt;&quot;
</code></pre>
<p>For more configuration options check our list of <a href="https://github.com/ionos-cloud/cluster-api-provider-ionoscloud/blob/main/docs/quickstart.md#environment-variables">available variables</a></p>
</section>
<section id="tab-K0smotron" class="tab-panel">
<p>Please visit the <a href="https://github.com/k0sproject/k0smotron">K0smotron provider</a> for more information.</p>
</section>
<section id="tab-KubeKey" class="tab-panel">
<pre><code class="language-bash"># Required environment variables
# The KKZONE is used to specify where to download the binaries. (e.g. &quot;&quot;, &quot;cn&quot;)
export KKZONE=&quot;&quot;
# The ssh name of the all instance Linux user. (e.g. root, ubuntu)
export USER_NAME=&lt;your-linux-user&gt;
# The ssh password of the all instance Linux user.
export PASSWORD=&lt;your-linux-user-password&gt;
# The ssh IP address of the all instance. (e.g. &quot;[{address: 192.168.100.3}, {address: 192.168.100.4}]&quot;)
export INSTANCES=&lt;your-linux-ip-address&gt;
# The cluster control plane VIP. (e.g. &quot;192.168.100.100&quot;)
export CONTROL_PLANE_ENDPOINT_IP=&lt;your-control-plane-virtual-ip&gt;
</code></pre>
<p>Please visit the <a href="https://github.com/kubesphere/kubekey">KubeKey provider</a> for more information.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>In this example, we’ll use the image for Kubernetes v1.32.1:</p>
<pre><code class="language-bash">export NODE_VM_IMAGE_TEMPLATE=&quot;quay.io/capk/ubuntu-2404-container-disk:v1.32.1&quot;
export CAPK_GUEST_K8S_VERSION=&quot;${NODE_VM_IMAGE_TEMPLATE/*:/}&quot;
export CRI_PATH=&quot;unix:///var/run/containerd/containerd.sock&quot;
</code></pre>
<p>Please visit the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt/">KubeVirt project</a> for more information.</p>
<aside class="note">
<h1><a class="header" href="#note" id="note">Note</a></h1>
<p>Find additional images under <a href="https://quay.io/capk/ubuntu-2404-container-disk">quay.io/capk/ubuntu-2404-container-disk</a>,
<a href="https://quay.io/capk/ubuntu-2204-container-disk">quay.io/capk/ubuntu-2204-container-disk</a>,
or <a href="https://quay.io/capk/ubuntu-2004-container-disk">quay.io/capk/ubuntu-2004-container-disk</a>.</p>
<p>Alternatively, create your own image; see <a href="https://github.com/kubernetes-sigs/image-builder">here</a>.</p>
</aside>
</section>
<section id="tab-Metal3" class="tab-panel">
<p><strong>Note</strong>: If you are running CAPM3 release prior to v0.5.0, make sure to export the following
environment variables. However, you don’t need them to be exported if you use
CAPM3 release v0.5.0 or higher.</p>
<pre><code class="language-bash"># The URL of the kernel to deploy.
export DEPLOY_KERNEL_URL=&quot;http://172.22.0.1:6180/images/ironic-python-agent.kernel&quot;
# The URL of the ramdisk to deploy.
export DEPLOY_RAMDISK_URL=&quot;http://172.22.0.1:6180/images/ironic-python-agent.initramfs&quot;
# The URL of the Ironic endpoint.
export IRONIC_URL=&quot;http://172.22.0.1:6385/v1/&quot;
# The URL of the Ironic inspector endpoint.
export IRONIC_INSPECTOR_URL=&quot;http://172.22.0.1:5050/v1/&quot;
# Do not use a dedicated CA certificate for Ironic API. Any value provided in this variable disables additional CA certificate validation.
# To provide a CA certificate, leave this variable unset. If unset, then IRONIC_CA_CERT_B64 must be set.
export IRONIC_NO_CA_CERT=true
# Disables basic authentication for Ironic API. Any value provided in this variable disables authentication.
# To enable authentication, leave this variable unset. If unset, then IRONIC_USERNAME and IRONIC_PASSWORD must be set.
export IRONIC_NO_BASIC_AUTH=true
# Disables basic authentication for Ironic inspector API. Any value provided in this variable disables authentication.
# To enable authentication, leave this variable unset. If unset, then IRONIC_INSPECTOR_USERNAME and IRONIC_INSPECTOR_PASSWORD must be set.
export IRONIC_INSPECTOR_NO_BASIC_AUTH=true
</code></pre>
<p>Please visit the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/docs/getting-started.md">Metal3 getting started guide</a> for more details.</p>
</section>
<section id="tab-metal-stack" class="tab-panel">
<pre><code class="language-bash">export METAL_PARTITION=&lt;metal-stack-partition&gt;
export METAL_PROJECT_ID=&lt;metal-stack-project-id&gt;
export CONTROL_PLANE_IP=&lt;metal-stack-control-plane-ip&gt;

export FIREWALL_MACHINE_IMAGE=&lt;firewall-os-image&gt;
export FIREWALL_MACHINE_SIZE=&lt;firewall-size&gt;

export CONTROL_PLANE_MACHINE_IMAGE=&lt;machine-os-image&gt;
export CONTROL_PLANE_MACHINE_SIZE=&lt;machine-size&gt;
export WORKER_MACHINE_IMAGE=&lt;machine-os-image&gt;
export WORKER_MACHINE_SIZE=&lt;machine-size&gt;
</code></pre>
<p>Please visit the <a href="https://metal-stack.io/docs/references/cluster-api-provider-metal-stack#getting-started">metal-stack getting started guide</a> for more details.</p>
</section>
<section id="tab-Nutanix" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Nutanix image library. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<p>To see all required Nutanix environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure nutanix --list-variables capi-quickstart
</code></pre>
</section>
<section id="tab-OpenNebula" class="tab-panel">
<pre><code class="language-bash"># OpenNebula API endpoint and credentials
export ONE_XMLRPC='http://10.2.11.40:2633/RPC2'
export ONE_AUTH='oneadmin:opennebula'

# VM and VR templates to construct workload clusters from
export MACHINE_TEMPLATE_NAME='capone131'
export ROUTER_TEMPLATE_NAME='capone131-vr'

# VNs to deploy workload clusters into
export PUBLIC_NETWORK_NAME='service'
export PRIVATE_NETWORK_NAME='private'

# Name of the new workload cluster
export CLUSTER_NAME='one'

# Cloud-Provider image to deploy inside the new workload cluster
export CCM_IMG='ghcr.io/opennebula/cloud-provider-opennebula:latest'

# Initial size of the new workload cluster
export CONTROL_PLANE_MACHINE_COUNT='1'
export WORKER_MACHINE_COUNT='1'
</code></pre>
<p>Please visit <a href="https://github.com/OpenNebula/cluster-api-provider-opennebula/wiki">OpenNebula Cluster API Provider Wiki</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your OpenStack. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.
Depending on your OpenStack and underlying hypervisor the following options might be of interest:</p>
<ul>
<li><a href="https://image-builder.sigs.k8s.io/capi/providers/openstack.html">image-builder (OpenStack)</a></li>
<li><a href="https://image-builder.sigs.k8s.io/capi/providers/vsphere.html">image-builder (vSphere)</a></li>
</ul>
<p>To see all required OpenStack environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure openstack --list-variables capi-quickstart
</code></pre>
<p>The following script can be used to export some of them:</p>
<pre><code class="language-bash">wget https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc -O /tmp/env.rc
source /tmp/env.rc &lt;path/to/clouds.yaml&gt; &lt;cloud&gt;
</code></pre>
<p>Apart from the script, the following OpenStack environment variables are required.</p>
<pre><code class="language-bash"># The list of nameservers for OpenStack Subnet being created.
# Set this value when you need create a new network/subnet while the access through DNS is required.
export OPENSTACK_DNS_NAMESERVERS=&lt;dns nameserver&gt;
# FailureDomain is the failure domain the machine will be created in.
export OPENSTACK_FAILURE_DOMAIN=&lt;availability zone name&gt;
# The flavor reference for the flavor for your server instance.
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=&lt;flavor&gt;
# The flavor reference for the flavor for your server instance.
export OPENSTACK_NODE_MACHINE_FLAVOR=&lt;flavor&gt;
# The name of the image to use for your server instance. If the RootVolume is specified, this will be ignored and use rootVolume directly.
export OPENSTACK_IMAGE_NAME=&lt;image name&gt;
# The SSH key pair name
export OPENSTACK_SSH_KEY_NAME=&lt;ssh key pair name&gt;
# The external network
export OPENSTACK_EXTERNAL_NETWORK_ID=&lt;external network ID&gt;
</code></pre>
<p>A full configuration reference can be found in <a href="https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/master/docs/book/src/clusteropenstack/configuration.md">configuration.md</a>.</p>
</section>
<section id="tab-Outscale" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Outscale account. For instructions on how to build a compatible image
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash"># The outscale root disk iops
export OSC_IOPS=&quot;&lt;IOPS&gt;&quot;
# The outscale root disk size
export OSC_VOLUME_SIZE=&quot;&lt;VOLUME_SIZE&gt;&quot;
# The outscale root disk volumeType
export OSC_VOLUME_TYPE=&quot;&lt;VOLUME_TYPE&gt;&quot;
# The outscale key pair
export OSC_KEYPAIR_NAME=&quot;&lt;KEYPAIR_NAME&gt;&quot;
# The outscale subregion name
export OSC_SUBREGION_NAME=&quot;&lt;SUBREGION_NAME&gt;&quot;
# The outscale vm type
export OSC_VM_TYPE=&quot;&lt;VM_TYPE&gt;&quot;
# The outscale image name
export OSC_IMAGE_NAME=&quot;&lt;IMAGE_NAME&gt;&quot;
</code></pre>
</section>
<section id="tab-Proxmox" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your Proxmox cluster. For instructions on how to build a compatible VM template
see <a href="https://image-builder.sigs.k8s.io/capi/capi.html">image-builder</a>.</p>
<pre><code class="language-bash"># The node that hosts the VM template to be used to provision VMs
export PROXMOX_SOURCENODE=&quot;pve&quot;
# The template VM ID used for cloning VMs
export TEMPLATE_VMID=100
# The ssh authorized keys used to ssh to the machines.
export VM_SSH_KEYS=&quot;ssh-ed25519 ..., ssh-ed25519 ...&quot;
# The IP address used for the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=10.10.10.4
# The IP ranges for Cluster nodes
export NODE_IP_RANGES=&quot;[10.10.10.5-10.10.10.50, 10.10.10.55-10.10.10.70]&quot;
# The gateway for the machines network-config.
export GATEWAY=&quot;10.10.10.1&quot;
# Subnet Mask in CIDR notation for your node IP ranges
export IP_PREFIX=24
# The Proxmox network device for VMs
export BRIDGE=&quot;vmbr1&quot;
# The dns nameservers for the machines network-config.
export DNS_SERVERS=&quot;[8.8.8.8,8.8.4.4]&quot;
# The Proxmox nodes used for VM deployments
export ALLOWED_NODES=&quot;[pve1,pve2,pve3]&quot;
</code></pre>
<p>For more information about prerequisites and advanced setups for Proxmox, see the <a href="https://github.com/ionos-cloud/cluster-api-provider-proxmox/blob/main/docs/Usage.md">Proxmox getting started guide</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<pre><code class="language-bash"># Scaleway credentials, project ID and region.
export SCW_ACCESS_KEY=&quot;&lt;ACCESS_KEY&gt;&quot;
export SCW_SECRET_KEY=&quot;&lt;SECRET_KEY&gt;&quot;
export SCW_PROJECT_ID=&quot;&lt;PROJECT_ID&gt;&quot;
export SCW_REGION=&quot;fr-par&quot;

# Scaleway Instance image names that will be used to provision servers.
export CONTROL_PLANE_MACHINE_IMAGE=&quot;&lt;IMAGE_NAME&gt;&quot;
export WORKER_MACHINE_IMAGE=&quot;&lt;IMAGE_NAME&gt;&quot;
</code></pre>
<p>For more information about prerequisites and advanced setups for CAPS, see the <a href="https://github.com/scaleway/cluster-api-provider-scaleway/blob/main/docs/getting-started.md">CAPS getting started guide</a>.</p>
</section>
<section id="tab-Tinkerbell" class="tab-panel">
<pre><code class="language-bash">export TINKERBELL_IP=&lt;hegel ip&gt;
</code></pre>
<p>For more information please visit <a href="https://github.com/tinkerbell/cluster-api-provider-tinkerbell/blob/main/docs/QUICK-START.md">Tinkerbell getting started guide</a>.</p>
</section>
<section id="tab-VCD" class="tab-panel">
<p>A ClusterAPI compatible image must be available in your VCD catalog. For instructions on how to build and upload a compatible image
see <a href="https://github.com/vmware/cluster-api-provider-cloud-director">CAPVCD</a></p>
<p>To see all required VCD environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure vcd --list-variables capi-quickstart
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">export CLUSTER_NAME=kind
export CLUSTER_NAMESPACE=vcluster
export KUBERNETES_VERSION=1.23.4
export HELM_VALUES=&quot;service:\n  type: NodePort&quot;
</code></pre>
<p>Please see the <a href="https://github.com/loft-sh/cluster-api-provider-vcluster#installation-instructions">vcluster installation instructions</a> for more details.</p>
</section>
<section id="tab-Virtink" class="tab-panel">
<p>To see all required Virtink environment variables execute:</p>
<pre><code class="language-bash">clusterctl generate cluster --infrastructure virtink --list-variables capi-quickstart
</code></pre>
<p>See the <a href="https://github.com/smartxworks/cluster-api-provider-virtink">Virtink provider</a> document for more details.</p>
</section>
<section id="tab-vSphere" class="tab-panel">
<p>It is required to use an official CAPV machine images for your vSphere VM templates. See <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md#uploading-the-machine-images">uploading CAPV machine images</a> for instructions on how to do this.</p>
<pre><code class="language-bash"># The vCenter server IP or FQDN
export VSPHERE_SERVER=&quot;10.0.0.1&quot;
# The vSphere datacenter to deploy the management cluster on
export VSPHERE_DATACENTER=&quot;SDDC-Datacenter&quot;
# The vSphere datastore to deploy the management cluster on
export VSPHERE_DATASTORE=&quot;vsanDatastore&quot;
# The VM network to deploy the management cluster on
export VSPHERE_NETWORK=&quot;VM Network&quot;
# The vSphere resource pool for your VMs
export VSPHERE_RESOURCE_POOL=&quot;*/Resources&quot;
# The VM folder for your VMs. Set to &quot;&quot; to use the root vSphere folder
export VSPHERE_FOLDER=&quot;vm&quot;
# The VM template to use for your VMs
export VSPHERE_TEMPLATE=&quot;ubuntu-1804-kube-v1.17.3&quot;
# The public ssh authorized key on all machines
export VSPHERE_SSH_AUTHORIZED_KEY=&quot;ssh-rsa AAAAB3N...&quot;
# The certificate thumbprint for the vCenter server
export VSPHERE_TLS_THUMBPRINT=&quot;97:48:03:8D:78:A9...&quot;
# The storage policy to be used (optional). Set to &quot;&quot; if not required
export VSPHERE_STORAGE_POLICY=&quot;policy-one&quot;
# The IP address used for the control plane endpoint
export CONTROL_PLANE_ENDPOINT_IP=&quot;1.2.3.4&quot;
</code></pre>
<p>For more information about prerequisites, credentials management, or permissions for vSphere, see the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md">vSphere getting started guide</a>.</p>
</section>
<section id="tab-Vultr" class="tab-panel">
<p>A Cluster API compatible image must be available in your Vultr account. For instructions on how to build a compatible image see image-builder for <a href="https://github.com/vultr/cluster-api-provider-vultr/blob/main/docs/getting-started.md">Vultr</a></p>
<pre><code class="language-bash">export CLUSTER_NAME=&lt;clustername&gt;
export KUBERNETES_VERSION=v1.28.9
export CONTROL_PLANE_MACHINE_COUNT=1
export CONTROL_PLANE_PLANID=&lt;plan_id&gt;
export WORKER_MACHINE_COUNT=1
export WORKER_PLANID=&lt;plan_id&gt;
export MACHINE_IMAGE=&lt;snapshot_id&gt;
export REGION=&lt;region&gt;
export PLANID=&lt;plan_id&gt;
export VPCID=&lt;vpc_id&gt;
export SSHKEY_ID=&lt;sshKey_id&gt;
</code></pre>
</section>
</div></div>
<h4><a class="header" href="#generating-the-cluster-configuration" id="generating-the-cluster-configuration">Generating the cluster configuration</a></h4>
<p>For the purpose of this tutorial, we’ll name our cluster capi-quickstart.</p>
<div id="tab-clusterctl-config-cluster" class="tabset"><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster-Docker" aria-controls="tab-clusterctl-config-cluster-Docker" checked><label for="tab-clusterctl-config-cluster-Docker">Docker</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- vcluster" aria-controls="tab-clusterctl-config-cluster- vcluster" ><label for="tab-clusterctl-config-cluster- vcluster"> vcluster</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- KubeVirt" aria-controls="tab-clusterctl-config-cluster- KubeVirt" ><label for="tab-clusterctl-config-cluster- KubeVirt"> KubeVirt</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- Azure" aria-controls="tab-clusterctl-config-cluster- Azure" ><label for="tab-clusterctl-config-cluster- Azure"> Azure</label><input type="radio" name="tab-clusterctl-config-cluster" id="tab-clusterctl-config-cluster- Other providers..." aria-controls="tab-clusterctl-config-cluster- Other providers..." ><label for="tab-clusterctl-config-cluster- Other providers..."> Other providers...</label><div class="tab-panels">
<section id="tab-Docker" class="tab-panel">
<aside class="note warning">
<h1><a class="header" href="#warning-4" id="warning-4">Warning</a></h1>
<p>The Docker provider is not designed for production use and is intended for development environments only.</p>
</aside>
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart --flavor development \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml
</code></pre>
<p>Note: If you want to use MachinePools use flavor <code>development-mp</code>.</p>
</section>
<section id="tab-vcluster" class="tab-panel">
<pre><code class="language-bash">export CLUSTER_NAME=kind
export CLUSTER_NAMESPACE=vcluster
export KUBERNETES_VERSION=1.31.2
export HELM_VALUES=&quot;service:\n  type: NodePort&quot;

kubectl create namespace ${CLUSTER_NAMESPACE}
clusterctl generate cluster ${CLUSTER_NAME} \
    --infrastructure vcluster \
    --kubernetes-version ${KUBERNETES_VERSION} \
    --target-namespace ${CLUSTER_NAMESPACE} | kubectl apply -f -
</code></pre>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>As we described above, in this tutorial, we will use a LoadBalancer service in order to expose the API server of the
workload cluster, so we want to use the load balancer (lb) template (rather than the default one). We’ll use the
clusterctl’s <code>--flavor</code> flag for that:</p>
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --infrastructure=&quot;kubevirt&quot; \
  --flavor lb \
  --kubernetes-version ${CAPK_GUEST_K8S_VERSION} \
  --control-plane-machine-count=1 \
  --worker-machine-count=1 \
  &gt; capi-quickstart.yaml
</code></pre>
</section>
<section id="tab-Azure" class="tab-panel">
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --infrastructure azure \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml

# Cluster templates authenticate with Workload Identity by default. Modify the AzureClusterIdentity for ServicePrincipal authentication.
# See https://capz.sigs.k8s.io/topics/identities for more details.
yq -i &quot;with(. | select(.kind == \&quot;AzureClusterIdentity\&quot;); .spec.type |= \&quot;ServicePrincipal\&quot; | .spec.clientSecret.name |= \&quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAME}\&quot; | .spec.clientSecret.namespace |= \&quot;${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}\&quot;)&quot; capi-quickstart.yaml
</code></pre>
</section>
<section id="tab-Other providers..." class="tab-panel">
<pre><code class="language-bash">clusterctl generate cluster capi-quickstart \
  --kubernetes-version v1.35.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  &gt; capi-quickstart.yaml
</code></pre>
</section>
</div></div>
<p>This creates a YAML file named <code>capi-quickstart.yaml</code> with a predefined list of Cluster API objects; Cluster, Machines,
Machine Deployments, etc.</p>
<p>The file can be eventually modified using your editor of choice.</p>
<p>See <a href="../clusterctl/commands/generate-cluster.html">clusterctl generate cluster</a> for more details.</p>
<h4><a class="header" href="#apply-the-workload-cluster" id="apply-the-workload-cluster">Apply the workload cluster</a></h4>
<p>When ready, run the following command to apply the cluster manifest.</p>
<pre><code class="language-bash">kubectl apply -f capi-quickstart.yaml
</code></pre>
<p>The output is similar to this:</p>
<pre><code class="language-bash">cluster.cluster.x-k8s.io/capi-quickstart created
dockercluster.infrastructure.cluster.x-k8s.io/capi-quickstart created
kubeadmcontrolplane.controlplane.cluster.x-k8s.io/capi-quickstart-control-plane created
dockermachinetemplate.infrastructure.cluster.x-k8s.io/capi-quickstart-control-plane created
machinedeployment.cluster.x-k8s.io/capi-quickstart-md-0 created
dockermachinetemplate.infrastructure.cluster.x-k8s.io/capi-quickstart-md-0 created
kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/capi-quickstart-md-0 created
</code></pre>
<h4><a class="header" href="#accessing-the-workload-cluster" id="accessing-the-workload-cluster">Accessing the workload cluster</a></h4>
<p>The cluster will now start provisioning. You can check status with:</p>
<pre><code class="language-bash">kubectl get cluster
</code></pre>
<p>You can also get an “at glance” view of the cluster and its resources by running:</p>
<pre><code class="language-bash">clusterctl describe cluster capi-quickstart
</code></pre>
<p>and see an output similar to this:</p>
<pre><code class="language-bash">NAME              PHASE         AGE   VERSION
capi-quickstart   Provisioned   8s    v1.35.0
</code></pre>
<p>To verify the first control plane is up:</p>
<pre><code class="language-bash">kubectl get kubeadmcontrolplane
</code></pre>
<p>You should see an output is similar to this:</p>
<pre><code class="language-bash">NAME                    CLUSTER           INITIALIZED   API SERVER AVAILABLE   REPLICAS   READY   UPDATED   UNAVAILABLE   AGE    VERSION
capi-quickstart-g2trk   capi-quickstart   true                                 3                  3         3             4m7s   v1.35.0
</code></pre>
<aside class="note warning">
<h1><a class="header" href="#warning-5" id="warning-5"> Warning </a></h1>
<p>The control plane won’t be <code>Ready</code> until we install a CNI in the next step.</p>
</aside>
<p>After the first control plane node is up and running, we can retrieve the <a href="../reference/glossary.html#workload-cluster">workload cluster</a> Kubeconfig.</p>
<div id="tab-get-kubeconfig" class="tabset"><input type="radio" name="tab-get-kubeconfig" id="tab-get-kubeconfig-Default" aria-controls="tab-get-kubeconfig-Default" checked><label for="tab-get-kubeconfig-Default">Default</label><input type="radio" name="tab-get-kubeconfig" id="tab-get-kubeconfig-Docker" aria-controls="tab-get-kubeconfig-Docker" ><label for="tab-get-kubeconfig-Docker">Docker</label><div class="tab-panels">
</section>
<section id="tab-Default" class="tab-panel">
<pre><code class="language-bash">clusterctl get kubeconfig capi-quickstart &gt; capi-quickstart.kubeconfig
</code></pre>
</section>
<section id="tab-Docker" class="tab-panel">
For Docker Desktop on macOS, Linux or Windows use kind to retrieve the kubeconfig. Docker Engine for Linux works with the default clusterctl approach.
<pre><code class="language-bash">kind get kubeconfig --name capi-quickstart &gt; capi-quickstart.kubeconfig
</code></pre>
<aside class="note warning">
<p>Note: To use the default clusterctl method to retrieve kubeconfig for a workload cluster created with the Docker provider when using Docker Desktop see <a href="../clusterctl/developers.html#additional-notes-for-the-docker-provider">Additional Notes for the Docker provider</a>.</p>
</aside>
</section>
</div></div>
<h3><a class="header" href="#install-a-cloud-provider" id="install-a-cloud-provider">Install a Cloud Provider</a></h3>
<p>The Kubernetes in-tree cloud provider implementations are being <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2395-removing-in-tree-cloud-providers">removed</a> in favor of external cloud providers (also referred to as “out-of-tree”). This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager. To learn more, see <a href="https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">this blog post</a>.</p>
<div id="tab-install-cloud-provider" class="tabset"><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-Azure" aria-controls="tab-install-cloud-provider-Azure" checked><label for="tab-install-cloud-provider-Azure">Azure</label><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-OpenStack" aria-controls="tab-install-cloud-provider-OpenStack" ><label for="tab-install-cloud-provider-OpenStack">OpenStack</label><input type="radio" name="tab-install-cloud-provider" id="tab-install-cloud-provider-Scaleway" aria-controls="tab-install-cloud-provider-Scaleway" ><label for="tab-install-cloud-provider-Scaleway">Scaleway</label><div class="tab-panels">
<section id="tab-Azure" class="tab-panel">
<p>Install the official cloud-provider-azure Helm chart on the workload cluster:</p>
<pre><code class="language-bash">helm install --kubeconfig=./capi-quickstart.kubeconfig --repo https://raw.githubusercontent.com/kubernetes-sigs/cloud-provider-azure/master/helm/repo cloud-provider-azure --generate-name --set infra.clusterName=capi-quickstart --set cloudControllerManager.clusterCIDR=&quot;192.168.0.0/16&quot;
</code></pre>
<p>For more information, see the <a href="https://capz.sigs.k8s.io/self-managed/addons.html">CAPZ book</a>.</p>
</section>
<section id="tab-OpenStack" class="tab-panel">
<p>Before deploying the OpenStack external cloud provider, configure the <code>cloud.conf</code> file for integration with your OpenStack environment:</p>
<pre><code class="language-bash">cat &gt; cloud.conf &lt;&lt;EOF
[Global]
auth-url=&lt;your_auth_url&gt;
application-credential-id=&lt;your_credential_id&gt;
application-credential-secret=&lt;your_credential_secret&gt;
region=&lt;your_region&gt;
domain-name=&lt;your_domain_name&gt;
EOF
</code></pre>
<p>For more detailed information on configuring the <code>cloud.conf</code> file, see the <a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/using-openstack-cloud-controller-manager.md#config-openstack-cloud-controller-manager">OpenStack Cloud Controller Manager documentation</a>.</p>
<p>Next, create a Kubernetes secret using this configuration to securely store your cloud environment details.
You can create this secret for example with:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig -n kube-system create secret generic cloud-config --from-file=cloud.conf
</code></pre>
<p>Now, you are ready to deploy the external cloud provider!</p>
<pre><code class="language-bash">kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/cloud-controller-manager-roles.yaml
kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml
kubectl apply --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml
</code></pre>
<p>Alternatively, refer to the <a href="https://github.com/kubernetes/cloud-provider-openstack/tree/master/charts/openstack-cloud-controller-manager">helm chart</a>.</p>
</section>
<section id="tab-Scaleway" class="tab-panel">
<p>Before deploying the Scaleway external cloud provider, you will need:</p>
<ul>
<li>Your Scaleway credentials (access key and secret key)</li>
<li>Your Scaleway project ID</li>
<li>The Scaleway region where your workload cluster is deployed</li>
<li>The Private Network ID of your cluster (optional)</li>
</ul>
<p>First, create the Secret named <code>scaleway-secret</code> in your workload cluster:</p>
<pre><code class="language-bash">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: scaleway-secret
  namespace: kube-system
type: Opaque
stringData:
  SCW_ACCESS_KEY: &quot;xxxxxxxxxxxxxxxx&quot;
  SCW_SECRET_KEY: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;
  SCW_DEFAULT_PROJECT_ID: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx&quot;
  SCW_DEFAULT_REGION: &quot;fr-par&quot;
  SCW_DEFAULT_ZONE: &quot;fr-par-1&quot;
  PN_ID: &quot;&quot; # If your have a private network on your cluster, you may set its ID here.
EOF
</code></pre>
<p>Finally, you can deploy the <code>scaleway-cloud-controller-manager</code>:</p>
<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/scaleway/scaleway-cloud-controller-manager/master/examples/k8s-scaleway-ccm-latest.yml
</code></pre>
<p>For more detailed information on configuring and using the Scaleway external cloud
provider, see the <a href="https://github.com/scaleway/scaleway-cloud-controller-manager">scaleway-cloud-controller-manager repository</a>.</p>
</section>
</div></div>
<h3><a class="header" href="#deploy-a-cni-solution" id="deploy-a-cni-solution">Deploy a CNI solution</a></h3>
<p>Calico is used here as an example.</p>
<div id="tab-deploy-cni" class="tabset"><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-Azure" aria-controls="tab-deploy-cni-Azure" checked><label for="tab-deploy-cni-Azure">Azure</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-vcluster" aria-controls="tab-deploy-cni-vcluster" ><label for="tab-deploy-cni-vcluster">vcluster</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-KubeVirt" aria-controls="tab-deploy-cni-KubeVirt" ><label for="tab-deploy-cni-KubeVirt">KubeVirt</label><input type="radio" name="tab-deploy-cni" id="tab-deploy-cni-Other providers..." aria-controls="tab-deploy-cni-Other providers..." ><label for="tab-deploy-cni-Other providers...">Other providers...</label><div class="tab-panels">
<section id="tab-Azure" class="tab-panel">
<p>Install the official Calico Helm chart on the workload cluster:</p>
<pre><code class="language-bash">helm repo add projectcalico https://docs.tigera.io/calico/charts --kubeconfig=./capi-quickstart.kubeconfig &amp;&amp; \
helm install calico projectcalico/tigera-operator --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-azure/main/templates/addons/calico/values.yaml --namespace tigera-operator --create-namespace
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state,
let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
<pre><code class="language-bash">NAME                                          STATUS   ROLES           AGE    VERSION
capi-quickstart-vs89t-gmbld                   Ready    control-plane   5m33s  v1.35.0
capi-quickstart-vs89t-kf9l5                   Ready    control-plane   6m20s  v1.35.0
capi-quickstart-vs89t-t8cfn                   Ready    control-plane   7m10s  v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-8tq9v   Ready    &lt;none&gt;          6m5s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-glnjd   Ready    &lt;none&gt;          6m9s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-sfzp6   Ready    &lt;none&gt;          6m9s   v1.35.0
</code></pre>
</section>
<section id="tab-vcluster" class="tab-panel">
<p>Calico not required for vcluster.</p>
</section>
<section id="tab-KubeVirt" class="tab-panel">
<p>Before deploying the Calico CNI, make sure the VMs are running:</p>
<pre><code class="language-bash">kubectl get vm
</code></pre>
<p>If our new VMs are running, we should see a response similar to this:</p>
<pre><code class="language-text">NAME                                  AGE    STATUS    READY
capi-quickstart-control-plane-7s945   167m   Running   True
capi-quickstart-md-0-zht5j            164m   Running   True
</code></pre>
<p>We can also read the virtual machine instances:</p>
<pre><code class="language-bash">kubectl get vmi
</code></pre>
<p>The output will be similar to:</p>
<pre><code class="language-text">NAME                                  AGE    PHASE     IP             NODENAME             READY
capi-quickstart-control-plane-7s945   167m   Running   10.244.82.16   kind-control-plane   True
capi-quickstart-md-0-zht5j            164m   Running   10.244.82.17   kind-control-plane   True
</code></pre>
<p>Since our workload cluster is running within the kind cluster, we need to prevent conflicts between the kind
(management) cluster’s CNI, and the workload cluster CNI. The following modifications in the default Calico settings
are enough for these two CNI to work on (actually) the same environment.</p>
<ul>
<li>Change the CIDR to a non-conflicting range</li>
<li>Change the value of the <code>CLUSTER_TYPE</code> environment variable to <code>k8s</code></li>
<li>Change the value of the <code>CALICO_IPV4POOL_IPIP</code> environment variable to <code>Never</code></li>
<li>Change the value of the <code>CALICO_IPV4POOL_VXLAN</code> environment variable to <code>Always</code></li>
<li>Add the <code>FELIX_VXLANPORT</code> environment variable with the value of a non-conflicting port, e.g. <code>&quot;6789&quot;</code>.</li>
</ul>
<p>The following script downloads the Calico manifest and modifies the required field. The CIDR and the port values are examples.</p>
<pre><code class="language-bash">curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/calico.yaml -o calico-workload.yaml

sed -i -E 's|^( +)# (- name: CALICO_IPV4POOL_CIDR)$|\1\2|g;'\
's|^( +)# (  value: )&quot;192.168.0.0/16&quot;|\1\2&quot;10.243.0.0/16&quot;|g;'\
'/- name: CLUSTER_TYPE/{ n; s/( +value: &quot;).+/\1k8s&quot;/g };'\
'/- name: CALICO_IPV4POOL_IPIP/{ n; s/value: &quot;Always&quot;/value: &quot;Never&quot;/ };'\
'/- name: CALICO_IPV4POOL_VXLAN/{ n; s/value: &quot;Never&quot;/value: &quot;Always&quot;/};'\
'/# Set Felix endpoint to host default action to ACCEPT./a\            - name: FELIX_VXLANPORT\n              value: &quot;6789&quot;' \
calico-workload.yaml
</code></pre>
<p>Now, deploy the Calico CNI on the workload cluster:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig create -f calico-workload.yaml
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state, let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
</section>
<section id="tab-Other providers..." class="tab-panel">
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig \
  apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
</code></pre>
<p>After a short while, our nodes should be running and in <code>Ready</code> state,
let’s check the status using <code>kubectl get nodes</code>:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
</code></pre>
<pre><code class="language-bash">NAME                                          STATUS   ROLES           AGE    VERSION
capi-quickstart-vs89t-gmbld                   Ready    control-plane   5m33s  v1.35.0
capi-quickstart-vs89t-kf9l5                   Ready    control-plane   6m20s  v1.35.0
capi-quickstart-vs89t-t8cfn                   Ready    control-plane   7m10s  v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-8tq9v   Ready    &lt;none&gt;          6m5s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-glnjd   Ready    &lt;none&gt;          6m9s   v1.35.0
capi-quickstart-md-0-55x6t-5649968bd7-sfzp6   Ready    &lt;none&gt;          6m9s   v1.35.0
</code></pre>
</section>
</div></div>
<aside class="note">
<h1><a class="header" href="#troubleshooting" id="troubleshooting">Troubleshooting</a></h1>
<p>If the nodes don’t become ready after a long period, read the pods in the <code>kube-system</code> namespace</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get pod -n kube-system
</code></pre>
<p>If the Calico pods are in image pull error state (<code>ErrImagePull</code>), it’s probably because of the Docker Hub pull rate limit.
We can try to fix that by adding a secret with our Docker Hub credentials, and use it;
see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials">here</a>
for details.</p>
<p>First, create the secret. Please notice the Docker config file path, and adjust it to your local setting.</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig create secret generic docker-creds \
    --from-file=.dockerconfigjson=&lt;YOUR DOCKER CONFIG FILE PATH&gt; \
    --type=kubernetes.io/dockerconfigjson \
    -n kube-system
</code></pre>
<p>Now, if the <code>calico-node</code> pods are with status of <code>ErrImagePull</code>, patch their DaemonSet to make them use the new secret to pull images:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig patch daemonset \
    -n kube-system calico-node \
    -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;imagePullSecrets&quot;:[{&quot;name&quot;:&quot;docker-creds&quot;}]}}}}'
</code></pre>
<p>After a short while, the calico-node pods will be with <code>Running</code> status. Now, if the calico-kube-controllers pod is also
in <code>ErrImagePull</code> status, patch its deployment to fix the problem:</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig patch deployment \
    -n kube-system calico-kube-controllers \
    -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;imagePullSecrets&quot;:[{&quot;name&quot;:&quot;docker-creds&quot;}]}}}}'
</code></pre>
<p>Read the pods again</p>
<pre><code class="language-bash">kubectl --kubeconfig=./capi-quickstart.kubeconfig get pod -n kube-system
</code></pre>
<p>Eventually, all the pods in the kube-system namespace will run, and the result should be similar to this:</p>
<pre><code class="language-text">NAME                                                          READY   STATUS    RESTARTS   AGE
calico-kube-controllers-c969cf844-dgld6                       1/1     Running   0          50s
calico-node-7zz7c                                             1/1     Running   0          54s
calico-node-jmjd6                                             1/1     Running   0          54s
coredns-64897985d-dspjm                                       1/1     Running   0          3m49s
coredns-64897985d-pgtgz                                       1/1     Running   0          3m49s
etcd-capi-quickstart-control-plane-kjjbb                      1/1     Running   0          3m57s
kube-apiserver-capi-quickstart-control-plane-kjjbb            1/1     Running   0          3m57s
kube-controller-manager-capi-quickstart-control-plane-kjjbb   1/1     Running   0          3m57s
kube-proxy-b9g5m                                              1/1     Running   0          3m12s
kube-proxy-p6xx8                                              1/1     Running   0          3m49s
kube-scheduler-capi-quickstart-control-plane-kjjbb            1/1     Running   0          3m57s
</code></pre>
</aside>
<h3><a class="header" href="#clean-up" id="clean-up">Clean Up</a></h3>
<p>Delete workload cluster.</p>
<pre><code class="language-bash">kubectl delete cluster capi-quickstart
</code></pre>
<aside class="note warning">
<p>IMPORTANT: In order to ensure a proper cleanup of your infrastructure you must always delete the cluster object. Deleting the entire cluster template with <code>kubectl delete -f capi-quickstart.yaml</code> might lead to pending resources to be cleaned up manually.</p>
</aside>
<p>Delete management cluster</p>
<pre><code class="language-bash">kind delete cluster
</code></pre>
<h2><a class="header" href="#next-steps" id="next-steps">Next steps</a></h2>
<ul>
<li>Create a second workload cluster. Simply follow the steps outlined above, but remember to provide a different name for your second workload cluster.</li>
<li>Deploy applications to your workload cluster. Use the <a href="#deploy-a-cni-solution">CNI deployment steps</a> for pointers.</li>
<li>See the <a href="../clusterctl/overview.html">clusterctl</a> documentation for more detail about clusterctl supported actions.</li>
</ul>
<!-- links -->

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../introduction.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../user/quick-start-operator.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../introduction.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../user/quick-start-operator.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
